{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f157e330",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-13T05:48:49.775231Z",
     "iopub.status.busy": "2025-02-13T05:48:49.774878Z",
     "iopub.status.idle": "2025-02-13T05:48:50.607129Z",
     "shell.execute_reply": "2025-02-13T05:48:50.606005Z"
    },
    "papermill": {
     "duration": 0.837542,
     "end_time": "2025-02-13T05:48:50.608671",
     "exception": false,
     "start_time": "2025-02-13T05:48:49.771129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dgraph-fin/Readme.md\n",
      "/kaggle/input/dgraph-fin/dgraphfin.npz\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ffb995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T05:48:50.614689Z",
     "iopub.status.busy": "2025-02-13T05:48:50.614348Z",
     "iopub.status.idle": "2025-02-13T05:48:56.804894Z",
     "shell.execute_reply": "2025-02-13T05:48:56.803969Z"
    },
    "papermill": {
     "duration": 6.195188,
     "end_time": "2025-02-13T05:48:56.806646",
     "exception": false,
     "start_time": "2025-02-13T05:48:50.611458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygod\r\n",
      "  Downloading pygod-1.1.0-py3-none-any.whl.metadata (15 kB)\r\n",
      "Collecting torch_geometric\r\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pygod) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pygod) (1.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pygod) (3.4.2)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pygod) (1.2.2)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pygod) (75.1.0)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.11)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.9.0)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.67.1)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (25.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->pygod) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->pygod) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->pygod) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->pygod) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->pygod) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->pygod) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2025.1.31)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pygod) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pygod) (3.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pygod) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pygod) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->pygod) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->pygod) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->pygod) (2024.2.0)\r\n",
      "Downloading pygod-1.1.0-py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torch_geometric, pygod\r\n",
      "Successfully installed pygod-1.1.0 torch_geometric-2.6.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pygod torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d8ea10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T05:48:56.814435Z",
     "iopub.status.busy": "2025-02-13T05:48:56.814141Z",
     "iopub.status.idle": "2025-02-13T06:23:02.175366Z",
     "shell.execute_reply": "2025-02-13T06:23:02.174308Z"
    },
    "papermill": {
     "duration": 2045.366981,
     "end_time": "2025-02-13T06:23:02.176935",
     "exception": false,
     "start_time": "2025-02-13T05:48:56.809954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pygod/utils/utility.py:186: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training autoencoder ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ae_dict = torch.load(os.path.join(self.ae_path, 'Graph_AE.pt'))\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 41.74825\n",
      "Epoch: 0010 loss= 28.71758\n",
      "Epoch: 0020 loss= 22.57305\n",
      "Epoch: 0030 loss= 19.26435\n",
      "Epoch: 0040 loss= 16.66113\n",
      "Epoch: 0050 loss= 9.22207\n",
      "Epoch: 0060 loss= 2.07658\n",
      "Epoch: 0070 loss= 1.01230\n",
      "Epoch: 0080 loss= 0.82857\n",
      "Epoch: 0090 loss= 0.70719\n",
      "Epoch: 0100 loss= 0.70981\n",
      "Epoch: 0110 loss= 0.67289\n",
      "Epoch: 0120 loss= 0.57775\n",
      "Epoch: 0130 loss= 0.63716\n",
      "Epoch: 0140 loss= 0.67159\n",
      "Epoch: 0150 loss= 0.59822\n",
      "Epoch: 0160 loss= 0.66566\n",
      "Epoch: 0170 loss= 0.60800\n",
      "Epoch: 0180 loss= 0.56894\n",
      "Epoch: 0190 loss= 0.62744\n",
      "Epoch: 0200 loss= 0.59100\n",
      "Epoch: 0210 loss= 0.56577\n",
      "Epoch: 0220 loss= 0.58603\n",
      "Epoch: 0230 loss= 0.50014\n",
      "Epoch: 0240 loss= 0.53733\n",
      "Epoch: 0250 loss= 0.58627\n",
      "Epoch: 0260 loss= 0.53912\n",
      "Epoch: 0270 loss= 0.59731\n",
      "Epoch: 0280 loss= 0.54537\n",
      "Epoch: 0290 loss= 0.56867\n",
      "Epoch: 0300 loss= 0.48801\n",
      "Epoch: 0310 loss= 0.57566\n",
      "Epoch: 0320 loss= 0.52488\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.8218,  4.7629, -5.3470, -4.6599, -4.4118,  5.6766,  5.5082, -5.2519]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 38.58255\n",
      "Epoch: 0010 loss= 27.31596\n",
      "Epoch: 0020 loss= 16.04342\n",
      "Epoch: 0030 loss= 13.42252\n",
      "Epoch: 0040 loss= 5.97612\n",
      "Epoch: 0050 loss= 1.92771\n",
      "Epoch: 0060 loss= 1.52439\n",
      "Epoch: 0070 loss= 1.00807\n",
      "Epoch: 0080 loss= 0.73409\n",
      "Epoch: 0090 loss= 0.73142\n",
      "Epoch: 0100 loss= 0.70292\n",
      "Epoch: 0110 loss= 0.65698\n",
      "Epoch: 0120 loss= 0.68502\n",
      "Epoch: 0130 loss= 0.65894\n",
      "Epoch: 0140 loss= 0.68381\n",
      "Epoch: 0150 loss= 0.61781\n",
      "Epoch: 0160 loss= 0.56299\n",
      "Epoch: 0170 loss= 0.64625\n",
      "Epoch: 0180 loss= 0.61310\n",
      "Epoch: 0190 loss= 0.55782\n",
      "Epoch: 0200 loss= 0.59901\n",
      "Epoch: 0210 loss= 0.55578\n",
      "Epoch: 0220 loss= 0.62338\n",
      "Epoch: 0230 loss= 0.65824\n",
      "Epoch: 0240 loss= 0.58399\n",
      "Epoch: 0250 loss= 0.57690\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4859\n",
      "timestep:1, pyg_AUC: 0.4887\n",
      "timestep:2, pyg_AUC: 0.4915\n",
      "timestep:3, pyg_AUC: 0.4887\n",
      "timestep:4, pyg_AUC: 0.4859\n",
      "timestep:5, pyg_AUC: 0.4845\n",
      "timestep:6, pyg_AUC: 0.4873\n",
      "timestep:7, pyg_AUC: 0.4929\n",
      "timestep:8, pyg_AUC: 0.4915\n",
      "timestep:9, pyg_AUC: 0.4887\n",
      "timestep:10, pyg_AUC: 0.4915\n",
      "timestep:11, pyg_AUC: 0.4901\n",
      "timestep:12, pyg_AUC: 0.4915\n",
      "timestep:13, pyg_AUC: 0.4915\n",
      "timestep:14, pyg_AUC: 0.4901\n",
      "timestep:15, pyg_AUC: 0.4873\n",
      "timestep:16, pyg_AUC: 0.4944\n",
      "timestep:17, pyg_AUC: 0.4887\n",
      "timestep:18, pyg_AUC: 0.4915\n",
      "timestep:19, pyg_AUC: 0.4873\n",
      "timestep:20, pyg_AUC: 0.4859\n",
      "timestep:21, pyg_AUC: 0.4887\n",
      "timestep:22, pyg_AUC: 0.4887\n",
      "timestep:23, pyg_AUC: 0.4958\n",
      "timestep:24, pyg_AUC: 0.4887\n",
      "timestep:25, pyg_AUC: 0.4944\n",
      "timestep:26, pyg_AUC: 0.4901\n",
      "timestep:27, pyg_AUC: 0.4901\n",
      "timestep:28, pyg_AUC: 0.4887\n",
      "timestep:29, pyg_AUC: 0.4873\n",
      "timestep:30, pyg_AUC: 0.4929\n",
      "timestep:31, pyg_AUC: 0.4887\n",
      "timestep:32, pyg_AUC: 0.4929\n",
      "timestep:33, pyg_AUC: 0.4901\n",
      "timestep:34, pyg_AUC: 0.4873\n",
      "timestep:35, pyg_AUC: 0.4887\n",
      "timestep:36, pyg_AUC: 0.4859\n",
      "timestep:37, pyg_AUC: 0.4873\n",
      "timestep:38, pyg_AUC: 0.4915\n",
      "timestep:39, pyg_AUC: 0.4887\n",
      "timestep:40, pyg_AUC: 0.4901\n",
      "timestep:41, pyg_AUC: 0.4929\n",
      "timestep:42, pyg_AUC: 0.4859\n",
      "timestep:43, pyg_AUC: 0.4859\n",
      "timestep:44, pyg_AUC: 0.4929\n",
      "timestep:45, pyg_AUC: 0.4929\n",
      "timestep:46, pyg_AUC: 0.4901\n",
      "timestep:47, pyg_AUC: 0.4929\n",
      "timestep:48, pyg_AUC: 0.4887\n",
      "timestep:49, pyg_AUC: 0.4887\n",
      "timestep:50, pyg_AUC: 0.4915\n",
      "timestep:51, pyg_AUC: 0.4831\n",
      "timestep:52, pyg_AUC: 0.4915\n",
      "timestep:53, pyg_AUC: 0.4915\n",
      "timestep:54, pyg_AUC: 0.4929\n",
      "timestep:55, pyg_AUC: 0.4915\n",
      "timestep:56, pyg_AUC: 0.4915\n",
      "timestep:57, pyg_AUC: 0.4915\n",
      "timestep:58, pyg_AUC: 0.4929\n",
      "timestep:59, pyg_AUC: 0.4929\n",
      "timestep:60, pyg_AUC: 0.4887\n",
      "timestep:61, pyg_AUC: 0.4873\n",
      "timestep:62, pyg_AUC: 0.4929\n",
      "timestep:63, pyg_AUC: 0.4816\n",
      "timestep:64, pyg_AUC: 0.4859\n",
      "timestep:65, pyg_AUC: 0.4873\n",
      "timestep:66, pyg_AUC: 0.4887\n",
      "timestep:67, pyg_AUC: 0.4915\n",
      "timestep:68, pyg_AUC: 0.4887\n",
      "timestep:69, pyg_AUC: 0.4915\n",
      "timestep:70, pyg_AUC: 0.4944\n",
      "timestep:71, pyg_AUC: 0.4901\n",
      "timestep:72, pyg_AUC: 0.4887\n",
      "timestep:73, pyg_AUC: 0.4929\n",
      "timestep:74, pyg_AUC: 0.4901\n",
      "timestep:75, pyg_AUC: 0.4887\n",
      "timestep:76, pyg_AUC: 0.4901\n",
      "timestep:77, pyg_AUC: 0.4845\n",
      "timestep:78, pyg_AUC: 0.4929\n",
      "timestep:79, pyg_AUC: 0.4901\n",
      "timestep:80, pyg_AUC: 0.4873\n",
      "timestep:81, pyg_AUC: 0.4901\n",
      "timestep:82, pyg_AUC: 0.4915\n",
      "timestep:83, pyg_AUC: 0.4944\n",
      "timestep:84, pyg_AUC: 0.4915\n",
      "timestep:85, pyg_AUC: 0.4859\n",
      "timestep:86, pyg_AUC: 0.4944\n",
      "timestep:87, pyg_AUC: 0.4915\n",
      "timestep:88, pyg_AUC: 0.4915\n",
      "timestep:89, pyg_AUC: 0.4915\n",
      "timestep:90, pyg_AUC: 0.4873\n",
      "timestep:91, pyg_AUC: 0.4929\n",
      "timestep:92, pyg_AUC: 0.4944\n",
      "timestep:93, pyg_AUC: 0.4887\n",
      "timestep:94, pyg_AUC: 0.4958\n",
      "timestep:95, pyg_AUC: 0.4901\n",
      "timestep:96, pyg_AUC: 0.4901\n",
      "timestep:97, pyg_AUC: 0.4887\n",
      "timestep:98, pyg_AUC: 0.4929\n",
      "timestep:99, pyg_AUC: 0.4887\n",
      "timestep:100, pyg_AUC: 0.4915\n",
      "timestep:101, pyg_AUC: 0.4901\n",
      "timestep:102, pyg_AUC: 0.4915\n",
      "timestep:103, pyg_AUC: 0.4929\n",
      "timestep:104, pyg_AUC: 0.4915\n",
      "timestep:105, pyg_AUC: 0.4887\n",
      "timestep:106, pyg_AUC: 0.4901\n",
      "timestep:107, pyg_AUC: 0.4901\n",
      "timestep:108, pyg_AUC: 0.4915\n",
      "timestep:109, pyg_AUC: 0.4929\n",
      "timestep:110, pyg_AUC: 0.4887\n",
      "timestep:111, pyg_AUC: 0.4944\n",
      "timestep:112, pyg_AUC: 0.4915\n",
      "timestep:113, pyg_AUC: 0.4929\n",
      "timestep:114, pyg_AUC: 0.4901\n",
      "timestep:115, pyg_AUC: 0.4887\n",
      "timestep:116, pyg_AUC: 0.4901\n",
      "timestep:117, pyg_AUC: 0.4901\n",
      "timestep:118, pyg_AUC: 0.4887\n",
      "timestep:119, pyg_AUC: 0.4887\n",
      "timestep:120, pyg_AUC: 0.4915\n",
      "timestep:121, pyg_AUC: 0.4873\n",
      "timestep:122, pyg_AUC: 0.4915\n",
      "timestep:123, pyg_AUC: 0.4915\n",
      "timestep:124, pyg_AUC: 0.4915\n",
      "timestep:125, pyg_AUC: 0.4859\n",
      "timestep:126, pyg_AUC: 0.4901\n",
      "timestep:127, pyg_AUC: 0.4915\n",
      "timestep:128, pyg_AUC: 0.4887\n",
      "timestep:129, pyg_AUC: 0.4915\n",
      "timestep:130, pyg_AUC: 0.4887\n",
      "timestep:131, pyg_AUC: 0.4901\n",
      "timestep:132, pyg_AUC: 0.4901\n",
      "timestep:133, pyg_AUC: 0.4915\n",
      "timestep:134, pyg_AUC: 0.4929\n",
      "timestep:135, pyg_AUC: 0.4859\n",
      "timestep:136, pyg_AUC: 0.4929\n",
      "timestep:137, pyg_AUC: 0.4929\n",
      "timestep:138, pyg_AUC: 0.4929\n",
      "timestep:139, pyg_AUC: 0.4901\n",
      "timestep:140, pyg_AUC: 0.4915\n",
      "timestep:141, pyg_AUC: 0.4915\n",
      "timestep:142, pyg_AUC: 0.4901\n",
      "timestep:143, pyg_AUC: 0.4929\n",
      "timestep:144, pyg_AUC: 0.4915\n",
      "timestep:145, pyg_AUC: 0.4929\n",
      "timestep:146, pyg_AUC: 0.4901\n",
      "timestep:147, pyg_AUC: 0.4915\n",
      "timestep:148, pyg_AUC: 0.4887\n",
      "timestep:149, pyg_AUC: 0.4873\n",
      "timestep:150, pyg_AUC: 0.4901\n",
      "timestep:151, pyg_AUC: 0.4915\n",
      "timestep:152, pyg_AUC: 0.4944\n",
      "timestep:153, pyg_AUC: 0.4901\n",
      "timestep:154, pyg_AUC: 0.4915\n",
      "timestep:155, pyg_AUC: 0.4887\n",
      "timestep:156, pyg_AUC: 0.4887\n",
      "timestep:157, pyg_AUC: 0.4901\n",
      "timestep:158, pyg_AUC: 0.4915\n",
      "timestep:159, pyg_AUC: 0.4901\n",
      "timestep:160, pyg_AUC: 0.4859\n",
      "timestep:161, pyg_AUC: 0.4915\n",
      "timestep:162, pyg_AUC: 0.4873\n",
      "timestep:163, pyg_AUC: 0.4915\n",
      "timestep:164, pyg_AUC: 0.4901\n",
      "timestep:165, pyg_AUC: 0.4887\n",
      "timestep:166, pyg_AUC: 0.4873\n",
      "timestep:167, pyg_AUC: 0.4859\n",
      "timestep:168, pyg_AUC: 0.4887\n",
      "timestep:169, pyg_AUC: 0.4887\n",
      "timestep:170, pyg_AUC: 0.4887\n",
      "timestep:171, pyg_AUC: 0.4887\n",
      "timestep:172, pyg_AUC: 0.4929\n",
      "timestep:173, pyg_AUC: 0.4915\n",
      "timestep:174, pyg_AUC: 0.4915\n",
      "timestep:175, pyg_AUC: 0.4915\n",
      "timestep:176, pyg_AUC: 0.4873\n",
      "timestep:177, pyg_AUC: 0.4915\n",
      "timestep:178, pyg_AUC: 0.4915\n",
      "timestep:179, pyg_AUC: 0.4873\n",
      "timestep:180, pyg_AUC: 0.4901\n",
      "timestep:181, pyg_AUC: 0.4831\n",
      "timestep:182, pyg_AUC: 0.4901\n",
      "timestep:183, pyg_AUC: 0.4859\n",
      "timestep:184, pyg_AUC: 0.4845\n",
      "timestep:185, pyg_AUC: 0.4915\n",
      "timestep:186, pyg_AUC: 0.4901\n",
      "timestep:187, pyg_AUC: 0.4901\n",
      "timestep:188, pyg_AUC: 0.4901\n",
      "timestep:189, pyg_AUC: 0.4887\n",
      "timestep:190, pyg_AUC: 0.4887\n",
      "timestep:191, pyg_AUC: 0.4915\n",
      "timestep:192, pyg_AUC: 0.4901\n",
      "timestep:193, pyg_AUC: 0.4901\n",
      "timestep:194, pyg_AUC: 0.4873\n",
      "timestep:195, pyg_AUC: 0.4901\n",
      "timestep:196, pyg_AUC: 0.4873\n",
      "timestep:197, pyg_AUC: 0.4859\n",
      "timestep:198, pyg_AUC: 0.4901\n",
      "timestep:199, pyg_AUC: 0.4887\n",
      "timestep:200, pyg_AUC: 0.4915\n",
      "timestep:201, pyg_AUC: 0.4901\n",
      "timestep:202, pyg_AUC: 0.4873\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4887\n",
      "timestep:205, pyg_AUC: 0.4887\n",
      "timestep:206, pyg_AUC: 0.4859\n",
      "timestep:207, pyg_AUC: 0.4901\n",
      "timestep:208, pyg_AUC: 0.4887\n",
      "timestep:209, pyg_AUC: 0.4873\n",
      "timestep:210, pyg_AUC: 0.4915\n",
      "timestep:211, pyg_AUC: 0.4845\n",
      "timestep:212, pyg_AUC: 0.4901\n",
      "timestep:213, pyg_AUC: 0.4887\n",
      "timestep:214, pyg_AUC: 0.4873\n",
      "timestep:215, pyg_AUC: 0.4873\n",
      "timestep:216, pyg_AUC: 0.4887\n",
      "timestep:217, pyg_AUC: 0.4901\n",
      "timestep:218, pyg_AUC: 0.4901\n",
      "timestep:219, pyg_AUC: 0.4873\n",
      "timestep:220, pyg_AUC: 0.4873\n",
      "timestep:221, pyg_AUC: 0.4915\n",
      "timestep:222, pyg_AUC: 0.4887\n",
      "timestep:223, pyg_AUC: 0.4929\n",
      "timestep:224, pyg_AUC: 0.4859\n",
      "timestep:225, pyg_AUC: 0.4859\n",
      "timestep:226, pyg_AUC: 0.4887\n",
      "timestep:227, pyg_AUC: 0.4915\n",
      "timestep:228, pyg_AUC: 0.4887\n",
      "timestep:229, pyg_AUC: 0.4873\n",
      "timestep:230, pyg_AUC: 0.4887\n",
      "timestep:231, pyg_AUC: 0.4887\n",
      "timestep:232, pyg_AUC: 0.4845\n",
      "timestep:233, pyg_AUC: 0.4901\n",
      "timestep:234, pyg_AUC: 0.4859\n",
      "timestep:235, pyg_AUC: 0.4901\n",
      "timestep:236, pyg_AUC: 0.4901\n",
      "timestep:237, pyg_AUC: 0.4915\n",
      "timestep:238, pyg_AUC: 0.4915\n",
      "timestep:239, pyg_AUC: 0.4873\n",
      "timestep:240, pyg_AUC: 0.4873\n",
      "timestep:241, pyg_AUC: 0.4873\n",
      "timestep:242, pyg_AUC: 0.4859\n",
      "timestep:243, pyg_AUC: 0.4915\n",
      "timestep:244, pyg_AUC: 0.4901\n",
      "timestep:245, pyg_AUC: 0.4859\n",
      "timestep:246, pyg_AUC: 0.4901\n",
      "timestep:247, pyg_AUC: 0.4887\n",
      "timestep:248, pyg_AUC: 0.4845\n",
      "timestep:249, pyg_AUC: 0.4887\n",
      "timestep:250, pyg_AUC: 0.4887\n",
      "timestep:251, pyg_AUC: 0.4873\n",
      "timestep:252, pyg_AUC: 0.4873\n",
      "timestep:253, pyg_AUC: 0.4901\n",
      "timestep:254, pyg_AUC: 0.4901\n",
      "timestep:255, pyg_AUC: 0.4859\n",
      "timestep:256, pyg_AUC: 0.4887\n",
      "timestep:257, pyg_AUC: 0.4873\n",
      "timestep:258, pyg_AUC: 0.4887\n",
      "timestep:259, pyg_AUC: 0.4859\n",
      "timestep:260, pyg_AUC: 0.4887\n",
      "timestep:261, pyg_AUC: 0.4901\n",
      "timestep:262, pyg_AUC: 0.4887\n",
      "timestep:263, pyg_AUC: 0.4901\n",
      "timestep:264, pyg_AUC: 0.4887\n",
      "timestep:265, pyg_AUC: 0.4859\n",
      "timestep:266, pyg_AUC: 0.4873\n",
      "timestep:267, pyg_AUC: 0.4859\n",
      "timestep:268, pyg_AUC: 0.4887\n",
      "timestep:269, pyg_AUC: 0.4873\n",
      "timestep:270, pyg_AUC: 0.4887\n",
      "timestep:271, pyg_AUC: 0.4873\n",
      "timestep:272, pyg_AUC: 0.4901\n",
      "timestep:273, pyg_AUC: 0.4859\n",
      "timestep:274, pyg_AUC: 0.4873\n",
      "timestep:275, pyg_AUC: 0.4915\n",
      "timestep:276, pyg_AUC: 0.4873\n",
      "timestep:277, pyg_AUC: 0.4901\n",
      "timestep:278, pyg_AUC: 0.4859\n",
      "timestep:279, pyg_AUC: 0.4873\n",
      "timestep:280, pyg_AUC: 0.4887\n",
      "timestep:281, pyg_AUC: 0.4845\n",
      "timestep:282, pyg_AUC: 0.4915\n",
      "timestep:283, pyg_AUC: 0.4873\n",
      "timestep:284, pyg_AUC: 0.4859\n",
      "timestep:285, pyg_AUC: 0.4873\n",
      "timestep:286, pyg_AUC: 0.4901\n",
      "timestep:287, pyg_AUC: 0.4915\n",
      "timestep:288, pyg_AUC: 0.4859\n",
      "timestep:289, pyg_AUC: 0.4859\n",
      "timestep:290, pyg_AUC: 0.4845\n",
      "timestep:291, pyg_AUC: 0.4873\n",
      "timestep:292, pyg_AUC: 0.4887\n",
      "timestep:293, pyg_AUC: 0.4887\n",
      "timestep:294, pyg_AUC: 0.4873\n",
      "timestep:295, pyg_AUC: 0.4845\n",
      "timestep:296, pyg_AUC: 0.4859\n",
      "timestep:297, pyg_AUC: 0.4873\n",
      "timestep:298, pyg_AUC: 0.4873\n",
      "timestep:299, pyg_AUC: 0.4887\n",
      "timestep:300, pyg_AUC: 0.4873\n",
      "timestep:301, pyg_AUC: 0.4845\n",
      "timestep:302, pyg_AUC: 0.4859\n",
      "timestep:303, pyg_AUC: 0.4873\n",
      "timestep:304, pyg_AUC: 0.4845\n",
      "timestep:305, pyg_AUC: 0.4887\n",
      "timestep:306, pyg_AUC: 0.4873\n",
      "timestep:307, pyg_AUC: 0.4845\n",
      "timestep:308, pyg_AUC: 0.4901\n",
      "timestep:309, pyg_AUC: 0.4873\n",
      "timestep:310, pyg_AUC: 0.4887\n",
      "timestep:311, pyg_AUC: 0.4873\n",
      "timestep:312, pyg_AUC: 0.4887\n",
      "timestep:313, pyg_AUC: 0.4873\n",
      "timestep:314, pyg_AUC: 0.4859\n",
      "timestep:315, pyg_AUC: 0.4901\n",
      "timestep:316, pyg_AUC: 0.4887\n",
      "timestep:317, pyg_AUC: 0.4859\n",
      "timestep:318, pyg_AUC: 0.4831\n",
      "timestep:319, pyg_AUC: 0.4887\n",
      "timestep:320, pyg_AUC: 0.4831\n",
      "timestep:321, pyg_AUC: 0.4873\n",
      "timestep:322, pyg_AUC: 0.4887\n",
      "timestep:323, pyg_AUC: 0.4901\n",
      "timestep:324, pyg_AUC: 0.4873\n",
      "timestep:325, pyg_AUC: 0.4873\n",
      "timestep:326, pyg_AUC: 0.4887\n",
      "timestep:327, pyg_AUC: 0.4901\n",
      "timestep:328, pyg_AUC: 0.4887\n",
      "timestep:329, pyg_AUC: 0.4873\n",
      "timestep:330, pyg_AUC: 0.4859\n",
      "timestep:331, pyg_AUC: 0.4859\n",
      "timestep:332, pyg_AUC: 0.4845\n",
      "timestep:333, pyg_AUC: 0.4887\n",
      "timestep:334, pyg_AUC: 0.4901\n",
      "timestep:335, pyg_AUC: 0.4859\n",
      "timestep:336, pyg_AUC: 0.4873\n",
      "timestep:337, pyg_AUC: 0.4873\n",
      "timestep:338, pyg_AUC: 0.4873\n",
      "timestep:339, pyg_AUC: 0.4887\n",
      "timestep:340, pyg_AUC: 0.4887\n",
      "timestep:341, pyg_AUC: 0.4873\n",
      "timestep:342, pyg_AUC: 0.4887\n",
      "timestep:343, pyg_AUC: 0.4859\n",
      "timestep:344, pyg_AUC: 0.4901\n",
      "timestep:345, pyg_AUC: 0.4831\n",
      "timestep:346, pyg_AUC: 0.4901\n",
      "timestep:347, pyg_AUC: 0.4887\n",
      "timestep:348, pyg_AUC: 0.4873\n",
      "timestep:349, pyg_AUC: 0.4887\n",
      "timestep:350, pyg_AUC: 0.4887\n",
      "timestep:351, pyg_AUC: 0.4859\n",
      "timestep:352, pyg_AUC: 0.4873\n",
      "timestep:353, pyg_AUC: 0.4873\n",
      "timestep:354, pyg_AUC: 0.4887\n",
      "timestep:355, pyg_AUC: 0.4887\n",
      "timestep:356, pyg_AUC: 0.4887\n",
      "timestep:357, pyg_AUC: 0.4901\n",
      "timestep:358, pyg_AUC: 0.4873\n",
      "timestep:359, pyg_AUC: 0.4873\n",
      "timestep:360, pyg_AUC: 0.4915\n",
      "timestep:361, pyg_AUC: 0.4859\n",
      "timestep:362, pyg_AUC: 0.4901\n",
      "timestep:363, pyg_AUC: 0.4901\n",
      "timestep:364, pyg_AUC: 0.4901\n",
      "timestep:365, pyg_AUC: 0.4901\n",
      "timestep:366, pyg_AUC: 0.4859\n",
      "timestep:367, pyg_AUC: 0.4915\n",
      "timestep:368, pyg_AUC: 0.4901\n",
      "timestep:369, pyg_AUC: 0.4915\n",
      "timestep:370, pyg_AUC: 0.4887\n",
      "timestep:371, pyg_AUC: 0.4901\n",
      "timestep:372, pyg_AUC: 0.4901\n",
      "timestep:373, pyg_AUC: 0.4859\n",
      "timestep:374, pyg_AUC: 0.4901\n",
      "timestep:375, pyg_AUC: 0.4901\n",
      "timestep:376, pyg_AUC: 0.4873\n",
      "timestep:377, pyg_AUC: 0.4887\n",
      "timestep:378, pyg_AUC: 0.4901\n",
      "timestep:379, pyg_AUC: 0.4901\n",
      "timestep:380, pyg_AUC: 0.4873\n",
      "timestep:381, pyg_AUC: 0.4901\n",
      "timestep:382, pyg_AUC: 0.4901\n",
      "timestep:383, pyg_AUC: 0.4873\n",
      "timestep:384, pyg_AUC: 0.4887\n",
      "timestep:385, pyg_AUC: 0.4873\n",
      "timestep:386, pyg_AUC: 0.4887\n",
      "timestep:387, pyg_AUC: 0.4873\n",
      "timestep:388, pyg_AUC: 0.4901\n",
      "timestep:389, pyg_AUC: 0.4887\n",
      "timestep:390, pyg_AUC: 0.4915\n",
      "timestep:391, pyg_AUC: 0.4901\n",
      "timestep:392, pyg_AUC: 0.4887\n",
      "timestep:393, pyg_AUC: 0.4915\n",
      "timestep:394, pyg_AUC: 0.4915\n",
      "timestep:395, pyg_AUC: 0.4887\n",
      "timestep:396, pyg_AUC: 0.4887\n",
      "timestep:397, pyg_AUC: 0.4901\n",
      "timestep:398, pyg_AUC: 0.4901\n",
      "timestep:399, pyg_AUC: 0.4901\n",
      "timestep:400, pyg_AUC: 0.4901\n",
      "timestep:401, pyg_AUC: 0.4915\n",
      "timestep:402, pyg_AUC: 0.4901\n",
      "timestep:403, pyg_AUC: 0.4915\n",
      "timestep:404, pyg_AUC: 0.4901\n",
      "timestep:405, pyg_AUC: 0.4901\n",
      "timestep:406, pyg_AUC: 0.4887\n",
      "timestep:407, pyg_AUC: 0.4901\n",
      "timestep:408, pyg_AUC: 0.4929\n",
      "timestep:409, pyg_AUC: 0.4915\n",
      "timestep:410, pyg_AUC: 0.4901\n",
      "timestep:411, pyg_AUC: 0.4887\n",
      "timestep:412, pyg_AUC: 0.4929\n",
      "timestep:413, pyg_AUC: 0.4915\n",
      "timestep:414, pyg_AUC: 0.4887\n",
      "timestep:415, pyg_AUC: 0.4887\n",
      "timestep:416, pyg_AUC: 0.4901\n",
      "timestep:417, pyg_AUC: 0.4929\n",
      "timestep:418, pyg_AUC: 0.4901\n",
      "timestep:419, pyg_AUC: 0.4873\n",
      "timestep:420, pyg_AUC: 0.4901\n",
      "timestep:421, pyg_AUC: 0.4915\n",
      "timestep:422, pyg_AUC: 0.4901\n",
      "timestep:423, pyg_AUC: 0.4887\n",
      "timestep:424, pyg_AUC: 0.4901\n",
      "timestep:425, pyg_AUC: 0.4887\n",
      "timestep:426, pyg_AUC: 0.4915\n",
      "timestep:427, pyg_AUC: 0.4873\n",
      "timestep:428, pyg_AUC: 0.4873\n",
      "timestep:429, pyg_AUC: 0.4873\n",
      "timestep:430, pyg_AUC: 0.4915\n",
      "timestep:431, pyg_AUC: 0.4901\n",
      "timestep:432, pyg_AUC: 0.4915\n",
      "timestep:433, pyg_AUC: 0.4901\n",
      "timestep:434, pyg_AUC: 0.4915\n",
      "timestep:435, pyg_AUC: 0.4915\n",
      "timestep:436, pyg_AUC: 0.4873\n",
      "timestep:437, pyg_AUC: 0.4915\n",
      "timestep:438, pyg_AUC: 0.4901\n",
      "timestep:439, pyg_AUC: 0.4901\n",
      "timestep:440, pyg_AUC: 0.4887\n",
      "timestep:441, pyg_AUC: 0.4873\n",
      "timestep:442, pyg_AUC: 0.4901\n",
      "timestep:443, pyg_AUC: 0.4887\n",
      "timestep:444, pyg_AUC: 0.4901\n",
      "timestep:445, pyg_AUC: 0.4873\n",
      "timestep:446, pyg_AUC: 0.4887\n",
      "timestep:447, pyg_AUC: 0.4887\n",
      "timestep:448, pyg_AUC: 0.4887\n",
      "timestep:449, pyg_AUC: 0.4901\n",
      "timestep:450, pyg_AUC: 0.4873\n",
      "timestep:451, pyg_AUC: 0.4915\n",
      "timestep:452, pyg_AUC: 0.4887\n",
      "timestep:453, pyg_AUC: 0.4915\n",
      "timestep:454, pyg_AUC: 0.4873\n",
      "timestep:455, pyg_AUC: 0.4915\n",
      "timestep:456, pyg_AUC: 0.4887\n",
      "timestep:457, pyg_AUC: 0.4901\n",
      "timestep:458, pyg_AUC: 0.4873\n",
      "timestep:459, pyg_AUC: 0.4845\n",
      "timestep:460, pyg_AUC: 0.4887\n",
      "timestep:461, pyg_AUC: 0.4873\n",
      "timestep:462, pyg_AUC: 0.4901\n",
      "timestep:463, pyg_AUC: 0.4887\n",
      "timestep:464, pyg_AUC: 0.4887\n",
      "timestep:465, pyg_AUC: 0.4887\n",
      "timestep:466, pyg_AUC: 0.4915\n",
      "timestep:467, pyg_AUC: 0.4901\n",
      "timestep:468, pyg_AUC: 0.4887\n",
      "timestep:469, pyg_AUC: 0.4887\n",
      "timestep:470, pyg_AUC: 0.4901\n",
      "timestep:471, pyg_AUC: 0.4901\n",
      "timestep:472, pyg_AUC: 0.4901\n",
      "timestep:473, pyg_AUC: 0.4873\n",
      "timestep:474, pyg_AUC: 0.4915\n",
      "timestep:475, pyg_AUC: 0.4901\n",
      "timestep:476, pyg_AUC: 0.4873\n",
      "timestep:477, pyg_AUC: 0.4887\n",
      "timestep:478, pyg_AUC: 0.4901\n",
      "timestep:479, pyg_AUC: 0.4901\n",
      "timestep:480, pyg_AUC: 0.4859\n",
      "timestep:481, pyg_AUC: 0.4901\n",
      "timestep:482, pyg_AUC: 0.4901\n",
      "timestep:483, pyg_AUC: 0.4887\n",
      "timestep:484, pyg_AUC: 0.4845\n",
      "timestep:485, pyg_AUC: 0.4873\n",
      "timestep:486, pyg_AUC: 0.4915\n",
      "timestep:487, pyg_AUC: 0.4901\n",
      "timestep:488, pyg_AUC: 0.4901\n",
      "timestep:489, pyg_AUC: 0.4915\n",
      "timestep:490, pyg_AUC: 0.4901\n",
      "timestep:491, pyg_AUC: 0.4901\n",
      "timestep:492, pyg_AUC: 0.4887\n",
      "timestep:493, pyg_AUC: 0.4915\n",
      "timestep:494, pyg_AUC: 0.4901\n",
      "timestep:495, pyg_AUC: 0.4915\n",
      "timestep:496, pyg_AUC: 0.4901\n",
      "timestep:497, pyg_AUC: 0.4901\n",
      "timestep:498, pyg_AUC: 0.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [01:39<31:25, 99.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:499, pyg_AUC: 0.4915\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 43.09970\n",
      "Epoch: 0010 loss= 23.89958\n",
      "Epoch: 0020 loss= 20.00839\n",
      "Epoch: 0030 loss= 15.99837\n",
      "Epoch: 0040 loss= 13.97786\n",
      "Epoch: 0050 loss= 8.65574\n",
      "Epoch: 0060 loss= 2.16946\n",
      "Epoch: 0070 loss= 0.96767\n",
      "Epoch: 0080 loss= 0.84541\n",
      "Epoch: 0090 loss= 0.72149\n",
      "Epoch: 0100 loss= 0.65871\n",
      "Epoch: 0110 loss= 0.61976\n",
      "Epoch: 0120 loss= 0.61856\n",
      "Epoch: 0130 loss= 0.66414\n",
      "Epoch: 0140 loss= 0.56016\n",
      "Epoch: 0150 loss= 0.60452\n",
      "Epoch: 0160 loss= 0.63460\n",
      "Epoch: 0170 loss= 0.57259\n",
      "Epoch: 0180 loss= 0.63114\n",
      "Epoch: 0190 loss= 0.60596\n",
      "Epoch: 0200 loss= 0.58978\n",
      "Epoch: 0210 loss= 0.56615\n",
      "Epoch: 0220 loss= 0.60732\n",
      "Epoch: 0230 loss= 0.53996\n",
      "Epoch: 0240 loss= 0.63378\n",
      "Epoch: 0250 loss= 0.61328\n",
      "Epoch: 0260 loss= 0.61691\n",
      "Epoch: 0270 loss= 0.61950\n",
      "Epoch: 0280 loss= 0.54428\n",
      "Epoch: 0290 loss= 0.58976\n",
      "Epoch: 0300 loss= 0.52828\n",
      "Epoch: 0310 loss= 0.56554\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7810,  4.7166, -5.3294, -4.6056, -4.4004,  5.7110,  5.4835, -5.2087]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 41.66368\n",
      "Epoch: 0010 loss= 27.37669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0020 loss= 16.06802\n",
      "Epoch: 0030 loss= 10.82818\n",
      "Epoch: 0040 loss= 3.66511\n",
      "Epoch: 0050 loss= 1.80205\n",
      "Epoch: 0060 loss= 1.07684\n",
      "Epoch: 0070 loss= 1.02895\n",
      "Epoch: 0080 loss= 0.99625\n",
      "Epoch: 0090 loss= 0.60326\n",
      "Epoch: 0100 loss= 0.62781\n",
      "Epoch: 0110 loss= 0.54011\n",
      "Epoch: 0120 loss= 0.64524\n",
      "Epoch: 0130 loss= 0.66701\n",
      "Epoch: 0140 loss= 0.59699\n",
      "Epoch: 0150 loss= 0.58893\n",
      "Epoch: 0160 loss= 0.61192\n",
      "Epoch: 0170 loss= 0.61490\n",
      "Epoch: 0180 loss= 0.58743\n",
      "Epoch: 0190 loss= 0.57469\n",
      "Epoch: 0200 loss= 0.63438\n",
      "Epoch: 0210 loss= 0.55815\n",
      "Epoch: 0220 loss= 0.58641\n",
      "Epoch: 0230 loss= 0.59232\n",
      "Epoch: 0240 loss= 0.55423\n",
      "Epoch: 0250 loss= 0.58151\n",
      "Epoch: 0260 loss= 0.58581\n",
      "Epoch: 0270 loss= 0.52504\n",
      "Epoch: 0280 loss= 0.57083\n",
      "Epoch: 0290 loss= 0.60417\n",
      "Epoch: 0300 loss= 0.53191\n",
      "Epoch: 0310 loss= 0.58192\n",
      "Epoch: 0320 loss= 0.57072\n",
      "Epoch: 0330 loss= 0.53715\n",
      "Epoch: 0340 loss= 0.61432\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4944\n",
      "timestep:1, pyg_AUC: 0.4873\n",
      "timestep:2, pyg_AUC: 0.4845\n",
      "timestep:3, pyg_AUC: 0.4873\n",
      "timestep:4, pyg_AUC: 0.4873\n",
      "timestep:5, pyg_AUC: 0.4887\n",
      "timestep:6, pyg_AUC: 0.4901\n",
      "timestep:7, pyg_AUC: 0.4901\n",
      "timestep:8, pyg_AUC: 0.4958\n",
      "timestep:9, pyg_AUC: 0.4915\n",
      "timestep:10, pyg_AUC: 0.4915\n",
      "timestep:11, pyg_AUC: 0.4859\n",
      "timestep:12, pyg_AUC: 0.4901\n",
      "timestep:13, pyg_AUC: 0.4901\n",
      "timestep:14, pyg_AUC: 0.4915\n",
      "timestep:15, pyg_AUC: 0.4887\n",
      "timestep:16, pyg_AUC: 0.4887\n",
      "timestep:17, pyg_AUC: 0.4958\n",
      "timestep:18, pyg_AUC: 0.4915\n",
      "timestep:19, pyg_AUC: 0.4915\n",
      "timestep:20, pyg_AUC: 0.4915\n",
      "timestep:21, pyg_AUC: 0.4887\n",
      "timestep:22, pyg_AUC: 0.4859\n",
      "timestep:23, pyg_AUC: 0.4845\n",
      "timestep:24, pyg_AUC: 0.4915\n",
      "timestep:25, pyg_AUC: 0.4901\n",
      "timestep:26, pyg_AUC: 0.4901\n",
      "timestep:27, pyg_AUC: 0.4873\n",
      "timestep:28, pyg_AUC: 0.4901\n",
      "timestep:29, pyg_AUC: 0.4873\n",
      "timestep:30, pyg_AUC: 0.4915\n",
      "timestep:31, pyg_AUC: 0.4901\n",
      "timestep:32, pyg_AUC: 0.4915\n",
      "timestep:33, pyg_AUC: 0.4929\n",
      "timestep:34, pyg_AUC: 0.4929\n",
      "timestep:35, pyg_AUC: 0.4929\n",
      "timestep:36, pyg_AUC: 0.4901\n",
      "timestep:37, pyg_AUC: 0.4915\n",
      "timestep:38, pyg_AUC: 0.4887\n",
      "timestep:39, pyg_AUC: 0.4887\n",
      "timestep:40, pyg_AUC: 0.4831\n",
      "timestep:41, pyg_AUC: 0.4887\n",
      "timestep:42, pyg_AUC: 0.4915\n",
      "timestep:43, pyg_AUC: 0.4873\n",
      "timestep:44, pyg_AUC: 0.4873\n",
      "timestep:45, pyg_AUC: 0.4887\n",
      "timestep:46, pyg_AUC: 0.4859\n",
      "timestep:47, pyg_AUC: 0.4887\n",
      "timestep:48, pyg_AUC: 0.4915\n",
      "timestep:49, pyg_AUC: 0.4873\n",
      "timestep:50, pyg_AUC: 0.4887\n",
      "timestep:51, pyg_AUC: 0.4929\n",
      "timestep:52, pyg_AUC: 0.4915\n",
      "timestep:53, pyg_AUC: 0.4944\n",
      "timestep:54, pyg_AUC: 0.4901\n",
      "timestep:55, pyg_AUC: 0.4901\n",
      "timestep:56, pyg_AUC: 0.4887\n",
      "timestep:57, pyg_AUC: 0.4929\n",
      "timestep:58, pyg_AUC: 0.4901\n",
      "timestep:59, pyg_AUC: 0.4873\n",
      "timestep:60, pyg_AUC: 0.4873\n",
      "timestep:61, pyg_AUC: 0.4859\n",
      "timestep:62, pyg_AUC: 0.4887\n",
      "timestep:63, pyg_AUC: 0.4887\n",
      "timestep:64, pyg_AUC: 0.4873\n",
      "timestep:65, pyg_AUC: 0.4915\n",
      "timestep:66, pyg_AUC: 0.4915\n",
      "timestep:67, pyg_AUC: 0.4901\n",
      "timestep:68, pyg_AUC: 0.4873\n",
      "timestep:69, pyg_AUC: 0.4901\n",
      "timestep:70, pyg_AUC: 0.4901\n",
      "timestep:71, pyg_AUC: 0.4873\n",
      "timestep:72, pyg_AUC: 0.4915\n",
      "timestep:73, pyg_AUC: 0.4901\n",
      "timestep:74, pyg_AUC: 0.4944\n",
      "timestep:75, pyg_AUC: 0.4915\n",
      "timestep:76, pyg_AUC: 0.4901\n",
      "timestep:77, pyg_AUC: 0.4929\n",
      "timestep:78, pyg_AUC: 0.4901\n",
      "timestep:79, pyg_AUC: 0.4859\n",
      "timestep:80, pyg_AUC: 0.4859\n",
      "timestep:81, pyg_AUC: 0.4859\n",
      "timestep:82, pyg_AUC: 0.4887\n",
      "timestep:83, pyg_AUC: 0.4873\n",
      "timestep:84, pyg_AUC: 0.4915\n",
      "timestep:85, pyg_AUC: 0.4929\n",
      "timestep:86, pyg_AUC: 0.4887\n",
      "timestep:87, pyg_AUC: 0.4901\n",
      "timestep:88, pyg_AUC: 0.4887\n",
      "timestep:89, pyg_AUC: 0.4845\n",
      "timestep:90, pyg_AUC: 0.4873\n",
      "timestep:91, pyg_AUC: 0.4831\n",
      "timestep:92, pyg_AUC: 0.4845\n",
      "timestep:93, pyg_AUC: 0.4887\n",
      "timestep:94, pyg_AUC: 0.4845\n",
      "timestep:95, pyg_AUC: 0.4873\n",
      "timestep:96, pyg_AUC: 0.4873\n",
      "timestep:97, pyg_AUC: 0.4887\n",
      "timestep:98, pyg_AUC: 0.4944\n",
      "timestep:99, pyg_AUC: 0.4901\n",
      "timestep:100, pyg_AUC: 0.4901\n",
      "timestep:101, pyg_AUC: 0.4929\n",
      "timestep:102, pyg_AUC: 0.4944\n",
      "timestep:103, pyg_AUC: 0.4958\n",
      "timestep:104, pyg_AUC: 0.4887\n",
      "timestep:105, pyg_AUC: 0.4887\n",
      "timestep:106, pyg_AUC: 0.4887\n",
      "timestep:107, pyg_AUC: 0.4845\n",
      "timestep:108, pyg_AUC: 0.4958\n",
      "timestep:109, pyg_AUC: 0.4915\n",
      "timestep:110, pyg_AUC: 0.4915\n",
      "timestep:111, pyg_AUC: 0.4929\n",
      "timestep:112, pyg_AUC: 0.4873\n",
      "timestep:113, pyg_AUC: 0.4845\n",
      "timestep:114, pyg_AUC: 0.4859\n",
      "timestep:115, pyg_AUC: 0.4845\n",
      "timestep:116, pyg_AUC: 0.4958\n",
      "timestep:117, pyg_AUC: 0.4901\n",
      "timestep:118, pyg_AUC: 0.4958\n",
      "timestep:119, pyg_AUC: 0.4859\n",
      "timestep:120, pyg_AUC: 0.4915\n",
      "timestep:121, pyg_AUC: 0.4944\n",
      "timestep:122, pyg_AUC: 0.4873\n",
      "timestep:123, pyg_AUC: 0.4929\n",
      "timestep:124, pyg_AUC: 0.4958\n",
      "timestep:125, pyg_AUC: 0.4929\n",
      "timestep:126, pyg_AUC: 0.4831\n",
      "timestep:127, pyg_AUC: 0.4958\n",
      "timestep:128, pyg_AUC: 0.4901\n",
      "timestep:129, pyg_AUC: 0.4802\n",
      "timestep:130, pyg_AUC: 0.4915\n",
      "timestep:131, pyg_AUC: 0.4901\n",
      "timestep:132, pyg_AUC: 0.4915\n",
      "timestep:133, pyg_AUC: 0.4901\n",
      "timestep:134, pyg_AUC: 0.4887\n",
      "timestep:135, pyg_AUC: 0.4958\n",
      "timestep:136, pyg_AUC: 0.4901\n",
      "timestep:137, pyg_AUC: 0.4901\n",
      "timestep:138, pyg_AUC: 0.4929\n",
      "timestep:139, pyg_AUC: 0.4901\n",
      "timestep:140, pyg_AUC: 0.4873\n",
      "timestep:141, pyg_AUC: 0.4986\n",
      "timestep:142, pyg_AUC: 0.4944\n",
      "timestep:143, pyg_AUC: 0.4845\n",
      "timestep:144, pyg_AUC: 0.4944\n",
      "timestep:145, pyg_AUC: 0.4944\n",
      "timestep:146, pyg_AUC: 0.4859\n",
      "timestep:147, pyg_AUC: 0.4887\n",
      "timestep:148, pyg_AUC: 0.4915\n",
      "timestep:149, pyg_AUC: 0.4901\n",
      "timestep:150, pyg_AUC: 0.4901\n",
      "timestep:151, pyg_AUC: 0.4944\n",
      "timestep:152, pyg_AUC: 0.4901\n",
      "timestep:153, pyg_AUC: 0.4859\n",
      "timestep:154, pyg_AUC: 0.4901\n",
      "timestep:155, pyg_AUC: 0.4929\n",
      "timestep:156, pyg_AUC: 0.4944\n",
      "timestep:157, pyg_AUC: 0.4915\n",
      "timestep:158, pyg_AUC: 0.4915\n",
      "timestep:159, pyg_AUC: 0.4901\n",
      "timestep:160, pyg_AUC: 0.4915\n",
      "timestep:161, pyg_AUC: 0.4944\n",
      "timestep:162, pyg_AUC: 0.4901\n",
      "timestep:163, pyg_AUC: 0.4901\n",
      "timestep:164, pyg_AUC: 0.4915\n",
      "timestep:165, pyg_AUC: 0.4901\n",
      "timestep:166, pyg_AUC: 0.4915\n",
      "timestep:167, pyg_AUC: 0.4929\n",
      "timestep:168, pyg_AUC: 0.4887\n",
      "timestep:169, pyg_AUC: 0.4915\n",
      "timestep:170, pyg_AUC: 0.4859\n",
      "timestep:171, pyg_AUC: 0.4901\n",
      "timestep:172, pyg_AUC: 0.4915\n",
      "timestep:173, pyg_AUC: 0.4929\n",
      "timestep:174, pyg_AUC: 0.4873\n",
      "timestep:175, pyg_AUC: 0.4929\n",
      "timestep:176, pyg_AUC: 0.4929\n",
      "timestep:177, pyg_AUC: 0.4929\n",
      "timestep:178, pyg_AUC: 0.4873\n",
      "timestep:179, pyg_AUC: 0.4929\n",
      "timestep:180, pyg_AUC: 0.4929\n",
      "timestep:181, pyg_AUC: 0.4915\n",
      "timestep:182, pyg_AUC: 0.4873\n",
      "timestep:183, pyg_AUC: 0.4929\n",
      "timestep:184, pyg_AUC: 0.4929\n",
      "timestep:185, pyg_AUC: 0.4929\n",
      "timestep:186, pyg_AUC: 0.4887\n",
      "timestep:187, pyg_AUC: 0.4887\n",
      "timestep:188, pyg_AUC: 0.4901\n",
      "timestep:189, pyg_AUC: 0.4944\n",
      "timestep:190, pyg_AUC: 0.4901\n",
      "timestep:191, pyg_AUC: 0.4915\n",
      "timestep:192, pyg_AUC: 0.4944\n",
      "timestep:193, pyg_AUC: 0.4929\n",
      "timestep:194, pyg_AUC: 0.4901\n",
      "timestep:195, pyg_AUC: 0.4929\n",
      "timestep:196, pyg_AUC: 0.4929\n",
      "timestep:197, pyg_AUC: 0.4915\n",
      "timestep:198, pyg_AUC: 0.4929\n",
      "timestep:199, pyg_AUC: 0.4901\n",
      "timestep:200, pyg_AUC: 0.4901\n",
      "timestep:201, pyg_AUC: 0.4915\n",
      "timestep:202, pyg_AUC: 0.4929\n",
      "timestep:203, pyg_AUC: 0.4929\n",
      "timestep:204, pyg_AUC: 0.4915\n",
      "timestep:205, pyg_AUC: 0.4887\n",
      "timestep:206, pyg_AUC: 0.4929\n",
      "timestep:207, pyg_AUC: 0.4901\n",
      "timestep:208, pyg_AUC: 0.4915\n",
      "timestep:209, pyg_AUC: 0.4901\n",
      "timestep:210, pyg_AUC: 0.4887\n",
      "timestep:211, pyg_AUC: 0.4929\n",
      "timestep:212, pyg_AUC: 0.4901\n",
      "timestep:213, pyg_AUC: 0.4944\n",
      "timestep:214, pyg_AUC: 0.4901\n",
      "timestep:215, pyg_AUC: 0.4929\n",
      "timestep:216, pyg_AUC: 0.4929\n",
      "timestep:217, pyg_AUC: 0.4901\n",
      "timestep:218, pyg_AUC: 0.4944\n",
      "timestep:219, pyg_AUC: 0.4859\n",
      "timestep:220, pyg_AUC: 0.4873\n",
      "timestep:221, pyg_AUC: 0.4929\n",
      "timestep:222, pyg_AUC: 0.4915\n",
      "timestep:223, pyg_AUC: 0.4915\n",
      "timestep:224, pyg_AUC: 0.4887\n",
      "timestep:225, pyg_AUC: 0.4915\n",
      "timestep:226, pyg_AUC: 0.4901\n",
      "timestep:227, pyg_AUC: 0.4887\n",
      "timestep:228, pyg_AUC: 0.4887\n",
      "timestep:229, pyg_AUC: 0.4915\n",
      "timestep:230, pyg_AUC: 0.4915\n",
      "timestep:231, pyg_AUC: 0.4887\n",
      "timestep:232, pyg_AUC: 0.4901\n",
      "timestep:233, pyg_AUC: 0.4901\n",
      "timestep:234, pyg_AUC: 0.4915\n",
      "timestep:235, pyg_AUC: 0.4915\n",
      "timestep:236, pyg_AUC: 0.4887\n",
      "timestep:237, pyg_AUC: 0.4929\n",
      "timestep:238, pyg_AUC: 0.4873\n",
      "timestep:239, pyg_AUC: 0.4873\n",
      "timestep:240, pyg_AUC: 0.4901\n",
      "timestep:241, pyg_AUC: 0.4901\n",
      "timestep:242, pyg_AUC: 0.4915\n",
      "timestep:243, pyg_AUC: 0.4887\n",
      "timestep:244, pyg_AUC: 0.4901\n",
      "timestep:245, pyg_AUC: 0.4873\n",
      "timestep:246, pyg_AUC: 0.4901\n",
      "timestep:247, pyg_AUC: 0.4915\n",
      "timestep:248, pyg_AUC: 0.4915\n",
      "timestep:249, pyg_AUC: 0.4887\n",
      "timestep:250, pyg_AUC: 0.4915\n",
      "timestep:251, pyg_AUC: 0.4901\n",
      "timestep:252, pyg_AUC: 0.4873\n",
      "timestep:253, pyg_AUC: 0.4915\n",
      "timestep:254, pyg_AUC: 0.4901\n",
      "timestep:255, pyg_AUC: 0.4929\n",
      "timestep:256, pyg_AUC: 0.4887\n",
      "timestep:257, pyg_AUC: 0.4887\n",
      "timestep:258, pyg_AUC: 0.4901\n",
      "timestep:259, pyg_AUC: 0.4901\n",
      "timestep:260, pyg_AUC: 0.4887\n",
      "timestep:261, pyg_AUC: 0.4901\n",
      "timestep:262, pyg_AUC: 0.4887\n",
      "timestep:263, pyg_AUC: 0.4915\n",
      "timestep:264, pyg_AUC: 0.4887\n",
      "timestep:265, pyg_AUC: 0.4901\n",
      "timestep:266, pyg_AUC: 0.4873\n",
      "timestep:267, pyg_AUC: 0.4887\n",
      "timestep:268, pyg_AUC: 0.4915\n",
      "timestep:269, pyg_AUC: 0.4859\n",
      "timestep:270, pyg_AUC: 0.4887\n",
      "timestep:271, pyg_AUC: 0.4859\n",
      "timestep:272, pyg_AUC: 0.4873\n",
      "timestep:273, pyg_AUC: 0.4873\n",
      "timestep:274, pyg_AUC: 0.4915\n",
      "timestep:275, pyg_AUC: 0.4859\n",
      "timestep:276, pyg_AUC: 0.4873\n",
      "timestep:277, pyg_AUC: 0.4901\n",
      "timestep:278, pyg_AUC: 0.4915\n",
      "timestep:279, pyg_AUC: 0.4901\n",
      "timestep:280, pyg_AUC: 0.4929\n",
      "timestep:281, pyg_AUC: 0.4845\n",
      "timestep:282, pyg_AUC: 0.4915\n",
      "timestep:283, pyg_AUC: 0.4845\n",
      "timestep:284, pyg_AUC: 0.4901\n",
      "timestep:285, pyg_AUC: 0.4873\n",
      "timestep:286, pyg_AUC: 0.4859\n",
      "timestep:287, pyg_AUC: 0.4901\n",
      "timestep:288, pyg_AUC: 0.4915\n",
      "timestep:289, pyg_AUC: 0.4873\n",
      "timestep:290, pyg_AUC: 0.4901\n",
      "timestep:291, pyg_AUC: 0.4915\n",
      "timestep:292, pyg_AUC: 0.4901\n",
      "timestep:293, pyg_AUC: 0.4915\n",
      "timestep:294, pyg_AUC: 0.4859\n",
      "timestep:295, pyg_AUC: 0.4944\n",
      "timestep:296, pyg_AUC: 0.4873\n",
      "timestep:297, pyg_AUC: 0.4887\n",
      "timestep:298, pyg_AUC: 0.4887\n",
      "timestep:299, pyg_AUC: 0.4887\n",
      "timestep:300, pyg_AUC: 0.4901\n",
      "timestep:301, pyg_AUC: 0.4887\n",
      "timestep:302, pyg_AUC: 0.4901\n",
      "timestep:303, pyg_AUC: 0.4873\n",
      "timestep:304, pyg_AUC: 0.4901\n",
      "timestep:305, pyg_AUC: 0.4845\n",
      "timestep:306, pyg_AUC: 0.4873\n",
      "timestep:307, pyg_AUC: 0.4887\n",
      "timestep:308, pyg_AUC: 0.4915\n",
      "timestep:309, pyg_AUC: 0.4887\n",
      "timestep:310, pyg_AUC: 0.4873\n",
      "timestep:311, pyg_AUC: 0.4901\n",
      "timestep:312, pyg_AUC: 0.4887\n",
      "timestep:313, pyg_AUC: 0.4944\n",
      "timestep:314, pyg_AUC: 0.4859\n",
      "timestep:315, pyg_AUC: 0.4887\n",
      "timestep:316, pyg_AUC: 0.4887\n",
      "timestep:317, pyg_AUC: 0.4887\n",
      "timestep:318, pyg_AUC: 0.4901\n",
      "timestep:319, pyg_AUC: 0.4901\n",
      "timestep:320, pyg_AUC: 0.4873\n",
      "timestep:321, pyg_AUC: 0.4887\n",
      "timestep:322, pyg_AUC: 0.4887\n",
      "timestep:323, pyg_AUC: 0.4831\n",
      "timestep:324, pyg_AUC: 0.4901\n",
      "timestep:325, pyg_AUC: 0.4901\n",
      "timestep:326, pyg_AUC: 0.4873\n",
      "timestep:327, pyg_AUC: 0.4929\n",
      "timestep:328, pyg_AUC: 0.4929\n",
      "timestep:329, pyg_AUC: 0.4901\n",
      "timestep:330, pyg_AUC: 0.4887\n",
      "timestep:331, pyg_AUC: 0.4845\n",
      "timestep:332, pyg_AUC: 0.4845\n",
      "timestep:333, pyg_AUC: 0.4901\n",
      "timestep:334, pyg_AUC: 0.4887\n",
      "timestep:335, pyg_AUC: 0.4901\n",
      "timestep:336, pyg_AUC: 0.4873\n",
      "timestep:337, pyg_AUC: 0.4901\n",
      "timestep:338, pyg_AUC: 0.4929\n",
      "timestep:339, pyg_AUC: 0.4873\n",
      "timestep:340, pyg_AUC: 0.4901\n",
      "timestep:341, pyg_AUC: 0.4915\n",
      "timestep:342, pyg_AUC: 0.4845\n",
      "timestep:343, pyg_AUC: 0.4887\n",
      "timestep:344, pyg_AUC: 0.4901\n",
      "timestep:345, pyg_AUC: 0.4873\n",
      "timestep:346, pyg_AUC: 0.4859\n",
      "timestep:347, pyg_AUC: 0.4845\n",
      "timestep:348, pyg_AUC: 0.4887\n",
      "timestep:349, pyg_AUC: 0.4873\n",
      "timestep:350, pyg_AUC: 0.4915\n",
      "timestep:351, pyg_AUC: 0.4901\n",
      "timestep:352, pyg_AUC: 0.4944\n",
      "timestep:353, pyg_AUC: 0.4887\n",
      "timestep:354, pyg_AUC: 0.4901\n",
      "timestep:355, pyg_AUC: 0.4859\n",
      "timestep:356, pyg_AUC: 0.4901\n",
      "timestep:357, pyg_AUC: 0.4859\n",
      "timestep:358, pyg_AUC: 0.4859\n",
      "timestep:359, pyg_AUC: 0.4901\n",
      "timestep:360, pyg_AUC: 0.4887\n",
      "timestep:361, pyg_AUC: 0.4887\n",
      "timestep:362, pyg_AUC: 0.4915\n",
      "timestep:363, pyg_AUC: 0.4887\n",
      "timestep:364, pyg_AUC: 0.4901\n",
      "timestep:365, pyg_AUC: 0.4873\n",
      "timestep:366, pyg_AUC: 0.4887\n",
      "timestep:367, pyg_AUC: 0.4887\n",
      "timestep:368, pyg_AUC: 0.4901\n",
      "timestep:369, pyg_AUC: 0.4873\n",
      "timestep:370, pyg_AUC: 0.4887\n",
      "timestep:371, pyg_AUC: 0.4859\n",
      "timestep:372, pyg_AUC: 0.4873\n",
      "timestep:373, pyg_AUC: 0.4901\n",
      "timestep:374, pyg_AUC: 0.4887\n",
      "timestep:375, pyg_AUC: 0.4887\n",
      "timestep:376, pyg_AUC: 0.4915\n",
      "timestep:377, pyg_AUC: 0.4901\n",
      "timestep:378, pyg_AUC: 0.4915\n",
      "timestep:379, pyg_AUC: 0.4873\n",
      "timestep:380, pyg_AUC: 0.4915\n",
      "timestep:381, pyg_AUC: 0.4845\n",
      "timestep:382, pyg_AUC: 0.4845\n",
      "timestep:383, pyg_AUC: 0.4873\n",
      "timestep:384, pyg_AUC: 0.4873\n",
      "timestep:385, pyg_AUC: 0.4887\n",
      "timestep:386, pyg_AUC: 0.4887\n",
      "timestep:387, pyg_AUC: 0.4901\n",
      "timestep:388, pyg_AUC: 0.4887\n",
      "timestep:389, pyg_AUC: 0.4887\n",
      "timestep:390, pyg_AUC: 0.4929\n",
      "timestep:391, pyg_AUC: 0.4901\n",
      "timestep:392, pyg_AUC: 0.4887\n",
      "timestep:393, pyg_AUC: 0.4887\n",
      "timestep:394, pyg_AUC: 0.4915\n",
      "timestep:395, pyg_AUC: 0.4887\n",
      "timestep:396, pyg_AUC: 0.4887\n",
      "timestep:397, pyg_AUC: 0.4873\n",
      "timestep:398, pyg_AUC: 0.4915\n",
      "timestep:399, pyg_AUC: 0.4901\n",
      "timestep:400, pyg_AUC: 0.4915\n",
      "timestep:401, pyg_AUC: 0.4915\n",
      "timestep:402, pyg_AUC: 0.4915\n",
      "timestep:403, pyg_AUC: 0.4887\n",
      "timestep:404, pyg_AUC: 0.4901\n",
      "timestep:405, pyg_AUC: 0.4873\n",
      "timestep:406, pyg_AUC: 0.4873\n",
      "timestep:407, pyg_AUC: 0.4887\n",
      "timestep:408, pyg_AUC: 0.4887\n",
      "timestep:409, pyg_AUC: 0.4901\n",
      "timestep:410, pyg_AUC: 0.4873\n",
      "timestep:411, pyg_AUC: 0.4901\n",
      "timestep:412, pyg_AUC: 0.4901\n",
      "timestep:413, pyg_AUC: 0.4915\n",
      "timestep:414, pyg_AUC: 0.4901\n",
      "timestep:415, pyg_AUC: 0.4915\n",
      "timestep:416, pyg_AUC: 0.4887\n",
      "timestep:417, pyg_AUC: 0.4901\n",
      "timestep:418, pyg_AUC: 0.4873\n",
      "timestep:419, pyg_AUC: 0.4915\n",
      "timestep:420, pyg_AUC: 0.4873\n",
      "timestep:421, pyg_AUC: 0.4873\n",
      "timestep:422, pyg_AUC: 0.4887\n",
      "timestep:423, pyg_AUC: 0.4873\n",
      "timestep:424, pyg_AUC: 0.4915\n",
      "timestep:425, pyg_AUC: 0.4873\n",
      "timestep:426, pyg_AUC: 0.4887\n",
      "timestep:427, pyg_AUC: 0.4915\n",
      "timestep:428, pyg_AUC: 0.4915\n",
      "timestep:429, pyg_AUC: 0.4915\n",
      "timestep:430, pyg_AUC: 0.4915\n",
      "timestep:431, pyg_AUC: 0.4873\n",
      "timestep:432, pyg_AUC: 0.4901\n",
      "timestep:433, pyg_AUC: 0.4901\n",
      "timestep:434, pyg_AUC: 0.4901\n",
      "timestep:435, pyg_AUC: 0.4887\n",
      "timestep:436, pyg_AUC: 0.4901\n",
      "timestep:437, pyg_AUC: 0.4915\n",
      "timestep:438, pyg_AUC: 0.4901\n",
      "timestep:439, pyg_AUC: 0.4859\n",
      "timestep:440, pyg_AUC: 0.4887\n",
      "timestep:441, pyg_AUC: 0.4915\n",
      "timestep:442, pyg_AUC: 0.4859\n",
      "timestep:443, pyg_AUC: 0.4901\n",
      "timestep:444, pyg_AUC: 0.4873\n",
      "timestep:445, pyg_AUC: 0.4915\n",
      "timestep:446, pyg_AUC: 0.4901\n",
      "timestep:447, pyg_AUC: 0.4915\n",
      "timestep:448, pyg_AUC: 0.4859\n",
      "timestep:449, pyg_AUC: 0.4915\n",
      "timestep:450, pyg_AUC: 0.4901\n",
      "timestep:451, pyg_AUC: 0.4901\n",
      "timestep:452, pyg_AUC: 0.4929\n",
      "timestep:453, pyg_AUC: 0.4929\n",
      "timestep:454, pyg_AUC: 0.4901\n",
      "timestep:455, pyg_AUC: 0.4901\n",
      "timestep:456, pyg_AUC: 0.4944\n",
      "timestep:457, pyg_AUC: 0.4887\n",
      "timestep:458, pyg_AUC: 0.4845\n",
      "timestep:459, pyg_AUC: 0.4915\n",
      "timestep:460, pyg_AUC: 0.4901\n",
      "timestep:461, pyg_AUC: 0.4887\n",
      "timestep:462, pyg_AUC: 0.4901\n",
      "timestep:463, pyg_AUC: 0.4929\n",
      "timestep:464, pyg_AUC: 0.4901\n",
      "timestep:465, pyg_AUC: 0.4887\n",
      "timestep:466, pyg_AUC: 0.4859\n",
      "timestep:467, pyg_AUC: 0.4915\n",
      "timestep:468, pyg_AUC: 0.4859\n",
      "timestep:469, pyg_AUC: 0.4887\n",
      "timestep:470, pyg_AUC: 0.4887\n",
      "timestep:471, pyg_AUC: 0.4901\n",
      "timestep:472, pyg_AUC: 0.4915\n",
      "timestep:473, pyg_AUC: 0.4873\n",
      "timestep:474, pyg_AUC: 0.4887\n",
      "timestep:475, pyg_AUC: 0.4859\n",
      "timestep:476, pyg_AUC: 0.4873\n",
      "timestep:477, pyg_AUC: 0.4901\n",
      "timestep:478, pyg_AUC: 0.4901\n",
      "timestep:479, pyg_AUC: 0.4873\n",
      "timestep:480, pyg_AUC: 0.4901\n",
      "timestep:481, pyg_AUC: 0.4887\n",
      "timestep:482, pyg_AUC: 0.4901\n",
      "timestep:483, pyg_AUC: 0.4929\n",
      "timestep:484, pyg_AUC: 0.4915\n",
      "timestep:485, pyg_AUC: 0.4901\n",
      "timestep:486, pyg_AUC: 0.4901\n",
      "timestep:487, pyg_AUC: 0.4915\n",
      "timestep:488, pyg_AUC: 0.4901\n",
      "timestep:489, pyg_AUC: 0.4873\n",
      "timestep:490, pyg_AUC: 0.4901\n",
      "timestep:491, pyg_AUC: 0.4915\n",
      "timestep:492, pyg_AUC: 0.4915\n",
      "timestep:493, pyg_AUC: 0.4915\n",
      "timestep:494, pyg_AUC: 0.4901\n",
      "timestep:495, pyg_AUC: 0.4901\n",
      "timestep:496, pyg_AUC: 0.4915\n",
      "timestep:497, pyg_AUC: 0.4915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [03:18<29:47, 99.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:498, pyg_AUC: 0.4887\n",
      "timestep:499, pyg_AUC: 0.4915\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 34.30563\n",
      "Epoch: 0010 loss= 24.52308\n",
      "Epoch: 0020 loss= 18.01850\n",
      "Epoch: 0030 loss= 15.73822\n",
      "Epoch: 0040 loss= 14.46130\n",
      "Epoch: 0050 loss= 8.25099\n",
      "Epoch: 0060 loss= 1.74654\n",
      "Epoch: 0070 loss= 1.25022\n",
      "Epoch: 0080 loss= 0.74627\n",
      "Epoch: 0090 loss= 0.62320\n",
      "Epoch: 0100 loss= 0.77344\n",
      "Epoch: 0110 loss= 0.75890\n",
      "Epoch: 0120 loss= 0.60660\n",
      "Epoch: 0130 loss= 0.70672\n",
      "Epoch: 0140 loss= 0.64233\n",
      "Epoch: 0150 loss= 0.66480\n",
      "Epoch: 0160 loss= 0.68096\n",
      "Epoch: 0170 loss= 0.57385\n",
      "Epoch: 0180 loss= 0.59245\n",
      "Epoch: 0190 loss= 0.73376\n",
      "Epoch: 0200 loss= 0.61577\n",
      "Epoch: 0210 loss= 0.55745\n",
      "Epoch: 0220 loss= 0.61199\n",
      "Epoch: 0230 loss= 0.57383\n",
      "Epoch: 0240 loss= 0.63540\n",
      "Epoch: 0250 loss= 0.57566\n",
      "Epoch: 0260 loss= 0.52862\n",
      "Epoch: 0270 loss= 0.60758\n",
      "Epoch: 0280 loss= 0.53857\n",
      "Epoch: 0290 loss= 0.54783\n",
      "Epoch: 0300 loss= 0.55545\n",
      "Epoch: 0310 loss= 0.59615\n",
      "Epoch: 0320 loss= 0.54385\n",
      "Epoch: 0330 loss= 0.46250\n",
      "Epoch: 0340 loss= 0.55648\n",
      "Epoch: 0350 loss= 0.59390\n",
      "Epoch: 0360 loss= 0.57136\n",
      "Epoch: 0370 loss= 0.62707\n",
      "Epoch: 0380 loss= 0.58189\n",
      "Epoch: 0390 loss= 0.53766\n",
      "Epoch: 0400 loss= 0.55293\n",
      "Epoch: 0410 loss= 0.64215\n",
      "Epoch: 0420 loss= 0.55334\n",
      "Epoch: 0430 loss= 0.51863\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.8248,  4.7490, -5.3643, -4.6454, -4.3984,  5.6777,  5.4918, -5.2754]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 36.09566\n",
      "Epoch: 0010 loss= 27.58291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0020 loss= 17.71773\n",
      "Epoch: 0030 loss= 8.76839\n",
      "Epoch: 0040 loss= 1.72248\n",
      "Epoch: 0050 loss= 0.90573\n",
      "Epoch: 0060 loss= 1.06366\n",
      "Epoch: 0070 loss= 0.76197\n",
      "Epoch: 0080 loss= 0.79150\n",
      "Epoch: 0090 loss= 0.64370\n",
      "Epoch: 0100 loss= 0.70343\n",
      "Epoch: 0110 loss= 0.61989\n",
      "Epoch: 0120 loss= 0.60040\n",
      "Epoch: 0130 loss= 0.58892\n",
      "Epoch: 0140 loss= 0.80324\n",
      "Epoch: 0150 loss= 0.53787\n",
      "Epoch: 0160 loss= 0.69766\n",
      "Epoch: 0170 loss= 0.64403\n",
      "Epoch: 0180 loss= 0.80786\n",
      "Epoch: 0190 loss= 0.71461\n",
      "Epoch: 0200 loss= 0.81721\n",
      "Epoch: 0210 loss= 0.65608\n",
      "Epoch: 0220 loss= 0.66994\n",
      "Epoch: 0230 loss= 0.50288\n",
      "Epoch: 0240 loss= 0.65217\n",
      "Epoch: 0250 loss= 0.51890\n",
      "Epoch: 0260 loss= 0.51229\n",
      "Epoch: 0270 loss= 0.57621\n",
      "Epoch: 0280 loss= 0.52907\n",
      "Epoch: 0290 loss= 0.60237\n",
      "Epoch: 0300 loss= 0.57984\n",
      "Epoch: 0310 loss= 0.61565\n",
      "Epoch: 0320 loss= 0.55260\n",
      "Epoch: 0330 loss= 0.61863\n",
      "Epoch: 0340 loss= 0.53616\n",
      "Epoch: 0350 loss= 0.53919\n",
      "Epoch: 0360 loss= 0.54303\n",
      "Epoch: 0370 loss= 0.56474\n",
      "Epoch: 0380 loss= 0.63315\n",
      "Epoch: 0390 loss= 0.53331\n",
      "Epoch: 0400 loss= 0.58071\n",
      "Epoch: 0410 loss= 0.58183\n",
      "Epoch: 0420 loss= 0.59212\n",
      "Epoch: 0430 loss= 0.61564\n",
      "Epoch: 0440 loss= 0.49313\n",
      "Epoch: 0450 loss= 0.60301\n",
      "Epoch: 0460 loss= 0.52841\n",
      "Epoch: 0470 loss= 0.52093\n",
      "Epoch: 0480 loss= 0.52598\n",
      "Epoch: 0490 loss= 0.59012\n",
      "Epoch: 0500 loss= 0.56915\n",
      "Epoch: 0510 loss= 0.64555\n",
      "Epoch: 0520 loss= 0.54372\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4873\n",
      "timestep:1, pyg_AUC: 0.4873\n",
      "timestep:2, pyg_AUC: 0.4873\n",
      "timestep:3, pyg_AUC: 0.4831\n",
      "timestep:4, pyg_AUC: 0.4845\n",
      "timestep:5, pyg_AUC: 0.4944\n",
      "timestep:6, pyg_AUC: 0.4859\n",
      "timestep:7, pyg_AUC: 0.4859\n",
      "timestep:8, pyg_AUC: 0.4915\n",
      "timestep:9, pyg_AUC: 0.4845\n",
      "timestep:10, pyg_AUC: 0.4929\n",
      "timestep:11, pyg_AUC: 0.4845\n",
      "timestep:12, pyg_AUC: 0.4915\n",
      "timestep:13, pyg_AUC: 0.4873\n",
      "timestep:14, pyg_AUC: 0.4901\n",
      "timestep:15, pyg_AUC: 0.4915\n",
      "timestep:16, pyg_AUC: 0.4929\n",
      "timestep:17, pyg_AUC: 0.4901\n",
      "timestep:18, pyg_AUC: 0.4958\n",
      "timestep:19, pyg_AUC: 0.4887\n",
      "timestep:20, pyg_AUC: 0.4845\n",
      "timestep:21, pyg_AUC: 0.4873\n",
      "timestep:22, pyg_AUC: 0.4929\n",
      "timestep:23, pyg_AUC: 0.4873\n",
      "timestep:24, pyg_AUC: 0.4901\n",
      "timestep:25, pyg_AUC: 0.4887\n",
      "timestep:26, pyg_AUC: 0.4915\n",
      "timestep:27, pyg_AUC: 0.4887\n",
      "timestep:28, pyg_AUC: 0.4831\n",
      "timestep:29, pyg_AUC: 0.4944\n",
      "timestep:30, pyg_AUC: 0.4944\n",
      "timestep:31, pyg_AUC: 0.4915\n",
      "timestep:32, pyg_AUC: 0.4873\n",
      "timestep:33, pyg_AUC: 0.4958\n",
      "timestep:34, pyg_AUC: 0.4887\n",
      "timestep:35, pyg_AUC: 0.4944\n",
      "timestep:36, pyg_AUC: 0.4859\n",
      "timestep:37, pyg_AUC: 0.4859\n",
      "timestep:38, pyg_AUC: 0.4887\n",
      "timestep:39, pyg_AUC: 0.4859\n",
      "timestep:40, pyg_AUC: 0.4944\n",
      "timestep:41, pyg_AUC: 0.4816\n",
      "timestep:42, pyg_AUC: 0.4873\n",
      "timestep:43, pyg_AUC: 0.4901\n",
      "timestep:44, pyg_AUC: 0.4887\n",
      "timestep:45, pyg_AUC: 0.4972\n",
      "timestep:46, pyg_AUC: 0.4845\n",
      "timestep:47, pyg_AUC: 0.4944\n",
      "timestep:48, pyg_AUC: 0.4972\n",
      "timestep:49, pyg_AUC: 0.4929\n",
      "timestep:50, pyg_AUC: 0.4873\n",
      "timestep:51, pyg_AUC: 0.4873\n",
      "timestep:52, pyg_AUC: 0.4915\n",
      "timestep:53, pyg_AUC: 0.4915\n",
      "timestep:54, pyg_AUC: 0.4845\n",
      "timestep:55, pyg_AUC: 0.4887\n",
      "timestep:56, pyg_AUC: 0.4944\n",
      "timestep:57, pyg_AUC: 0.4859\n",
      "timestep:58, pyg_AUC: 0.4915\n",
      "timestep:59, pyg_AUC: 0.4986\n",
      "timestep:60, pyg_AUC: 0.4859\n",
      "timestep:61, pyg_AUC: 0.4915\n",
      "timestep:62, pyg_AUC: 0.4831\n",
      "timestep:63, pyg_AUC: 0.4901\n",
      "timestep:64, pyg_AUC: 0.4901\n",
      "timestep:65, pyg_AUC: 0.4887\n",
      "timestep:66, pyg_AUC: 0.4915\n",
      "timestep:67, pyg_AUC: 0.4929\n",
      "timestep:68, pyg_AUC: 0.4873\n",
      "timestep:69, pyg_AUC: 0.4859\n",
      "timestep:70, pyg_AUC: 0.4929\n",
      "timestep:71, pyg_AUC: 0.4859\n",
      "timestep:72, pyg_AUC: 0.4915\n",
      "timestep:73, pyg_AUC: 0.4845\n",
      "timestep:74, pyg_AUC: 0.4944\n",
      "timestep:75, pyg_AUC: 0.4887\n",
      "timestep:76, pyg_AUC: 0.4873\n",
      "timestep:77, pyg_AUC: 0.4859\n",
      "timestep:78, pyg_AUC: 0.4816\n",
      "timestep:79, pyg_AUC: 0.4915\n",
      "timestep:80, pyg_AUC: 0.4859\n",
      "timestep:81, pyg_AUC: 0.4915\n",
      "timestep:82, pyg_AUC: 0.4915\n",
      "timestep:83, pyg_AUC: 0.4929\n",
      "timestep:84, pyg_AUC: 0.4845\n",
      "timestep:85, pyg_AUC: 0.4873\n",
      "timestep:86, pyg_AUC: 0.4859\n",
      "timestep:87, pyg_AUC: 0.4873\n",
      "timestep:88, pyg_AUC: 0.4901\n",
      "timestep:89, pyg_AUC: 0.4859\n",
      "timestep:90, pyg_AUC: 0.4816\n",
      "timestep:91, pyg_AUC: 0.4901\n",
      "timestep:92, pyg_AUC: 0.4831\n",
      "timestep:93, pyg_AUC: 0.4915\n",
      "timestep:94, pyg_AUC: 0.4944\n",
      "timestep:95, pyg_AUC: 0.4929\n",
      "timestep:96, pyg_AUC: 0.4887\n",
      "timestep:97, pyg_AUC: 0.4859\n",
      "timestep:98, pyg_AUC: 0.4901\n",
      "timestep:99, pyg_AUC: 0.4873\n",
      "timestep:100, pyg_AUC: 0.4873\n",
      "timestep:101, pyg_AUC: 0.4915\n",
      "timestep:102, pyg_AUC: 0.4901\n",
      "timestep:103, pyg_AUC: 0.4845\n",
      "timestep:104, pyg_AUC: 0.4873\n",
      "timestep:105, pyg_AUC: 0.4859\n",
      "timestep:106, pyg_AUC: 0.4859\n",
      "timestep:107, pyg_AUC: 0.4859\n",
      "timestep:108, pyg_AUC: 0.4887\n",
      "timestep:109, pyg_AUC: 0.4831\n",
      "timestep:110, pyg_AUC: 0.4859\n",
      "timestep:111, pyg_AUC: 0.4873\n",
      "timestep:112, pyg_AUC: 0.4901\n",
      "timestep:113, pyg_AUC: 0.4929\n",
      "timestep:114, pyg_AUC: 0.4873\n",
      "timestep:115, pyg_AUC: 0.4901\n",
      "timestep:116, pyg_AUC: 0.4929\n",
      "timestep:117, pyg_AUC: 0.4887\n",
      "timestep:118, pyg_AUC: 0.4873\n",
      "timestep:119, pyg_AUC: 0.4915\n",
      "timestep:120, pyg_AUC: 0.4929\n",
      "timestep:121, pyg_AUC: 0.4944\n",
      "timestep:122, pyg_AUC: 0.4831\n",
      "timestep:123, pyg_AUC: 0.4887\n",
      "timestep:124, pyg_AUC: 0.4859\n",
      "timestep:125, pyg_AUC: 0.4859\n",
      "timestep:126, pyg_AUC: 0.4873\n",
      "timestep:127, pyg_AUC: 0.4831\n",
      "timestep:128, pyg_AUC: 0.4816\n",
      "timestep:129, pyg_AUC: 0.4901\n",
      "timestep:130, pyg_AUC: 0.4845\n",
      "timestep:131, pyg_AUC: 0.4929\n",
      "timestep:132, pyg_AUC: 0.4958\n",
      "timestep:133, pyg_AUC: 0.4901\n",
      "timestep:134, pyg_AUC: 0.4873\n",
      "timestep:135, pyg_AUC: 0.4915\n",
      "timestep:136, pyg_AUC: 0.4901\n",
      "timestep:137, pyg_AUC: 0.4901\n",
      "timestep:138, pyg_AUC: 0.4873\n",
      "timestep:139, pyg_AUC: 0.4901\n",
      "timestep:140, pyg_AUC: 0.4873\n",
      "timestep:141, pyg_AUC: 0.4887\n",
      "timestep:142, pyg_AUC: 0.4887\n",
      "timestep:143, pyg_AUC: 0.4929\n",
      "timestep:144, pyg_AUC: 0.4887\n",
      "timestep:145, pyg_AUC: 0.4901\n",
      "timestep:146, pyg_AUC: 0.4859\n",
      "timestep:147, pyg_AUC: 0.4901\n",
      "timestep:148, pyg_AUC: 0.4901\n",
      "timestep:149, pyg_AUC: 0.4859\n",
      "timestep:150, pyg_AUC: 0.4915\n",
      "timestep:151, pyg_AUC: 0.4873\n",
      "timestep:152, pyg_AUC: 0.4873\n",
      "timestep:153, pyg_AUC: 0.4873\n",
      "timestep:154, pyg_AUC: 0.4915\n",
      "timestep:155, pyg_AUC: 0.4887\n",
      "timestep:156, pyg_AUC: 0.4831\n",
      "timestep:157, pyg_AUC: 0.4873\n",
      "timestep:158, pyg_AUC: 0.4901\n",
      "timestep:159, pyg_AUC: 0.4915\n",
      "timestep:160, pyg_AUC: 0.4859\n",
      "timestep:161, pyg_AUC: 0.4859\n",
      "timestep:162, pyg_AUC: 0.4845\n",
      "timestep:163, pyg_AUC: 0.4915\n",
      "timestep:164, pyg_AUC: 0.4887\n",
      "timestep:165, pyg_AUC: 0.4901\n",
      "timestep:166, pyg_AUC: 0.4873\n",
      "timestep:167, pyg_AUC: 0.4845\n",
      "timestep:168, pyg_AUC: 0.4887\n",
      "timestep:169, pyg_AUC: 0.4845\n",
      "timestep:170, pyg_AUC: 0.4859\n",
      "timestep:171, pyg_AUC: 0.4901\n",
      "timestep:172, pyg_AUC: 0.4816\n",
      "timestep:173, pyg_AUC: 0.4788\n",
      "timestep:174, pyg_AUC: 0.4845\n",
      "timestep:175, pyg_AUC: 0.4915\n",
      "timestep:176, pyg_AUC: 0.4915\n",
      "timestep:177, pyg_AUC: 0.4802\n",
      "timestep:178, pyg_AUC: 0.4845\n",
      "timestep:179, pyg_AUC: 0.4887\n",
      "timestep:180, pyg_AUC: 0.4915\n",
      "timestep:181, pyg_AUC: 0.4859\n",
      "timestep:182, pyg_AUC: 0.4802\n",
      "timestep:183, pyg_AUC: 0.4901\n",
      "timestep:184, pyg_AUC: 0.4859\n",
      "timestep:185, pyg_AUC: 0.4929\n",
      "timestep:186, pyg_AUC: 0.4845\n",
      "timestep:187, pyg_AUC: 0.4887\n",
      "timestep:188, pyg_AUC: 0.4873\n",
      "timestep:189, pyg_AUC: 0.4859\n",
      "timestep:190, pyg_AUC: 0.4873\n",
      "timestep:191, pyg_AUC: 0.4887\n",
      "timestep:192, pyg_AUC: 0.4901\n",
      "timestep:193, pyg_AUC: 0.4859\n",
      "timestep:194, pyg_AUC: 0.4915\n",
      "timestep:195, pyg_AUC: 0.4831\n",
      "timestep:196, pyg_AUC: 0.4901\n",
      "timestep:197, pyg_AUC: 0.4859\n",
      "timestep:198, pyg_AUC: 0.4901\n",
      "timestep:199, pyg_AUC: 0.4887\n",
      "timestep:200, pyg_AUC: 0.4859\n",
      "timestep:201, pyg_AUC: 0.4887\n",
      "timestep:202, pyg_AUC: 0.4901\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4915\n",
      "timestep:205, pyg_AUC: 0.4873\n",
      "timestep:206, pyg_AUC: 0.4859\n",
      "timestep:207, pyg_AUC: 0.4859\n",
      "timestep:208, pyg_AUC: 0.4873\n",
      "timestep:209, pyg_AUC: 0.4873\n",
      "timestep:210, pyg_AUC: 0.4901\n",
      "timestep:211, pyg_AUC: 0.4915\n",
      "timestep:212, pyg_AUC: 0.4873\n",
      "timestep:213, pyg_AUC: 0.4887\n",
      "timestep:214, pyg_AUC: 0.4859\n",
      "timestep:215, pyg_AUC: 0.4915\n",
      "timestep:216, pyg_AUC: 0.4873\n",
      "timestep:217, pyg_AUC: 0.4845\n",
      "timestep:218, pyg_AUC: 0.4859\n",
      "timestep:219, pyg_AUC: 0.4859\n",
      "timestep:220, pyg_AUC: 0.4901\n",
      "timestep:221, pyg_AUC: 0.4831\n",
      "timestep:222, pyg_AUC: 0.4901\n",
      "timestep:223, pyg_AUC: 0.4887\n",
      "timestep:224, pyg_AUC: 0.4887\n",
      "timestep:225, pyg_AUC: 0.4831\n",
      "timestep:226, pyg_AUC: 0.4873\n",
      "timestep:227, pyg_AUC: 0.4859\n",
      "timestep:228, pyg_AUC: 0.4845\n",
      "timestep:229, pyg_AUC: 0.4845\n",
      "timestep:230, pyg_AUC: 0.4887\n",
      "timestep:231, pyg_AUC: 0.4859\n",
      "timestep:232, pyg_AUC: 0.4915\n",
      "timestep:233, pyg_AUC: 0.4873\n",
      "timestep:234, pyg_AUC: 0.4929\n",
      "timestep:235, pyg_AUC: 0.4831\n",
      "timestep:236, pyg_AUC: 0.4887\n",
      "timestep:237, pyg_AUC: 0.4887\n",
      "timestep:238, pyg_AUC: 0.4873\n",
      "timestep:239, pyg_AUC: 0.4831\n",
      "timestep:240, pyg_AUC: 0.4859\n",
      "timestep:241, pyg_AUC: 0.4901\n",
      "timestep:242, pyg_AUC: 0.4887\n",
      "timestep:243, pyg_AUC: 0.4915\n",
      "timestep:244, pyg_AUC: 0.4887\n",
      "timestep:245, pyg_AUC: 0.4887\n",
      "timestep:246, pyg_AUC: 0.4845\n",
      "timestep:247, pyg_AUC: 0.4915\n",
      "timestep:248, pyg_AUC: 0.4873\n",
      "timestep:249, pyg_AUC: 0.4873\n",
      "timestep:250, pyg_AUC: 0.4859\n",
      "timestep:251, pyg_AUC: 0.4929\n",
      "timestep:252, pyg_AUC: 0.4802\n",
      "timestep:253, pyg_AUC: 0.4831\n",
      "timestep:254, pyg_AUC: 0.4873\n",
      "timestep:255, pyg_AUC: 0.4859\n",
      "timestep:256, pyg_AUC: 0.4873\n",
      "timestep:257, pyg_AUC: 0.4831\n",
      "timestep:258, pyg_AUC: 0.4901\n",
      "timestep:259, pyg_AUC: 0.4859\n",
      "timestep:260, pyg_AUC: 0.4816\n",
      "timestep:261, pyg_AUC: 0.4831\n",
      "timestep:262, pyg_AUC: 0.4859\n",
      "timestep:263, pyg_AUC: 0.4887\n",
      "timestep:264, pyg_AUC: 0.4901\n",
      "timestep:265, pyg_AUC: 0.4873\n",
      "timestep:266, pyg_AUC: 0.4831\n",
      "timestep:267, pyg_AUC: 0.4859\n",
      "timestep:268, pyg_AUC: 0.4873\n",
      "timestep:269, pyg_AUC: 0.4845\n",
      "timestep:270, pyg_AUC: 0.4887\n",
      "timestep:271, pyg_AUC: 0.4859\n",
      "timestep:272, pyg_AUC: 0.4929\n",
      "timestep:273, pyg_AUC: 0.4873\n",
      "timestep:274, pyg_AUC: 0.4901\n",
      "timestep:275, pyg_AUC: 0.4859\n",
      "timestep:276, pyg_AUC: 0.4887\n",
      "timestep:277, pyg_AUC: 0.4887\n",
      "timestep:278, pyg_AUC: 0.4873\n",
      "timestep:279, pyg_AUC: 0.4831\n",
      "timestep:280, pyg_AUC: 0.4859\n",
      "timestep:281, pyg_AUC: 0.4873\n",
      "timestep:282, pyg_AUC: 0.4901\n",
      "timestep:283, pyg_AUC: 0.4859\n",
      "timestep:284, pyg_AUC: 0.4873\n",
      "timestep:285, pyg_AUC: 0.4831\n",
      "timestep:286, pyg_AUC: 0.4887\n",
      "timestep:287, pyg_AUC: 0.4845\n",
      "timestep:288, pyg_AUC: 0.4873\n",
      "timestep:289, pyg_AUC: 0.4845\n",
      "timestep:290, pyg_AUC: 0.4887\n",
      "timestep:291, pyg_AUC: 0.4859\n",
      "timestep:292, pyg_AUC: 0.4915\n",
      "timestep:293, pyg_AUC: 0.4887\n",
      "timestep:294, pyg_AUC: 0.4859\n",
      "timestep:295, pyg_AUC: 0.4816\n",
      "timestep:296, pyg_AUC: 0.4887\n",
      "timestep:297, pyg_AUC: 0.4873\n",
      "timestep:298, pyg_AUC: 0.4887\n",
      "timestep:299, pyg_AUC: 0.4859\n",
      "timestep:300, pyg_AUC: 0.4944\n",
      "timestep:301, pyg_AUC: 0.4859\n",
      "timestep:302, pyg_AUC: 0.4873\n",
      "timestep:303, pyg_AUC: 0.4845\n",
      "timestep:304, pyg_AUC: 0.4873\n",
      "timestep:305, pyg_AUC: 0.4901\n",
      "timestep:306, pyg_AUC: 0.4901\n",
      "timestep:307, pyg_AUC: 0.4901\n",
      "timestep:308, pyg_AUC: 0.4873\n",
      "timestep:309, pyg_AUC: 0.4873\n",
      "timestep:310, pyg_AUC: 0.4831\n",
      "timestep:311, pyg_AUC: 0.4831\n",
      "timestep:312, pyg_AUC: 0.4845\n",
      "timestep:313, pyg_AUC: 0.4901\n",
      "timestep:314, pyg_AUC: 0.4873\n",
      "timestep:315, pyg_AUC: 0.4887\n",
      "timestep:316, pyg_AUC: 0.4901\n",
      "timestep:317, pyg_AUC: 0.4859\n",
      "timestep:318, pyg_AUC: 0.4873\n",
      "timestep:319, pyg_AUC: 0.4831\n",
      "timestep:320, pyg_AUC: 0.4887\n",
      "timestep:321, pyg_AUC: 0.4831\n",
      "timestep:322, pyg_AUC: 0.4915\n",
      "timestep:323, pyg_AUC: 0.4887\n",
      "timestep:324, pyg_AUC: 0.4873\n",
      "timestep:325, pyg_AUC: 0.4901\n",
      "timestep:326, pyg_AUC: 0.4901\n",
      "timestep:327, pyg_AUC: 0.4887\n",
      "timestep:328, pyg_AUC: 0.4887\n",
      "timestep:329, pyg_AUC: 0.4831\n",
      "timestep:330, pyg_AUC: 0.4873\n",
      "timestep:331, pyg_AUC: 0.4887\n",
      "timestep:332, pyg_AUC: 0.4873\n",
      "timestep:333, pyg_AUC: 0.4873\n",
      "timestep:334, pyg_AUC: 0.4873\n",
      "timestep:335, pyg_AUC: 0.4816\n",
      "timestep:336, pyg_AUC: 0.4901\n",
      "timestep:337, pyg_AUC: 0.4873\n",
      "timestep:338, pyg_AUC: 0.4873\n",
      "timestep:339, pyg_AUC: 0.4873\n",
      "timestep:340, pyg_AUC: 0.4845\n",
      "timestep:341, pyg_AUC: 0.4845\n",
      "timestep:342, pyg_AUC: 0.4915\n",
      "timestep:343, pyg_AUC: 0.4887\n",
      "timestep:344, pyg_AUC: 0.4901\n",
      "timestep:345, pyg_AUC: 0.4873\n",
      "timestep:346, pyg_AUC: 0.4901\n",
      "timestep:347, pyg_AUC: 0.4831\n",
      "timestep:348, pyg_AUC: 0.4859\n",
      "timestep:349, pyg_AUC: 0.4802\n",
      "timestep:350, pyg_AUC: 0.4887\n",
      "timestep:351, pyg_AUC: 0.4873\n",
      "timestep:352, pyg_AUC: 0.4873\n",
      "timestep:353, pyg_AUC: 0.4816\n",
      "timestep:354, pyg_AUC: 0.4873\n",
      "timestep:355, pyg_AUC: 0.4873\n",
      "timestep:356, pyg_AUC: 0.4873\n",
      "timestep:357, pyg_AUC: 0.4901\n",
      "timestep:358, pyg_AUC: 0.4873\n",
      "timestep:359, pyg_AUC: 0.4873\n",
      "timestep:360, pyg_AUC: 0.4887\n",
      "timestep:361, pyg_AUC: 0.4915\n",
      "timestep:362, pyg_AUC: 0.4901\n",
      "timestep:363, pyg_AUC: 0.4887\n",
      "timestep:364, pyg_AUC: 0.4887\n",
      "timestep:365, pyg_AUC: 0.4887\n",
      "timestep:366, pyg_AUC: 0.4873\n",
      "timestep:367, pyg_AUC: 0.4859\n",
      "timestep:368, pyg_AUC: 0.4901\n",
      "timestep:369, pyg_AUC: 0.4859\n",
      "timestep:370, pyg_AUC: 0.4859\n",
      "timestep:371, pyg_AUC: 0.4901\n",
      "timestep:372, pyg_AUC: 0.4887\n",
      "timestep:373, pyg_AUC: 0.4915\n",
      "timestep:374, pyg_AUC: 0.4915\n",
      "timestep:375, pyg_AUC: 0.4887\n",
      "timestep:376, pyg_AUC: 0.4831\n",
      "timestep:377, pyg_AUC: 0.4845\n",
      "timestep:378, pyg_AUC: 0.4859\n",
      "timestep:379, pyg_AUC: 0.4859\n",
      "timestep:380, pyg_AUC: 0.4859\n",
      "timestep:381, pyg_AUC: 0.4873\n",
      "timestep:382, pyg_AUC: 0.4816\n",
      "timestep:383, pyg_AUC: 0.4887\n",
      "timestep:384, pyg_AUC: 0.4901\n",
      "timestep:385, pyg_AUC: 0.4915\n",
      "timestep:386, pyg_AUC: 0.4887\n",
      "timestep:387, pyg_AUC: 0.4859\n",
      "timestep:388, pyg_AUC: 0.4887\n",
      "timestep:389, pyg_AUC: 0.4859\n",
      "timestep:390, pyg_AUC: 0.4873\n",
      "timestep:391, pyg_AUC: 0.4915\n",
      "timestep:392, pyg_AUC: 0.4859\n",
      "timestep:393, pyg_AUC: 0.4845\n",
      "timestep:394, pyg_AUC: 0.4901\n",
      "timestep:395, pyg_AUC: 0.4859\n",
      "timestep:396, pyg_AUC: 0.4873\n",
      "timestep:397, pyg_AUC: 0.4901\n",
      "timestep:398, pyg_AUC: 0.4859\n",
      "timestep:399, pyg_AUC: 0.4873\n",
      "timestep:400, pyg_AUC: 0.4816\n",
      "timestep:401, pyg_AUC: 0.4887\n",
      "timestep:402, pyg_AUC: 0.4901\n",
      "timestep:403, pyg_AUC: 0.4859\n",
      "timestep:404, pyg_AUC: 0.4788\n",
      "timestep:405, pyg_AUC: 0.4887\n",
      "timestep:406, pyg_AUC: 0.4859\n",
      "timestep:407, pyg_AUC: 0.4873\n",
      "timestep:408, pyg_AUC: 0.4831\n",
      "timestep:409, pyg_AUC: 0.4887\n",
      "timestep:410, pyg_AUC: 0.4901\n",
      "timestep:411, pyg_AUC: 0.4901\n",
      "timestep:412, pyg_AUC: 0.4845\n",
      "timestep:413, pyg_AUC: 0.4873\n",
      "timestep:414, pyg_AUC: 0.4887\n",
      "timestep:415, pyg_AUC: 0.4873\n",
      "timestep:416, pyg_AUC: 0.4831\n",
      "timestep:417, pyg_AUC: 0.4845\n",
      "timestep:418, pyg_AUC: 0.4887\n",
      "timestep:419, pyg_AUC: 0.4859\n",
      "timestep:420, pyg_AUC: 0.4859\n",
      "timestep:421, pyg_AUC: 0.4887\n",
      "timestep:422, pyg_AUC: 0.4901\n",
      "timestep:423, pyg_AUC: 0.4845\n",
      "timestep:424, pyg_AUC: 0.4887\n",
      "timestep:425, pyg_AUC: 0.4845\n",
      "timestep:426, pyg_AUC: 0.4915\n",
      "timestep:427, pyg_AUC: 0.4873\n",
      "timestep:428, pyg_AUC: 0.4887\n",
      "timestep:429, pyg_AUC: 0.4845\n",
      "timestep:430, pyg_AUC: 0.4873\n",
      "timestep:431, pyg_AUC: 0.4859\n",
      "timestep:432, pyg_AUC: 0.4873\n",
      "timestep:433, pyg_AUC: 0.4859\n",
      "timestep:434, pyg_AUC: 0.4887\n",
      "timestep:435, pyg_AUC: 0.4915\n",
      "timestep:436, pyg_AUC: 0.4859\n",
      "timestep:437, pyg_AUC: 0.4845\n",
      "timestep:438, pyg_AUC: 0.4873\n",
      "timestep:439, pyg_AUC: 0.4901\n",
      "timestep:440, pyg_AUC: 0.4873\n",
      "timestep:441, pyg_AUC: 0.4859\n",
      "timestep:442, pyg_AUC: 0.4887\n",
      "timestep:443, pyg_AUC: 0.4788\n",
      "timestep:444, pyg_AUC: 0.4887\n",
      "timestep:445, pyg_AUC: 0.4887\n",
      "timestep:446, pyg_AUC: 0.4915\n",
      "timestep:447, pyg_AUC: 0.4831\n",
      "timestep:448, pyg_AUC: 0.4887\n",
      "timestep:449, pyg_AUC: 0.4887\n",
      "timestep:450, pyg_AUC: 0.4816\n",
      "timestep:451, pyg_AUC: 0.4845\n",
      "timestep:452, pyg_AUC: 0.4859\n",
      "timestep:453, pyg_AUC: 0.4873\n",
      "timestep:454, pyg_AUC: 0.4845\n",
      "timestep:455, pyg_AUC: 0.4831\n",
      "timestep:456, pyg_AUC: 0.4873\n",
      "timestep:457, pyg_AUC: 0.4887\n",
      "timestep:458, pyg_AUC: 0.4831\n",
      "timestep:459, pyg_AUC: 0.4831\n",
      "timestep:460, pyg_AUC: 0.4901\n",
      "timestep:461, pyg_AUC: 0.4859\n",
      "timestep:462, pyg_AUC: 0.4859\n",
      "timestep:463, pyg_AUC: 0.4887\n",
      "timestep:464, pyg_AUC: 0.4859\n",
      "timestep:465, pyg_AUC: 0.4859\n",
      "timestep:466, pyg_AUC: 0.4845\n",
      "timestep:467, pyg_AUC: 0.4845\n",
      "timestep:468, pyg_AUC: 0.4831\n",
      "timestep:469, pyg_AUC: 0.4887\n",
      "timestep:470, pyg_AUC: 0.4859\n",
      "timestep:471, pyg_AUC: 0.4816\n",
      "timestep:472, pyg_AUC: 0.4845\n",
      "timestep:473, pyg_AUC: 0.4873\n",
      "timestep:474, pyg_AUC: 0.4859\n",
      "timestep:475, pyg_AUC: 0.4887\n",
      "timestep:476, pyg_AUC: 0.4831\n",
      "timestep:477, pyg_AUC: 0.4859\n",
      "timestep:478, pyg_AUC: 0.4845\n",
      "timestep:479, pyg_AUC: 0.4887\n",
      "timestep:480, pyg_AUC: 0.4845\n",
      "timestep:481, pyg_AUC: 0.4901\n",
      "timestep:482, pyg_AUC: 0.4845\n",
      "timestep:483, pyg_AUC: 0.4873\n",
      "timestep:484, pyg_AUC: 0.4887\n",
      "timestep:485, pyg_AUC: 0.4859\n",
      "timestep:486, pyg_AUC: 0.4901\n",
      "timestep:487, pyg_AUC: 0.4901\n",
      "timestep:488, pyg_AUC: 0.4859\n",
      "timestep:489, pyg_AUC: 0.4901\n",
      "timestep:490, pyg_AUC: 0.4901\n",
      "timestep:491, pyg_AUC: 0.4915\n",
      "timestep:492, pyg_AUC: 0.4845\n",
      "timestep:493, pyg_AUC: 0.4831\n",
      "timestep:494, pyg_AUC: 0.4859\n",
      "timestep:495, pyg_AUC: 0.4873\n",
      "timestep:496, pyg_AUC: 0.4831\n",
      "timestep:497, pyg_AUC: 0.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [05:00<28:28, 100.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:498, pyg_AUC: 0.4859\n",
      "timestep:499, pyg_AUC: 0.4859\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 39.86916\n",
      "Epoch: 0010 loss= 30.48896\n",
      "Epoch: 0020 loss= 21.99897\n",
      "Epoch: 0030 loss= 15.32752\n",
      "Epoch: 0040 loss= 13.91725\n",
      "Epoch: 0050 loss= 10.19799\n",
      "Epoch: 0060 loss= 1.75214\n",
      "Epoch: 0070 loss= 0.96667\n",
      "Epoch: 0080 loss= 0.66522\n",
      "Epoch: 0090 loss= 0.65110\n",
      "Epoch: 0100 loss= 0.58038\n",
      "Epoch: 0110 loss= 0.56416\n",
      "Epoch: 0120 loss= 0.58306\n",
      "Epoch: 0130 loss= 0.63422\n",
      "Epoch: 0140 loss= 0.56405\n",
      "Epoch: 0150 loss= 0.53804\n",
      "Epoch: 0160 loss= 0.59004\n",
      "Epoch: 0170 loss= 0.57265\n",
      "Epoch: 0180 loss= 0.60825\n",
      "Epoch: 0190 loss= 0.65617\n",
      "Epoch: 0200 loss= 0.59594\n",
      "Epoch: 0210 loss= 0.63938\n",
      "Epoch: 0220 loss= 0.66307\n",
      "Epoch: 0230 loss= 0.58697\n",
      "Epoch: 0240 loss= 0.60111\n",
      "Epoch: 0250 loss= 0.53910\n",
      "Epoch: 0260 loss= 0.57162\n",
      "Epoch: 0270 loss= 0.59770\n",
      "Epoch: 0280 loss= 0.64961\n",
      "Epoch: 0290 loss= 0.52843\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.8257,  4.6789, -5.3808, -4.5979, -4.4125,  5.6426,  5.4814, -5.2395]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 38.38554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss= 26.02908\n",
      "Epoch: 0020 loss= 15.64923\n",
      "Epoch: 0030 loss= 10.09938\n",
      "Epoch: 0040 loss= 2.52251\n",
      "Epoch: 0050 loss= 0.99488\n",
      "Epoch: 0060 loss= 0.73221\n",
      "Epoch: 0070 loss= 1.24813\n",
      "Epoch: 0080 loss= 0.61150\n",
      "Epoch: 0090 loss= 0.57408\n",
      "Epoch: 0100 loss= 0.57113\n",
      "Epoch: 0110 loss= 0.63948\n",
      "Epoch: 0120 loss= 0.65565\n",
      "Epoch: 0130 loss= 0.62418\n",
      "Epoch: 0140 loss= 0.59848\n",
      "Epoch: 0150 loss= 0.60206\n",
      "Epoch: 0160 loss= 0.64204\n",
      "Epoch: 0170 loss= 0.59867\n",
      "Epoch: 0180 loss= 0.58721\n",
      "Epoch: 0190 loss= 0.61809\n",
      "Epoch: 0200 loss= 0.54739\n",
      "Epoch: 0210 loss= 0.57173\n",
      "Epoch: 0220 loss= 0.53242\n",
      "Epoch: 0230 loss= 0.48657\n",
      "Epoch: 0240 loss= 0.60381\n",
      "Epoch: 0250 loss= 0.62325\n",
      "Epoch: 0260 loss= 0.57790\n",
      "Epoch: 0270 loss= 0.52301\n",
      "Epoch: 0280 loss= 0.65179\n",
      "Epoch: 0290 loss= 0.62602\n",
      "Epoch: 0300 loss= 0.64857\n",
      "Epoch: 0310 loss= 0.52779\n",
      "Epoch: 0320 loss= 0.60063\n",
      "Epoch: 0330 loss= 0.56770\n",
      "Epoch: 0340 loss= 0.60228\n",
      "Epoch: 0350 loss= 0.52561\n",
      "Epoch: 0360 loss= 0.55866\n",
      "Epoch: 0370 loss= 0.57887\n",
      "Epoch: 0380 loss= 0.51621\n",
      "Epoch: 0390 loss= 0.56849\n",
      "Epoch: 0400 loss= 0.48613\n",
      "Epoch: 0410 loss= 0.56505\n",
      "Epoch: 0420 loss= 0.53921\n",
      "Epoch: 0430 loss= 0.53219\n",
      "Epoch: 0440 loss= 0.50123\n",
      "Epoch: 0450 loss= 0.59446\n",
      "Epoch: 0460 loss= 0.48755\n",
      "Epoch: 0470 loss= 0.57213\n",
      "Epoch: 0480 loss= 0.58274\n",
      "Epoch: 0490 loss= 0.62776\n",
      "Epoch: 0500 loss= 0.62443\n",
      "Epoch: 0510 loss= 0.51922\n",
      "Epoch: 0520 loss= 0.48559\n",
      "Epoch: 0530 loss= 0.61022\n",
      "Epoch: 0540 loss= 0.53109\n",
      "Epoch: 0550 loss= 0.55992\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4915\n",
      "timestep:1, pyg_AUC: 0.4901\n",
      "timestep:2, pyg_AUC: 0.4915\n",
      "timestep:3, pyg_AUC: 0.4845\n",
      "timestep:4, pyg_AUC: 0.4859\n",
      "timestep:5, pyg_AUC: 0.4859\n",
      "timestep:6, pyg_AUC: 0.4944\n",
      "timestep:7, pyg_AUC: 0.4859\n",
      "timestep:8, pyg_AUC: 0.4929\n",
      "timestep:9, pyg_AUC: 0.4859\n",
      "timestep:10, pyg_AUC: 0.4915\n",
      "timestep:11, pyg_AUC: 0.4915\n",
      "timestep:12, pyg_AUC: 0.4887\n",
      "timestep:13, pyg_AUC: 0.4887\n",
      "timestep:14, pyg_AUC: 0.4873\n",
      "timestep:15, pyg_AUC: 0.4873\n",
      "timestep:16, pyg_AUC: 0.4859\n",
      "timestep:17, pyg_AUC: 0.4901\n",
      "timestep:18, pyg_AUC: 0.4887\n",
      "timestep:19, pyg_AUC: 0.4944\n",
      "timestep:20, pyg_AUC: 0.4887\n",
      "timestep:21, pyg_AUC: 0.4929\n",
      "timestep:22, pyg_AUC: 0.4887\n",
      "timestep:23, pyg_AUC: 0.4873\n",
      "timestep:24, pyg_AUC: 0.4887\n",
      "timestep:25, pyg_AUC: 0.4887\n",
      "timestep:26, pyg_AUC: 0.4859\n",
      "timestep:27, pyg_AUC: 0.4929\n",
      "timestep:28, pyg_AUC: 0.4887\n",
      "timestep:29, pyg_AUC: 0.4859\n",
      "timestep:30, pyg_AUC: 0.4901\n",
      "timestep:31, pyg_AUC: 0.4887\n",
      "timestep:32, pyg_AUC: 0.4901\n",
      "timestep:33, pyg_AUC: 0.4915\n",
      "timestep:34, pyg_AUC: 0.4901\n",
      "timestep:35, pyg_AUC: 0.4915\n",
      "timestep:36, pyg_AUC: 0.4901\n",
      "timestep:37, pyg_AUC: 0.4887\n",
      "timestep:38, pyg_AUC: 0.4929\n",
      "timestep:39, pyg_AUC: 0.4944\n",
      "timestep:40, pyg_AUC: 0.4901\n",
      "timestep:41, pyg_AUC: 0.4929\n",
      "timestep:42, pyg_AUC: 0.4873\n",
      "timestep:43, pyg_AUC: 0.4887\n",
      "timestep:44, pyg_AUC: 0.4901\n",
      "timestep:45, pyg_AUC: 0.4845\n",
      "timestep:46, pyg_AUC: 0.4915\n",
      "timestep:47, pyg_AUC: 0.4873\n",
      "timestep:48, pyg_AUC: 0.4901\n",
      "timestep:49, pyg_AUC: 0.4873\n",
      "timestep:50, pyg_AUC: 0.4873\n",
      "timestep:51, pyg_AUC: 0.4859\n",
      "timestep:52, pyg_AUC: 0.4887\n",
      "timestep:53, pyg_AUC: 0.4901\n",
      "timestep:54, pyg_AUC: 0.4929\n",
      "timestep:55, pyg_AUC: 0.4873\n",
      "timestep:56, pyg_AUC: 0.4901\n",
      "timestep:57, pyg_AUC: 0.4887\n",
      "timestep:58, pyg_AUC: 0.4859\n",
      "timestep:59, pyg_AUC: 0.4845\n",
      "timestep:60, pyg_AUC: 0.4915\n",
      "timestep:61, pyg_AUC: 0.4929\n",
      "timestep:62, pyg_AUC: 0.4873\n",
      "timestep:63, pyg_AUC: 0.4887\n",
      "timestep:64, pyg_AUC: 0.4873\n",
      "timestep:65, pyg_AUC: 0.4887\n",
      "timestep:66, pyg_AUC: 0.4901\n",
      "timestep:67, pyg_AUC: 0.4944\n",
      "timestep:68, pyg_AUC: 0.4887\n",
      "timestep:69, pyg_AUC: 0.4831\n",
      "timestep:70, pyg_AUC: 0.4915\n",
      "timestep:71, pyg_AUC: 0.4944\n",
      "timestep:72, pyg_AUC: 0.4901\n",
      "timestep:73, pyg_AUC: 0.4915\n",
      "timestep:74, pyg_AUC: 0.4845\n",
      "timestep:75, pyg_AUC: 0.4887\n",
      "timestep:76, pyg_AUC: 0.4859\n",
      "timestep:77, pyg_AUC: 0.4901\n",
      "timestep:78, pyg_AUC: 0.4873\n",
      "timestep:79, pyg_AUC: 0.4887\n",
      "timestep:80, pyg_AUC: 0.4915\n",
      "timestep:81, pyg_AUC: 0.4831\n",
      "timestep:82, pyg_AUC: 0.4929\n",
      "timestep:83, pyg_AUC: 0.4901\n",
      "timestep:84, pyg_AUC: 0.4901\n",
      "timestep:85, pyg_AUC: 0.4859\n",
      "timestep:86, pyg_AUC: 0.4887\n",
      "timestep:87, pyg_AUC: 0.4901\n",
      "timestep:88, pyg_AUC: 0.4845\n",
      "timestep:89, pyg_AUC: 0.4901\n",
      "timestep:90, pyg_AUC: 0.4873\n",
      "timestep:91, pyg_AUC: 0.4901\n",
      "timestep:92, pyg_AUC: 0.4901\n",
      "timestep:93, pyg_AUC: 0.4887\n",
      "timestep:94, pyg_AUC: 0.4901\n",
      "timestep:95, pyg_AUC: 0.4901\n",
      "timestep:96, pyg_AUC: 0.4873\n",
      "timestep:97, pyg_AUC: 0.4901\n",
      "timestep:98, pyg_AUC: 0.4887\n",
      "timestep:99, pyg_AUC: 0.4887\n",
      "timestep:100, pyg_AUC: 0.4915\n",
      "timestep:101, pyg_AUC: 0.4887\n",
      "timestep:102, pyg_AUC: 0.4859\n",
      "timestep:103, pyg_AUC: 0.4831\n",
      "timestep:104, pyg_AUC: 0.4929\n",
      "timestep:105, pyg_AUC: 0.4901\n",
      "timestep:106, pyg_AUC: 0.4901\n",
      "timestep:107, pyg_AUC: 0.4944\n",
      "timestep:108, pyg_AUC: 0.4887\n",
      "timestep:109, pyg_AUC: 0.4887\n",
      "timestep:110, pyg_AUC: 0.4915\n",
      "timestep:111, pyg_AUC: 0.4901\n",
      "timestep:112, pyg_AUC: 0.4901\n",
      "timestep:113, pyg_AUC: 0.4887\n",
      "timestep:114, pyg_AUC: 0.4887\n",
      "timestep:115, pyg_AUC: 0.4915\n",
      "timestep:116, pyg_AUC: 0.4958\n",
      "timestep:117, pyg_AUC: 0.4887\n",
      "timestep:118, pyg_AUC: 0.4887\n",
      "timestep:119, pyg_AUC: 0.4929\n",
      "timestep:120, pyg_AUC: 0.4929\n",
      "timestep:121, pyg_AUC: 0.4859\n",
      "timestep:122, pyg_AUC: 0.4915\n",
      "timestep:123, pyg_AUC: 0.4859\n",
      "timestep:124, pyg_AUC: 0.4929\n",
      "timestep:125, pyg_AUC: 0.4887\n",
      "timestep:126, pyg_AUC: 0.4915\n",
      "timestep:127, pyg_AUC: 0.4915\n",
      "timestep:128, pyg_AUC: 0.4887\n",
      "timestep:129, pyg_AUC: 0.4929\n",
      "timestep:130, pyg_AUC: 0.4972\n",
      "timestep:131, pyg_AUC: 0.4901\n",
      "timestep:132, pyg_AUC: 0.4845\n",
      "timestep:133, pyg_AUC: 0.4929\n",
      "timestep:134, pyg_AUC: 0.4901\n",
      "timestep:135, pyg_AUC: 0.4929\n",
      "timestep:136, pyg_AUC: 0.4929\n",
      "timestep:137, pyg_AUC: 0.4831\n",
      "timestep:138, pyg_AUC: 0.4887\n",
      "timestep:139, pyg_AUC: 0.4929\n",
      "timestep:140, pyg_AUC: 0.4887\n",
      "timestep:141, pyg_AUC: 0.4915\n",
      "timestep:142, pyg_AUC: 0.4915\n",
      "timestep:143, pyg_AUC: 0.4845\n",
      "timestep:144, pyg_AUC: 0.4929\n",
      "timestep:145, pyg_AUC: 0.4873\n",
      "timestep:146, pyg_AUC: 0.4859\n",
      "timestep:147, pyg_AUC: 0.4901\n",
      "timestep:148, pyg_AUC: 0.4873\n",
      "timestep:149, pyg_AUC: 0.4901\n",
      "timestep:150, pyg_AUC: 0.4901\n",
      "timestep:151, pyg_AUC: 0.4859\n",
      "timestep:152, pyg_AUC: 0.4915\n",
      "timestep:153, pyg_AUC: 0.4901\n",
      "timestep:154, pyg_AUC: 0.4901\n",
      "timestep:155, pyg_AUC: 0.4901\n",
      "timestep:156, pyg_AUC: 0.4887\n",
      "timestep:157, pyg_AUC: 0.4929\n",
      "timestep:158, pyg_AUC: 0.4887\n",
      "timestep:159, pyg_AUC: 0.4887\n",
      "timestep:160, pyg_AUC: 0.4887\n",
      "timestep:161, pyg_AUC: 0.4887\n",
      "timestep:162, pyg_AUC: 0.4859\n",
      "timestep:163, pyg_AUC: 0.4915\n",
      "timestep:164, pyg_AUC: 0.4901\n",
      "timestep:165, pyg_AUC: 0.4859\n",
      "timestep:166, pyg_AUC: 0.4873\n",
      "timestep:167, pyg_AUC: 0.4915\n",
      "timestep:168, pyg_AUC: 0.4944\n",
      "timestep:169, pyg_AUC: 0.4915\n",
      "timestep:170, pyg_AUC: 0.4929\n",
      "timestep:171, pyg_AUC: 0.4929\n",
      "timestep:172, pyg_AUC: 0.4915\n",
      "timestep:173, pyg_AUC: 0.4873\n",
      "timestep:174, pyg_AUC: 0.4887\n",
      "timestep:175, pyg_AUC: 0.4901\n",
      "timestep:176, pyg_AUC: 0.4915\n",
      "timestep:177, pyg_AUC: 0.4887\n",
      "timestep:178, pyg_AUC: 0.4845\n",
      "timestep:179, pyg_AUC: 0.4887\n",
      "timestep:180, pyg_AUC: 0.4915\n",
      "timestep:181, pyg_AUC: 0.4887\n",
      "timestep:182, pyg_AUC: 0.4901\n",
      "timestep:183, pyg_AUC: 0.4873\n",
      "timestep:184, pyg_AUC: 0.4873\n",
      "timestep:185, pyg_AUC: 0.4901\n",
      "timestep:186, pyg_AUC: 0.4887\n",
      "timestep:187, pyg_AUC: 0.4901\n",
      "timestep:188, pyg_AUC: 0.4915\n",
      "timestep:189, pyg_AUC: 0.4901\n",
      "timestep:190, pyg_AUC: 0.4859\n",
      "timestep:191, pyg_AUC: 0.4887\n",
      "timestep:192, pyg_AUC: 0.4915\n",
      "timestep:193, pyg_AUC: 0.4901\n",
      "timestep:194, pyg_AUC: 0.4901\n",
      "timestep:195, pyg_AUC: 0.4915\n",
      "timestep:196, pyg_AUC: 0.4915\n",
      "timestep:197, pyg_AUC: 0.4929\n",
      "timestep:198, pyg_AUC: 0.4859\n",
      "timestep:199, pyg_AUC: 0.4915\n",
      "timestep:200, pyg_AUC: 0.4887\n",
      "timestep:201, pyg_AUC: 0.4887\n",
      "timestep:202, pyg_AUC: 0.4915\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4915\n",
      "timestep:205, pyg_AUC: 0.4887\n",
      "timestep:206, pyg_AUC: 0.4859\n",
      "timestep:207, pyg_AUC: 0.4845\n",
      "timestep:208, pyg_AUC: 0.4873\n",
      "timestep:209, pyg_AUC: 0.4915\n",
      "timestep:210, pyg_AUC: 0.4887\n",
      "timestep:211, pyg_AUC: 0.4887\n",
      "timestep:212, pyg_AUC: 0.4887\n",
      "timestep:213, pyg_AUC: 0.4873\n",
      "timestep:214, pyg_AUC: 0.4887\n",
      "timestep:215, pyg_AUC: 0.4901\n",
      "timestep:216, pyg_AUC: 0.4873\n",
      "timestep:217, pyg_AUC: 0.4859\n",
      "timestep:218, pyg_AUC: 0.4873\n",
      "timestep:219, pyg_AUC: 0.4859\n",
      "timestep:220, pyg_AUC: 0.4859\n",
      "timestep:221, pyg_AUC: 0.4873\n",
      "timestep:222, pyg_AUC: 0.4929\n",
      "timestep:223, pyg_AUC: 0.4873\n",
      "timestep:224, pyg_AUC: 0.4901\n",
      "timestep:225, pyg_AUC: 0.4873\n",
      "timestep:226, pyg_AUC: 0.4887\n",
      "timestep:227, pyg_AUC: 0.4901\n",
      "timestep:228, pyg_AUC: 0.4873\n",
      "timestep:229, pyg_AUC: 0.4887\n",
      "timestep:230, pyg_AUC: 0.4873\n",
      "timestep:231, pyg_AUC: 0.4887\n",
      "timestep:232, pyg_AUC: 0.4901\n",
      "timestep:233, pyg_AUC: 0.4915\n",
      "timestep:234, pyg_AUC: 0.4831\n",
      "timestep:235, pyg_AUC: 0.4887\n",
      "timestep:236, pyg_AUC: 0.4901\n",
      "timestep:237, pyg_AUC: 0.4887\n",
      "timestep:238, pyg_AUC: 0.4901\n",
      "timestep:239, pyg_AUC: 0.4887\n",
      "timestep:240, pyg_AUC: 0.4873\n",
      "timestep:241, pyg_AUC: 0.4887\n",
      "timestep:242, pyg_AUC: 0.4816\n",
      "timestep:243, pyg_AUC: 0.4873\n",
      "timestep:244, pyg_AUC: 0.4887\n",
      "timestep:245, pyg_AUC: 0.4887\n",
      "timestep:246, pyg_AUC: 0.4901\n",
      "timestep:247, pyg_AUC: 0.4859\n",
      "timestep:248, pyg_AUC: 0.4873\n",
      "timestep:249, pyg_AUC: 0.4831\n",
      "timestep:250, pyg_AUC: 0.4887\n",
      "timestep:251, pyg_AUC: 0.4859\n",
      "timestep:252, pyg_AUC: 0.4901\n",
      "timestep:253, pyg_AUC: 0.4887\n",
      "timestep:254, pyg_AUC: 0.4873\n",
      "timestep:255, pyg_AUC: 0.4859\n",
      "timestep:256, pyg_AUC: 0.4901\n",
      "timestep:257, pyg_AUC: 0.4845\n",
      "timestep:258, pyg_AUC: 0.4845\n",
      "timestep:259, pyg_AUC: 0.4887\n",
      "timestep:260, pyg_AUC: 0.4816\n",
      "timestep:261, pyg_AUC: 0.4831\n",
      "timestep:262, pyg_AUC: 0.4901\n",
      "timestep:263, pyg_AUC: 0.4915\n",
      "timestep:264, pyg_AUC: 0.4901\n",
      "timestep:265, pyg_AUC: 0.4859\n",
      "timestep:266, pyg_AUC: 0.4859\n",
      "timestep:267, pyg_AUC: 0.4873\n",
      "timestep:268, pyg_AUC: 0.4831\n",
      "timestep:269, pyg_AUC: 0.4887\n",
      "timestep:270, pyg_AUC: 0.4915\n",
      "timestep:271, pyg_AUC: 0.4901\n",
      "timestep:272, pyg_AUC: 0.4873\n",
      "timestep:273, pyg_AUC: 0.4887\n",
      "timestep:274, pyg_AUC: 0.4859\n",
      "timestep:275, pyg_AUC: 0.4845\n",
      "timestep:276, pyg_AUC: 0.4901\n",
      "timestep:277, pyg_AUC: 0.4901\n",
      "timestep:278, pyg_AUC: 0.4901\n",
      "timestep:279, pyg_AUC: 0.4873\n",
      "timestep:280, pyg_AUC: 0.4831\n",
      "timestep:281, pyg_AUC: 0.4887\n",
      "timestep:282, pyg_AUC: 0.4859\n",
      "timestep:283, pyg_AUC: 0.4901\n",
      "timestep:284, pyg_AUC: 0.4887\n",
      "timestep:285, pyg_AUC: 0.4873\n",
      "timestep:286, pyg_AUC: 0.4873\n",
      "timestep:287, pyg_AUC: 0.4873\n",
      "timestep:288, pyg_AUC: 0.4859\n",
      "timestep:289, pyg_AUC: 0.4873\n",
      "timestep:290, pyg_AUC: 0.4873\n",
      "timestep:291, pyg_AUC: 0.4887\n",
      "timestep:292, pyg_AUC: 0.4915\n",
      "timestep:293, pyg_AUC: 0.4887\n",
      "timestep:294, pyg_AUC: 0.4887\n",
      "timestep:295, pyg_AUC: 0.4887\n",
      "timestep:296, pyg_AUC: 0.4901\n",
      "timestep:297, pyg_AUC: 0.4859\n",
      "timestep:298, pyg_AUC: 0.4887\n",
      "timestep:299, pyg_AUC: 0.4887\n",
      "timestep:300, pyg_AUC: 0.4901\n",
      "timestep:301, pyg_AUC: 0.4915\n",
      "timestep:302, pyg_AUC: 0.4859\n",
      "timestep:303, pyg_AUC: 0.4831\n",
      "timestep:304, pyg_AUC: 0.4873\n",
      "timestep:305, pyg_AUC: 0.4873\n",
      "timestep:306, pyg_AUC: 0.4859\n",
      "timestep:307, pyg_AUC: 0.4845\n",
      "timestep:308, pyg_AUC: 0.4873\n",
      "timestep:309, pyg_AUC: 0.4887\n",
      "timestep:310, pyg_AUC: 0.4873\n",
      "timestep:311, pyg_AUC: 0.4873\n",
      "timestep:312, pyg_AUC: 0.4873\n",
      "timestep:313, pyg_AUC: 0.4873\n",
      "timestep:314, pyg_AUC: 0.4859\n",
      "timestep:315, pyg_AUC: 0.4873\n",
      "timestep:316, pyg_AUC: 0.4845\n",
      "timestep:317, pyg_AUC: 0.4859\n",
      "timestep:318, pyg_AUC: 0.4859\n",
      "timestep:319, pyg_AUC: 0.4901\n",
      "timestep:320, pyg_AUC: 0.4887\n",
      "timestep:321, pyg_AUC: 0.4873\n",
      "timestep:322, pyg_AUC: 0.4873\n",
      "timestep:323, pyg_AUC: 0.4845\n",
      "timestep:324, pyg_AUC: 0.4859\n",
      "timestep:325, pyg_AUC: 0.4873\n",
      "timestep:326, pyg_AUC: 0.4873\n",
      "timestep:327, pyg_AUC: 0.4845\n",
      "timestep:328, pyg_AUC: 0.4845\n",
      "timestep:329, pyg_AUC: 0.4873\n",
      "timestep:330, pyg_AUC: 0.4873\n",
      "timestep:331, pyg_AUC: 0.4873\n",
      "timestep:332, pyg_AUC: 0.4859\n",
      "timestep:333, pyg_AUC: 0.4887\n",
      "timestep:334, pyg_AUC: 0.4845\n",
      "timestep:335, pyg_AUC: 0.4859\n",
      "timestep:336, pyg_AUC: 0.4873\n",
      "timestep:337, pyg_AUC: 0.4887\n",
      "timestep:338, pyg_AUC: 0.4887\n",
      "timestep:339, pyg_AUC: 0.4859\n",
      "timestep:340, pyg_AUC: 0.4859\n",
      "timestep:341, pyg_AUC: 0.4859\n",
      "timestep:342, pyg_AUC: 0.4873\n",
      "timestep:343, pyg_AUC: 0.4873\n",
      "timestep:344, pyg_AUC: 0.4873\n",
      "timestep:345, pyg_AUC: 0.4887\n",
      "timestep:346, pyg_AUC: 0.4873\n",
      "timestep:347, pyg_AUC: 0.4887\n",
      "timestep:348, pyg_AUC: 0.4831\n",
      "timestep:349, pyg_AUC: 0.4859\n",
      "timestep:350, pyg_AUC: 0.4859\n",
      "timestep:351, pyg_AUC: 0.4873\n",
      "timestep:352, pyg_AUC: 0.4873\n",
      "timestep:353, pyg_AUC: 0.4859\n",
      "timestep:354, pyg_AUC: 0.4845\n",
      "timestep:355, pyg_AUC: 0.4859\n",
      "timestep:356, pyg_AUC: 0.4873\n",
      "timestep:357, pyg_AUC: 0.4859\n",
      "timestep:358, pyg_AUC: 0.4887\n",
      "timestep:359, pyg_AUC: 0.4859\n",
      "timestep:360, pyg_AUC: 0.4915\n",
      "timestep:361, pyg_AUC: 0.4831\n",
      "timestep:362, pyg_AUC: 0.4887\n",
      "timestep:363, pyg_AUC: 0.4873\n",
      "timestep:364, pyg_AUC: 0.4873\n",
      "timestep:365, pyg_AUC: 0.4873\n",
      "timestep:366, pyg_AUC: 0.4887\n",
      "timestep:367, pyg_AUC: 0.4859\n",
      "timestep:368, pyg_AUC: 0.4887\n",
      "timestep:369, pyg_AUC: 0.4873\n",
      "timestep:370, pyg_AUC: 0.4873\n",
      "timestep:371, pyg_AUC: 0.4887\n",
      "timestep:372, pyg_AUC: 0.4859\n",
      "timestep:373, pyg_AUC: 0.4901\n",
      "timestep:374, pyg_AUC: 0.4859\n",
      "timestep:375, pyg_AUC: 0.4873\n",
      "timestep:376, pyg_AUC: 0.4887\n",
      "timestep:377, pyg_AUC: 0.4859\n",
      "timestep:378, pyg_AUC: 0.4845\n",
      "timestep:379, pyg_AUC: 0.4845\n",
      "timestep:380, pyg_AUC: 0.4873\n",
      "timestep:381, pyg_AUC: 0.4873\n",
      "timestep:382, pyg_AUC: 0.4873\n",
      "timestep:383, pyg_AUC: 0.4859\n",
      "timestep:384, pyg_AUC: 0.4887\n",
      "timestep:385, pyg_AUC: 0.4873\n",
      "timestep:386, pyg_AUC: 0.4901\n",
      "timestep:387, pyg_AUC: 0.4873\n",
      "timestep:388, pyg_AUC: 0.4859\n",
      "timestep:389, pyg_AUC: 0.4859\n",
      "timestep:390, pyg_AUC: 0.4887\n",
      "timestep:391, pyg_AUC: 0.4887\n",
      "timestep:392, pyg_AUC: 0.4845\n",
      "timestep:393, pyg_AUC: 0.4859\n",
      "timestep:394, pyg_AUC: 0.4915\n",
      "timestep:395, pyg_AUC: 0.4887\n",
      "timestep:396, pyg_AUC: 0.4887\n",
      "timestep:397, pyg_AUC: 0.4845\n",
      "timestep:398, pyg_AUC: 0.4845\n",
      "timestep:399, pyg_AUC: 0.4887\n",
      "timestep:400, pyg_AUC: 0.4859\n",
      "timestep:401, pyg_AUC: 0.4873\n",
      "timestep:402, pyg_AUC: 0.4887\n",
      "timestep:403, pyg_AUC: 0.4845\n",
      "timestep:404, pyg_AUC: 0.4873\n",
      "timestep:405, pyg_AUC: 0.4859\n",
      "timestep:406, pyg_AUC: 0.4887\n",
      "timestep:407, pyg_AUC: 0.4873\n",
      "timestep:408, pyg_AUC: 0.4873\n",
      "timestep:409, pyg_AUC: 0.4859\n",
      "timestep:410, pyg_AUC: 0.4887\n",
      "timestep:411, pyg_AUC: 0.4859\n",
      "timestep:412, pyg_AUC: 0.4845\n",
      "timestep:413, pyg_AUC: 0.4859\n",
      "timestep:414, pyg_AUC: 0.4887\n",
      "timestep:415, pyg_AUC: 0.4859\n",
      "timestep:416, pyg_AUC: 0.4859\n",
      "timestep:417, pyg_AUC: 0.4845\n",
      "timestep:418, pyg_AUC: 0.4901\n",
      "timestep:419, pyg_AUC: 0.4873\n",
      "timestep:420, pyg_AUC: 0.4859\n",
      "timestep:421, pyg_AUC: 0.4873\n",
      "timestep:422, pyg_AUC: 0.4873\n",
      "timestep:423, pyg_AUC: 0.4915\n",
      "timestep:424, pyg_AUC: 0.4901\n",
      "timestep:425, pyg_AUC: 0.4873\n",
      "timestep:426, pyg_AUC: 0.4859\n",
      "timestep:427, pyg_AUC: 0.4859\n",
      "timestep:428, pyg_AUC: 0.4859\n",
      "timestep:429, pyg_AUC: 0.4887\n",
      "timestep:430, pyg_AUC: 0.4887\n",
      "timestep:431, pyg_AUC: 0.4873\n",
      "timestep:432, pyg_AUC: 0.4873\n",
      "timestep:433, pyg_AUC: 0.4873\n",
      "timestep:434, pyg_AUC: 0.4859\n",
      "timestep:435, pyg_AUC: 0.4887\n",
      "timestep:436, pyg_AUC: 0.4887\n",
      "timestep:437, pyg_AUC: 0.4873\n",
      "timestep:438, pyg_AUC: 0.4887\n",
      "timestep:439, pyg_AUC: 0.4901\n",
      "timestep:440, pyg_AUC: 0.4845\n",
      "timestep:441, pyg_AUC: 0.4901\n",
      "timestep:442, pyg_AUC: 0.4873\n",
      "timestep:443, pyg_AUC: 0.4873\n",
      "timestep:444, pyg_AUC: 0.4873\n",
      "timestep:445, pyg_AUC: 0.4859\n",
      "timestep:446, pyg_AUC: 0.4901\n",
      "timestep:447, pyg_AUC: 0.4901\n",
      "timestep:448, pyg_AUC: 0.4901\n",
      "timestep:449, pyg_AUC: 0.4859\n",
      "timestep:450, pyg_AUC: 0.4915\n",
      "timestep:451, pyg_AUC: 0.4887\n",
      "timestep:452, pyg_AUC: 0.4887\n",
      "timestep:453, pyg_AUC: 0.4887\n",
      "timestep:454, pyg_AUC: 0.4873\n",
      "timestep:455, pyg_AUC: 0.4873\n",
      "timestep:456, pyg_AUC: 0.4887\n",
      "timestep:457, pyg_AUC: 0.4859\n",
      "timestep:458, pyg_AUC: 0.4887\n",
      "timestep:459, pyg_AUC: 0.4901\n",
      "timestep:460, pyg_AUC: 0.4873\n",
      "timestep:461, pyg_AUC: 0.4901\n",
      "timestep:462, pyg_AUC: 0.4887\n",
      "timestep:463, pyg_AUC: 0.4873\n",
      "timestep:464, pyg_AUC: 0.4887\n",
      "timestep:465, pyg_AUC: 0.4901\n",
      "timestep:466, pyg_AUC: 0.4859\n",
      "timestep:467, pyg_AUC: 0.4859\n",
      "timestep:468, pyg_AUC: 0.4859\n",
      "timestep:469, pyg_AUC: 0.4887\n",
      "timestep:470, pyg_AUC: 0.4873\n",
      "timestep:471, pyg_AUC: 0.4915\n",
      "timestep:472, pyg_AUC: 0.4873\n",
      "timestep:473, pyg_AUC: 0.4845\n",
      "timestep:474, pyg_AUC: 0.4873\n",
      "timestep:475, pyg_AUC: 0.4901\n",
      "timestep:476, pyg_AUC: 0.4887\n",
      "timestep:477, pyg_AUC: 0.4901\n",
      "timestep:478, pyg_AUC: 0.4901\n",
      "timestep:479, pyg_AUC: 0.4901\n",
      "timestep:480, pyg_AUC: 0.4887\n",
      "timestep:481, pyg_AUC: 0.4873\n",
      "timestep:482, pyg_AUC: 0.4887\n",
      "timestep:483, pyg_AUC: 0.4873\n",
      "timestep:484, pyg_AUC: 0.4859\n",
      "timestep:485, pyg_AUC: 0.4901\n",
      "timestep:486, pyg_AUC: 0.4887\n",
      "timestep:487, pyg_AUC: 0.4915\n",
      "timestep:488, pyg_AUC: 0.4901\n",
      "timestep:489, pyg_AUC: 0.4901\n",
      "timestep:490, pyg_AUC: 0.4873\n",
      "timestep:491, pyg_AUC: 0.4901\n",
      "timestep:492, pyg_AUC: 0.4845\n",
      "timestep:493, pyg_AUC: 0.4873\n",
      "timestep:494, pyg_AUC: 0.4887\n",
      "timestep:495, pyg_AUC: 0.4873\n",
      "timestep:496, pyg_AUC: 0.4901\n",
      "timestep:497, pyg_AUC: 0.4845\n",
      "timestep:498, pyg_AUC: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [06:41<26:49, 100.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:499, pyg_AUC: 0.4901\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 41.68215\n",
      "Epoch: 0010 loss= 28.47275\n",
      "Epoch: 0020 loss= 24.73520\n",
      "Epoch: 0030 loss= 14.04542\n",
      "Epoch: 0040 loss= 9.71388\n",
      "Epoch: 0050 loss= 2.42823\n",
      "Epoch: 0060 loss= 1.32007\n",
      "Epoch: 0070 loss= 0.89542\n",
      "Epoch: 0080 loss= 0.67976\n",
      "Epoch: 0090 loss= 0.90967\n",
      "Epoch: 0100 loss= 0.65428\n",
      "Epoch: 0110 loss= 0.61078\n",
      "Epoch: 0120 loss= 0.60525\n",
      "Epoch: 0130 loss= 0.59291\n",
      "Epoch: 0140 loss= 0.64545\n",
      "Epoch: 0150 loss= 0.65498\n",
      "Epoch: 0160 loss= 0.68747\n",
      "Epoch: 0170 loss= 0.66787\n",
      "Epoch: 0180 loss= 0.65678\n",
      "Epoch: 0190 loss= 0.63102\n",
      "Epoch: 0200 loss= 0.63408\n",
      "Epoch: 0210 loss= 0.54873\n",
      "Epoch: 0220 loss= 0.67817\n",
      "Epoch: 0230 loss= 0.57302\n",
      "Epoch: 0240 loss= 0.61365\n",
      "Epoch: 0250 loss= 0.67216\n",
      "Epoch: 0260 loss= 0.52865\n",
      "Epoch: 0270 loss= 0.58357\n",
      "Epoch: 0280 loss= 0.61939\n",
      "Epoch: 0290 loss= 0.54986\n",
      "Epoch: 0300 loss= 0.57297\n",
      "Epoch: 0310 loss= 0.57103\n",
      "Epoch: 0320 loss= 0.60140\n",
      "Epoch: 0330 loss= 0.61300\n",
      "Epoch: 0340 loss= 0.47295\n",
      "Epoch: 0350 loss= 0.55409\n",
      "Epoch: 0360 loss= 0.59453\n",
      "Epoch: 0370 loss= 0.58004\n",
      "Epoch: 0380 loss= 0.50537\n",
      "Epoch: 0390 loss= 0.53394\n",
      "Epoch: 0400 loss= 0.55163\n",
      "Epoch: 0410 loss= 0.62536\n",
      "Epoch: 0420 loss= 0.58311\n",
      "Epoch: 0430 loss= 0.53999\n",
      "Epoch: 0440 loss= 0.60260\n",
      "Epoch: 0450 loss= 0.63682\n",
      "Epoch: 0460 loss= 0.56125\n",
      "Epoch: 0470 loss= 0.51828\n",
      "Epoch: 0480 loss= 0.64741\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7759,  4.7073, -5.3940, -4.6514, -4.3894,  5.6909,  5.4800, -5.2654]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 37.81384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss= 25.91788\n",
      "Epoch: 0020 loss= 16.83441\n",
      "Epoch: 0030 loss= 12.59090\n",
      "Epoch: 0040 loss= 4.39100\n",
      "Epoch: 0050 loss= 1.13026\n",
      "Epoch: 0060 loss= 0.71802\n",
      "Epoch: 0070 loss= 0.71378\n",
      "Epoch: 0080 loss= 0.73922\n",
      "Epoch: 0090 loss= 0.91783\n",
      "Epoch: 0100 loss= 0.64987\n",
      "Epoch: 0110 loss= 0.58740\n",
      "Epoch: 0120 loss= 0.65692\n",
      "Epoch: 0130 loss= 0.66047\n",
      "Epoch: 0140 loss= 0.58717\n",
      "Epoch: 0150 loss= 0.63363\n",
      "Epoch: 0160 loss= 0.55541\n",
      "Epoch: 0170 loss= 0.60270\n",
      "Epoch: 0180 loss= 0.67354\n",
      "Epoch: 0190 loss= 0.63424\n",
      "Epoch: 0200 loss= 0.61216\n",
      "Epoch: 0210 loss= 0.60508\n",
      "Epoch: 0220 loss= 0.61136\n",
      "Epoch: 0230 loss= 0.60094\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4873\n",
      "timestep:1, pyg_AUC: 0.4901\n",
      "timestep:2, pyg_AUC: 0.4887\n",
      "timestep:3, pyg_AUC: 0.4887\n",
      "timestep:4, pyg_AUC: 0.4887\n",
      "timestep:5, pyg_AUC: 0.4915\n",
      "timestep:6, pyg_AUC: 0.4816\n",
      "timestep:7, pyg_AUC: 0.4929\n",
      "timestep:8, pyg_AUC: 0.4845\n",
      "timestep:9, pyg_AUC: 0.4887\n",
      "timestep:10, pyg_AUC: 0.4901\n",
      "timestep:11, pyg_AUC: 0.4873\n",
      "timestep:12, pyg_AUC: 0.4944\n",
      "timestep:13, pyg_AUC: 0.4887\n",
      "timestep:14, pyg_AUC: 0.4915\n",
      "timestep:15, pyg_AUC: 0.4929\n",
      "timestep:16, pyg_AUC: 0.4929\n",
      "timestep:17, pyg_AUC: 0.4901\n",
      "timestep:18, pyg_AUC: 0.4915\n",
      "timestep:19, pyg_AUC: 0.4901\n",
      "timestep:20, pyg_AUC: 0.4944\n",
      "timestep:21, pyg_AUC: 0.4887\n",
      "timestep:22, pyg_AUC: 0.4887\n",
      "timestep:23, pyg_AUC: 0.4831\n",
      "timestep:24, pyg_AUC: 0.4901\n",
      "timestep:25, pyg_AUC: 0.4901\n",
      "timestep:26, pyg_AUC: 0.4887\n",
      "timestep:27, pyg_AUC: 0.4887\n",
      "timestep:28, pyg_AUC: 0.4901\n",
      "timestep:29, pyg_AUC: 0.4873\n",
      "timestep:30, pyg_AUC: 0.4887\n",
      "timestep:31, pyg_AUC: 0.4859\n",
      "timestep:32, pyg_AUC: 0.4887\n",
      "timestep:33, pyg_AUC: 0.4845\n",
      "timestep:34, pyg_AUC: 0.4901\n",
      "timestep:35, pyg_AUC: 0.4873\n",
      "timestep:36, pyg_AUC: 0.4901\n",
      "timestep:37, pyg_AUC: 0.4873\n",
      "timestep:38, pyg_AUC: 0.4873\n",
      "timestep:39, pyg_AUC: 0.4887\n",
      "timestep:40, pyg_AUC: 0.4901\n",
      "timestep:41, pyg_AUC: 0.4901\n",
      "timestep:42, pyg_AUC: 0.4901\n",
      "timestep:43, pyg_AUC: 0.4859\n",
      "timestep:44, pyg_AUC: 0.4915\n",
      "timestep:45, pyg_AUC: 0.4887\n",
      "timestep:46, pyg_AUC: 0.4901\n",
      "timestep:47, pyg_AUC: 0.4887\n",
      "timestep:48, pyg_AUC: 0.4972\n",
      "timestep:49, pyg_AUC: 0.4887\n",
      "timestep:50, pyg_AUC: 0.4915\n",
      "timestep:51, pyg_AUC: 0.4915\n",
      "timestep:52, pyg_AUC: 0.4901\n",
      "timestep:53, pyg_AUC: 0.4831\n",
      "timestep:54, pyg_AUC: 0.4887\n",
      "timestep:55, pyg_AUC: 0.4887\n",
      "timestep:56, pyg_AUC: 0.4845\n",
      "timestep:57, pyg_AUC: 0.4887\n",
      "timestep:58, pyg_AUC: 0.4859\n",
      "timestep:59, pyg_AUC: 0.4915\n",
      "timestep:60, pyg_AUC: 0.4901\n",
      "timestep:61, pyg_AUC: 0.4915\n",
      "timestep:62, pyg_AUC: 0.4887\n",
      "timestep:63, pyg_AUC: 0.4915\n",
      "timestep:64, pyg_AUC: 0.4873\n",
      "timestep:65, pyg_AUC: 0.4887\n",
      "timestep:66, pyg_AUC: 0.4887\n",
      "timestep:67, pyg_AUC: 0.4887\n",
      "timestep:68, pyg_AUC: 0.4915\n",
      "timestep:69, pyg_AUC: 0.4873\n",
      "timestep:70, pyg_AUC: 0.4859\n",
      "timestep:71, pyg_AUC: 0.4845\n",
      "timestep:72, pyg_AUC: 0.4901\n",
      "timestep:73, pyg_AUC: 0.4929\n",
      "timestep:74, pyg_AUC: 0.4859\n",
      "timestep:75, pyg_AUC: 0.4887\n",
      "timestep:76, pyg_AUC: 0.4887\n",
      "timestep:77, pyg_AUC: 0.4958\n",
      "timestep:78, pyg_AUC: 0.4887\n",
      "timestep:79, pyg_AUC: 0.4944\n",
      "timestep:80, pyg_AUC: 0.4887\n",
      "timestep:81, pyg_AUC: 0.4929\n",
      "timestep:82, pyg_AUC: 0.4859\n",
      "timestep:83, pyg_AUC: 0.4859\n",
      "timestep:84, pyg_AUC: 0.4887\n",
      "timestep:85, pyg_AUC: 0.4873\n",
      "timestep:86, pyg_AUC: 0.4972\n",
      "timestep:87, pyg_AUC: 0.4873\n",
      "timestep:88, pyg_AUC: 0.4944\n",
      "timestep:89, pyg_AUC: 0.4859\n",
      "timestep:90, pyg_AUC: 0.4887\n",
      "timestep:91, pyg_AUC: 0.4915\n",
      "timestep:92, pyg_AUC: 0.4859\n",
      "timestep:93, pyg_AUC: 0.4859\n",
      "timestep:94, pyg_AUC: 0.4873\n",
      "timestep:95, pyg_AUC: 0.4901\n",
      "timestep:96, pyg_AUC: 0.4915\n",
      "timestep:97, pyg_AUC: 0.4859\n",
      "timestep:98, pyg_AUC: 0.4915\n",
      "timestep:99, pyg_AUC: 0.4873\n",
      "timestep:100, pyg_AUC: 0.4887\n",
      "timestep:101, pyg_AUC: 0.4887\n",
      "timestep:102, pyg_AUC: 0.4944\n",
      "timestep:103, pyg_AUC: 0.4915\n",
      "timestep:104, pyg_AUC: 0.4887\n",
      "timestep:105, pyg_AUC: 0.4915\n",
      "timestep:106, pyg_AUC: 0.4845\n",
      "timestep:107, pyg_AUC: 0.4873\n",
      "timestep:108, pyg_AUC: 0.4887\n",
      "timestep:109, pyg_AUC: 0.4901\n",
      "timestep:110, pyg_AUC: 0.4929\n",
      "timestep:111, pyg_AUC: 0.4887\n",
      "timestep:112, pyg_AUC: 0.4887\n",
      "timestep:113, pyg_AUC: 0.4901\n",
      "timestep:114, pyg_AUC: 0.4859\n",
      "timestep:115, pyg_AUC: 0.4873\n",
      "timestep:116, pyg_AUC: 0.4901\n",
      "timestep:117, pyg_AUC: 0.4901\n",
      "timestep:118, pyg_AUC: 0.4887\n",
      "timestep:119, pyg_AUC: 0.4901\n",
      "timestep:120, pyg_AUC: 0.4831\n",
      "timestep:121, pyg_AUC: 0.4929\n",
      "timestep:122, pyg_AUC: 0.4873\n",
      "timestep:123, pyg_AUC: 0.4901\n",
      "timestep:124, pyg_AUC: 0.4915\n",
      "timestep:125, pyg_AUC: 0.4887\n",
      "timestep:126, pyg_AUC: 0.4887\n",
      "timestep:127, pyg_AUC: 0.4859\n",
      "timestep:128, pyg_AUC: 0.4929\n",
      "timestep:129, pyg_AUC: 0.4873\n",
      "timestep:130, pyg_AUC: 0.4873\n",
      "timestep:131, pyg_AUC: 0.4929\n",
      "timestep:132, pyg_AUC: 0.4859\n",
      "timestep:133, pyg_AUC: 0.4901\n",
      "timestep:134, pyg_AUC: 0.4915\n",
      "timestep:135, pyg_AUC: 0.4887\n",
      "timestep:136, pyg_AUC: 0.4901\n",
      "timestep:137, pyg_AUC: 0.4802\n",
      "timestep:138, pyg_AUC: 0.4845\n",
      "timestep:139, pyg_AUC: 0.4915\n",
      "timestep:140, pyg_AUC: 0.4915\n",
      "timestep:141, pyg_AUC: 0.4859\n",
      "timestep:142, pyg_AUC: 0.4802\n",
      "timestep:143, pyg_AUC: 0.4831\n",
      "timestep:144, pyg_AUC: 0.4859\n",
      "timestep:145, pyg_AUC: 0.4915\n",
      "timestep:146, pyg_AUC: 0.4831\n",
      "timestep:147, pyg_AUC: 0.4887\n",
      "timestep:148, pyg_AUC: 0.4845\n",
      "timestep:149, pyg_AUC: 0.4873\n",
      "timestep:150, pyg_AUC: 0.4873\n",
      "timestep:151, pyg_AUC: 0.4901\n",
      "timestep:152, pyg_AUC: 0.4859\n",
      "timestep:153, pyg_AUC: 0.4915\n",
      "timestep:154, pyg_AUC: 0.4901\n",
      "timestep:155, pyg_AUC: 0.4887\n",
      "timestep:156, pyg_AUC: 0.4901\n",
      "timestep:157, pyg_AUC: 0.4845\n",
      "timestep:158, pyg_AUC: 0.4887\n",
      "timestep:159, pyg_AUC: 0.4873\n",
      "timestep:160, pyg_AUC: 0.4887\n",
      "timestep:161, pyg_AUC: 0.4929\n",
      "timestep:162, pyg_AUC: 0.4929\n",
      "timestep:163, pyg_AUC: 0.4944\n",
      "timestep:164, pyg_AUC: 0.4873\n",
      "timestep:165, pyg_AUC: 0.4915\n",
      "timestep:166, pyg_AUC: 0.4929\n",
      "timestep:167, pyg_AUC: 0.4901\n",
      "timestep:168, pyg_AUC: 0.4929\n",
      "timestep:169, pyg_AUC: 0.4845\n",
      "timestep:170, pyg_AUC: 0.4915\n",
      "timestep:171, pyg_AUC: 0.4901\n",
      "timestep:172, pyg_AUC: 0.4873\n",
      "timestep:173, pyg_AUC: 0.4915\n",
      "timestep:174, pyg_AUC: 0.4887\n",
      "timestep:175, pyg_AUC: 0.4873\n",
      "timestep:176, pyg_AUC: 0.4887\n",
      "timestep:177, pyg_AUC: 0.4873\n",
      "timestep:178, pyg_AUC: 0.4859\n",
      "timestep:179, pyg_AUC: 0.4859\n",
      "timestep:180, pyg_AUC: 0.4929\n",
      "timestep:181, pyg_AUC: 0.4901\n",
      "timestep:182, pyg_AUC: 0.4873\n",
      "timestep:183, pyg_AUC: 0.4901\n",
      "timestep:184, pyg_AUC: 0.4915\n",
      "timestep:185, pyg_AUC: 0.4901\n",
      "timestep:186, pyg_AUC: 0.4859\n",
      "timestep:187, pyg_AUC: 0.4887\n",
      "timestep:188, pyg_AUC: 0.4915\n",
      "timestep:189, pyg_AUC: 0.4901\n",
      "timestep:190, pyg_AUC: 0.4887\n",
      "timestep:191, pyg_AUC: 0.4873\n",
      "timestep:192, pyg_AUC: 0.4887\n",
      "timestep:193, pyg_AUC: 0.4873\n",
      "timestep:194, pyg_AUC: 0.4887\n",
      "timestep:195, pyg_AUC: 0.4845\n",
      "timestep:196, pyg_AUC: 0.4873\n",
      "timestep:197, pyg_AUC: 0.4859\n",
      "timestep:198, pyg_AUC: 0.4901\n",
      "timestep:199, pyg_AUC: 0.4901\n",
      "timestep:200, pyg_AUC: 0.4901\n",
      "timestep:201, pyg_AUC: 0.4901\n",
      "timestep:202, pyg_AUC: 0.4873\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4901\n",
      "timestep:205, pyg_AUC: 0.4901\n",
      "timestep:206, pyg_AUC: 0.4915\n",
      "timestep:207, pyg_AUC: 0.4915\n",
      "timestep:208, pyg_AUC: 0.4831\n",
      "timestep:209, pyg_AUC: 0.4873\n",
      "timestep:210, pyg_AUC: 0.4887\n",
      "timestep:211, pyg_AUC: 0.4873\n",
      "timestep:212, pyg_AUC: 0.4859\n",
      "timestep:213, pyg_AUC: 0.4901\n",
      "timestep:214, pyg_AUC: 0.4831\n",
      "timestep:215, pyg_AUC: 0.4901\n",
      "timestep:216, pyg_AUC: 0.4873\n",
      "timestep:217, pyg_AUC: 0.4901\n",
      "timestep:218, pyg_AUC: 0.4859\n",
      "timestep:219, pyg_AUC: 0.4901\n",
      "timestep:220, pyg_AUC: 0.4859\n",
      "timestep:221, pyg_AUC: 0.4873\n",
      "timestep:222, pyg_AUC: 0.4873\n",
      "timestep:223, pyg_AUC: 0.4831\n",
      "timestep:224, pyg_AUC: 0.4816\n",
      "timestep:225, pyg_AUC: 0.4901\n",
      "timestep:226, pyg_AUC: 0.4887\n",
      "timestep:227, pyg_AUC: 0.4873\n",
      "timestep:228, pyg_AUC: 0.4873\n",
      "timestep:229, pyg_AUC: 0.4887\n",
      "timestep:230, pyg_AUC: 0.4845\n",
      "timestep:231, pyg_AUC: 0.4887\n",
      "timestep:232, pyg_AUC: 0.4887\n",
      "timestep:233, pyg_AUC: 0.4887\n",
      "timestep:234, pyg_AUC: 0.4859\n",
      "timestep:235, pyg_AUC: 0.4873\n",
      "timestep:236, pyg_AUC: 0.4845\n",
      "timestep:237, pyg_AUC: 0.4901\n",
      "timestep:238, pyg_AUC: 0.4887\n",
      "timestep:239, pyg_AUC: 0.4845\n",
      "timestep:240, pyg_AUC: 0.4873\n",
      "timestep:241, pyg_AUC: 0.4873\n",
      "timestep:242, pyg_AUC: 0.4887\n",
      "timestep:243, pyg_AUC: 0.4845\n",
      "timestep:244, pyg_AUC: 0.4901\n",
      "timestep:245, pyg_AUC: 0.4873\n",
      "timestep:246, pyg_AUC: 0.4845\n",
      "timestep:247, pyg_AUC: 0.4901\n",
      "timestep:248, pyg_AUC: 0.4873\n",
      "timestep:249, pyg_AUC: 0.4901\n",
      "timestep:250, pyg_AUC: 0.4887\n",
      "timestep:251, pyg_AUC: 0.4859\n",
      "timestep:252, pyg_AUC: 0.4845\n",
      "timestep:253, pyg_AUC: 0.4845\n",
      "timestep:254, pyg_AUC: 0.4873\n",
      "timestep:255, pyg_AUC: 0.4845\n",
      "timestep:256, pyg_AUC: 0.4887\n",
      "timestep:257, pyg_AUC: 0.4887\n",
      "timestep:258, pyg_AUC: 0.4845\n",
      "timestep:259, pyg_AUC: 0.4859\n",
      "timestep:260, pyg_AUC: 0.4873\n",
      "timestep:261, pyg_AUC: 0.4873\n",
      "timestep:262, pyg_AUC: 0.4901\n",
      "timestep:263, pyg_AUC: 0.4873\n",
      "timestep:264, pyg_AUC: 0.4859\n",
      "timestep:265, pyg_AUC: 0.4901\n",
      "timestep:266, pyg_AUC: 0.4845\n",
      "timestep:267, pyg_AUC: 0.4859\n",
      "timestep:268, pyg_AUC: 0.4845\n",
      "timestep:269, pyg_AUC: 0.4845\n",
      "timestep:270, pyg_AUC: 0.4859\n",
      "timestep:271, pyg_AUC: 0.4859\n",
      "timestep:272, pyg_AUC: 0.4816\n",
      "timestep:273, pyg_AUC: 0.4859\n",
      "timestep:274, pyg_AUC: 0.4845\n",
      "timestep:275, pyg_AUC: 0.4887\n",
      "timestep:276, pyg_AUC: 0.4859\n",
      "timestep:277, pyg_AUC: 0.4873\n",
      "timestep:278, pyg_AUC: 0.4873\n",
      "timestep:279, pyg_AUC: 0.4831\n",
      "timestep:280, pyg_AUC: 0.4831\n",
      "timestep:281, pyg_AUC: 0.4887\n",
      "timestep:282, pyg_AUC: 0.4901\n",
      "timestep:283, pyg_AUC: 0.4831\n",
      "timestep:284, pyg_AUC: 0.4845\n",
      "timestep:285, pyg_AUC: 0.4887\n",
      "timestep:286, pyg_AUC: 0.4845\n",
      "timestep:287, pyg_AUC: 0.4915\n",
      "timestep:288, pyg_AUC: 0.4873\n",
      "timestep:289, pyg_AUC: 0.4859\n",
      "timestep:290, pyg_AUC: 0.4901\n",
      "timestep:291, pyg_AUC: 0.4887\n",
      "timestep:292, pyg_AUC: 0.4859\n",
      "timestep:293, pyg_AUC: 0.4831\n",
      "timestep:294, pyg_AUC: 0.4831\n",
      "timestep:295, pyg_AUC: 0.4873\n",
      "timestep:296, pyg_AUC: 0.4816\n",
      "timestep:297, pyg_AUC: 0.4845\n",
      "timestep:298, pyg_AUC: 0.4887\n",
      "timestep:299, pyg_AUC: 0.4873\n",
      "timestep:300, pyg_AUC: 0.4887\n",
      "timestep:301, pyg_AUC: 0.4873\n",
      "timestep:302, pyg_AUC: 0.4831\n",
      "timestep:303, pyg_AUC: 0.4845\n",
      "timestep:304, pyg_AUC: 0.4845\n",
      "timestep:305, pyg_AUC: 0.4845\n",
      "timestep:306, pyg_AUC: 0.4873\n",
      "timestep:307, pyg_AUC: 0.4859\n",
      "timestep:308, pyg_AUC: 0.4831\n",
      "timestep:309, pyg_AUC: 0.4859\n",
      "timestep:310, pyg_AUC: 0.4859\n",
      "timestep:311, pyg_AUC: 0.4887\n",
      "timestep:312, pyg_AUC: 0.4845\n",
      "timestep:313, pyg_AUC: 0.4845\n",
      "timestep:314, pyg_AUC: 0.4887\n",
      "timestep:315, pyg_AUC: 0.4887\n",
      "timestep:316, pyg_AUC: 0.4873\n",
      "timestep:317, pyg_AUC: 0.4859\n",
      "timestep:318, pyg_AUC: 0.4873\n",
      "timestep:319, pyg_AUC: 0.4873\n",
      "timestep:320, pyg_AUC: 0.4845\n",
      "timestep:321, pyg_AUC: 0.4831\n",
      "timestep:322, pyg_AUC: 0.4873\n",
      "timestep:323, pyg_AUC: 0.4901\n",
      "timestep:324, pyg_AUC: 0.4859\n",
      "timestep:325, pyg_AUC: 0.4887\n",
      "timestep:326, pyg_AUC: 0.4845\n",
      "timestep:327, pyg_AUC: 0.4873\n",
      "timestep:328, pyg_AUC: 0.4859\n",
      "timestep:329, pyg_AUC: 0.4859\n",
      "timestep:330, pyg_AUC: 0.4873\n",
      "timestep:331, pyg_AUC: 0.4845\n",
      "timestep:332, pyg_AUC: 0.4859\n",
      "timestep:333, pyg_AUC: 0.4845\n",
      "timestep:334, pyg_AUC: 0.4845\n",
      "timestep:335, pyg_AUC: 0.4887\n",
      "timestep:336, pyg_AUC: 0.4873\n",
      "timestep:337, pyg_AUC: 0.4873\n",
      "timestep:338, pyg_AUC: 0.4831\n",
      "timestep:339, pyg_AUC: 0.4873\n",
      "timestep:340, pyg_AUC: 0.4887\n",
      "timestep:341, pyg_AUC: 0.4873\n",
      "timestep:342, pyg_AUC: 0.4873\n",
      "timestep:343, pyg_AUC: 0.4859\n",
      "timestep:344, pyg_AUC: 0.4887\n",
      "timestep:345, pyg_AUC: 0.4845\n",
      "timestep:346, pyg_AUC: 0.4887\n",
      "timestep:347, pyg_AUC: 0.4873\n",
      "timestep:348, pyg_AUC: 0.4887\n",
      "timestep:349, pyg_AUC: 0.4901\n",
      "timestep:350, pyg_AUC: 0.4859\n",
      "timestep:351, pyg_AUC: 0.4859\n",
      "timestep:352, pyg_AUC: 0.4873\n",
      "timestep:353, pyg_AUC: 0.4859\n",
      "timestep:354, pyg_AUC: 0.4859\n",
      "timestep:355, pyg_AUC: 0.4845\n",
      "timestep:356, pyg_AUC: 0.4859\n",
      "timestep:357, pyg_AUC: 0.4873\n",
      "timestep:358, pyg_AUC: 0.4831\n",
      "timestep:359, pyg_AUC: 0.4845\n",
      "timestep:360, pyg_AUC: 0.4859\n",
      "timestep:361, pyg_AUC: 0.4831\n",
      "timestep:362, pyg_AUC: 0.4873\n",
      "timestep:363, pyg_AUC: 0.4887\n",
      "timestep:364, pyg_AUC: 0.4845\n",
      "timestep:365, pyg_AUC: 0.4873\n",
      "timestep:366, pyg_AUC: 0.4845\n",
      "timestep:367, pyg_AUC: 0.4887\n",
      "timestep:368, pyg_AUC: 0.4845\n",
      "timestep:369, pyg_AUC: 0.4887\n",
      "timestep:370, pyg_AUC: 0.4887\n",
      "timestep:371, pyg_AUC: 0.4859\n",
      "timestep:372, pyg_AUC: 0.4845\n",
      "timestep:373, pyg_AUC: 0.4873\n",
      "timestep:374, pyg_AUC: 0.4873\n",
      "timestep:375, pyg_AUC: 0.4901\n",
      "timestep:376, pyg_AUC: 0.4887\n",
      "timestep:377, pyg_AUC: 0.4915\n",
      "timestep:378, pyg_AUC: 0.4873\n",
      "timestep:379, pyg_AUC: 0.4887\n",
      "timestep:380, pyg_AUC: 0.4887\n",
      "timestep:381, pyg_AUC: 0.4859\n",
      "timestep:382, pyg_AUC: 0.4901\n",
      "timestep:383, pyg_AUC: 0.4901\n",
      "timestep:384, pyg_AUC: 0.4873\n",
      "timestep:385, pyg_AUC: 0.4887\n",
      "timestep:386, pyg_AUC: 0.4901\n",
      "timestep:387, pyg_AUC: 0.4887\n",
      "timestep:388, pyg_AUC: 0.4873\n",
      "timestep:389, pyg_AUC: 0.4873\n",
      "timestep:390, pyg_AUC: 0.4873\n",
      "timestep:391, pyg_AUC: 0.4887\n",
      "timestep:392, pyg_AUC: 0.4845\n",
      "timestep:393, pyg_AUC: 0.4873\n",
      "timestep:394, pyg_AUC: 0.4845\n",
      "timestep:395, pyg_AUC: 0.4915\n",
      "timestep:396, pyg_AUC: 0.4873\n",
      "timestep:397, pyg_AUC: 0.4887\n",
      "timestep:398, pyg_AUC: 0.4859\n",
      "timestep:399, pyg_AUC: 0.4859\n",
      "timestep:400, pyg_AUC: 0.4845\n",
      "timestep:401, pyg_AUC: 0.4901\n",
      "timestep:402, pyg_AUC: 0.4887\n",
      "timestep:403, pyg_AUC: 0.4873\n",
      "timestep:404, pyg_AUC: 0.4845\n",
      "timestep:405, pyg_AUC: 0.4873\n",
      "timestep:406, pyg_AUC: 0.4873\n",
      "timestep:407, pyg_AUC: 0.4929\n",
      "timestep:408, pyg_AUC: 0.4901\n",
      "timestep:409, pyg_AUC: 0.4887\n",
      "timestep:410, pyg_AUC: 0.4887\n",
      "timestep:411, pyg_AUC: 0.4887\n",
      "timestep:412, pyg_AUC: 0.4873\n",
      "timestep:413, pyg_AUC: 0.4887\n",
      "timestep:414, pyg_AUC: 0.4845\n",
      "timestep:415, pyg_AUC: 0.4887\n",
      "timestep:416, pyg_AUC: 0.4845\n",
      "timestep:417, pyg_AUC: 0.4873\n",
      "timestep:418, pyg_AUC: 0.4873\n",
      "timestep:419, pyg_AUC: 0.4887\n",
      "timestep:420, pyg_AUC: 0.4901\n",
      "timestep:421, pyg_AUC: 0.4887\n",
      "timestep:422, pyg_AUC: 0.4845\n",
      "timestep:423, pyg_AUC: 0.4901\n",
      "timestep:424, pyg_AUC: 0.4901\n",
      "timestep:425, pyg_AUC: 0.4901\n",
      "timestep:426, pyg_AUC: 0.4859\n",
      "timestep:427, pyg_AUC: 0.4873\n",
      "timestep:428, pyg_AUC: 0.4859\n",
      "timestep:429, pyg_AUC: 0.4845\n",
      "timestep:430, pyg_AUC: 0.4859\n",
      "timestep:431, pyg_AUC: 0.4915\n",
      "timestep:432, pyg_AUC: 0.4887\n",
      "timestep:433, pyg_AUC: 0.4901\n",
      "timestep:434, pyg_AUC: 0.4901\n",
      "timestep:435, pyg_AUC: 0.4887\n",
      "timestep:436, pyg_AUC: 0.4845\n",
      "timestep:437, pyg_AUC: 0.4915\n",
      "timestep:438, pyg_AUC: 0.4887\n",
      "timestep:439, pyg_AUC: 0.4845\n",
      "timestep:440, pyg_AUC: 0.4901\n",
      "timestep:441, pyg_AUC: 0.4859\n",
      "timestep:442, pyg_AUC: 0.4915\n",
      "timestep:443, pyg_AUC: 0.4887\n",
      "timestep:444, pyg_AUC: 0.4873\n",
      "timestep:445, pyg_AUC: 0.4887\n",
      "timestep:446, pyg_AUC: 0.4887\n",
      "timestep:447, pyg_AUC: 0.4873\n",
      "timestep:448, pyg_AUC: 0.4859\n",
      "timestep:449, pyg_AUC: 0.4831\n",
      "timestep:450, pyg_AUC: 0.4859\n",
      "timestep:451, pyg_AUC: 0.4887\n",
      "timestep:452, pyg_AUC: 0.4887\n",
      "timestep:453, pyg_AUC: 0.4859\n",
      "timestep:454, pyg_AUC: 0.4845\n",
      "timestep:455, pyg_AUC: 0.4915\n",
      "timestep:456, pyg_AUC: 0.4901\n",
      "timestep:457, pyg_AUC: 0.4859\n",
      "timestep:458, pyg_AUC: 0.4845\n",
      "timestep:459, pyg_AUC: 0.4859\n",
      "timestep:460, pyg_AUC: 0.4887\n",
      "timestep:461, pyg_AUC: 0.4873\n",
      "timestep:462, pyg_AUC: 0.4887\n",
      "timestep:463, pyg_AUC: 0.4859\n",
      "timestep:464, pyg_AUC: 0.4915\n",
      "timestep:465, pyg_AUC: 0.4845\n",
      "timestep:466, pyg_AUC: 0.4873\n",
      "timestep:467, pyg_AUC: 0.4887\n",
      "timestep:468, pyg_AUC: 0.4859\n",
      "timestep:469, pyg_AUC: 0.4929\n",
      "timestep:470, pyg_AUC: 0.4901\n",
      "timestep:471, pyg_AUC: 0.4873\n",
      "timestep:472, pyg_AUC: 0.4887\n",
      "timestep:473, pyg_AUC: 0.4887\n",
      "timestep:474, pyg_AUC: 0.4887\n",
      "timestep:475, pyg_AUC: 0.4859\n",
      "timestep:476, pyg_AUC: 0.4873\n",
      "timestep:477, pyg_AUC: 0.4901\n",
      "timestep:478, pyg_AUC: 0.4845\n",
      "timestep:479, pyg_AUC: 0.4887\n",
      "timestep:480, pyg_AUC: 0.4859\n",
      "timestep:481, pyg_AUC: 0.4859\n",
      "timestep:482, pyg_AUC: 0.4887\n",
      "timestep:483, pyg_AUC: 0.4901\n",
      "timestep:484, pyg_AUC: 0.4887\n",
      "timestep:485, pyg_AUC: 0.4873\n",
      "timestep:486, pyg_AUC: 0.4901\n",
      "timestep:487, pyg_AUC: 0.4845\n",
      "timestep:488, pyg_AUC: 0.4901\n",
      "timestep:489, pyg_AUC: 0.4887\n",
      "timestep:490, pyg_AUC: 0.4915\n",
      "timestep:491, pyg_AUC: 0.4873\n",
      "timestep:492, pyg_AUC: 0.4887\n",
      "timestep:493, pyg_AUC: 0.4915\n",
      "timestep:494, pyg_AUC: 0.4873\n",
      "timestep:495, pyg_AUC: 0.4831\n",
      "timestep:496, pyg_AUC: 0.4873\n",
      "timestep:497, pyg_AUC: 0.4859\n",
      "timestep:498, pyg_AUC: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [08:21<25:08, 100.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:499, pyg_AUC: 0.4929\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 36.81496\n",
      "Epoch: 0010 loss= 24.01574\n",
      "Epoch: 0020 loss= 20.37944\n",
      "Epoch: 0030 loss= 17.50788\n",
      "Epoch: 0040 loss= 10.74729\n",
      "Epoch: 0050 loss= 5.85092\n",
      "Epoch: 0060 loss= 2.17113\n",
      "Epoch: 0070 loss= 1.07578\n",
      "Epoch: 0080 loss= 0.98621\n",
      "Epoch: 0090 loss= 0.76385\n",
      "Epoch: 0100 loss= 0.66929\n",
      "Epoch: 0110 loss= 0.67320\n",
      "Epoch: 0120 loss= 0.63755\n",
      "Epoch: 0130 loss= 0.60513\n",
      "Epoch: 0140 loss= 0.61207\n",
      "Epoch: 0150 loss= 0.59184\n",
      "Epoch: 0160 loss= 0.59582\n",
      "Epoch: 0170 loss= 0.55232\n",
      "Epoch: 0180 loss= 0.57364\n",
      "Epoch: 0190 loss= 0.61054\n",
      "Epoch: 0200 loss= 0.61766\n",
      "Epoch: 0210 loss= 0.63381\n",
      "Epoch: 0220 loss= 0.52509\n",
      "Epoch: 0230 loss= 0.64633\n",
      "Epoch: 0240 loss= 0.57522\n",
      "Epoch: 0250 loss= 0.61636\n",
      "Epoch: 0260 loss= 0.51932\n",
      "Epoch: 0270 loss= 0.55697\n",
      "Epoch: 0280 loss= 0.56068\n",
      "Epoch: 0290 loss= 0.60778\n",
      "Epoch: 0300 loss= 0.55290\n",
      "Epoch: 0310 loss= 0.55296\n",
      "Epoch: 0320 loss= 0.56979\n",
      "Epoch: 0330 loss= 0.60260\n",
      "Epoch: 0340 loss= 0.60295\n",
      "Epoch: 0350 loss= 0.56528\n",
      "Epoch: 0360 loss= 0.58474\n",
      "Epoch: 0370 loss= 0.60569\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7845,  4.7227, -5.3415, -4.6534, -4.3907,  5.6056,  5.4442, -5.2412]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 33.31351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss= 21.09327\n",
      "Epoch: 0020 loss= 12.61672\n",
      "Epoch: 0030 loss= 7.89281\n",
      "Epoch: 0040 loss= 2.13680\n",
      "Epoch: 0050 loss= 0.98075\n",
      "Epoch: 0060 loss= 0.79291\n",
      "Epoch: 0070 loss= 0.91469\n",
      "Epoch: 0080 loss= 0.77614\n",
      "Epoch: 0090 loss= 0.62193\n",
      "Epoch: 0100 loss= 1.23853\n",
      "Epoch: 0110 loss= 0.69466\n",
      "Epoch: 0120 loss= 0.57718\n",
      "Epoch: 0130 loss= 0.64769\n",
      "Epoch: 0140 loss= 0.65806\n",
      "Epoch: 0150 loss= 0.53412\n",
      "Epoch: 0160 loss= 0.65618\n",
      "Epoch: 0170 loss= 0.64133\n",
      "Epoch: 0180 loss= 0.62494\n",
      "Epoch: 0190 loss= 0.63264\n",
      "Epoch: 0200 loss= 0.55154\n",
      "Epoch: 0210 loss= 0.58608\n",
      "Epoch: 0220 loss= 0.60385\n",
      "Epoch: 0230 loss= 0.53290\n",
      "Epoch: 0240 loss= 0.61490\n",
      "Epoch: 0250 loss= 0.52041\n",
      "Epoch: 0260 loss= 0.54703\n",
      "Epoch: 0270 loss= 0.59778\n",
      "Epoch: 0280 loss= 0.54817\n",
      "Epoch: 0290 loss= 0.50545\n",
      "Epoch: 0300 loss= 0.61654\n",
      "Epoch: 0310 loss= 0.56467\n",
      "Epoch: 0320 loss= 0.52343\n",
      "Epoch: 0330 loss= 0.54388\n",
      "Epoch: 0340 loss= 0.60762\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4915\n",
      "timestep:1, pyg_AUC: 0.4915\n",
      "timestep:2, pyg_AUC: 0.4873\n",
      "timestep:3, pyg_AUC: 0.4859\n",
      "timestep:4, pyg_AUC: 0.4915\n",
      "timestep:5, pyg_AUC: 0.4915\n",
      "timestep:6, pyg_AUC: 0.4901\n",
      "timestep:7, pyg_AUC: 0.4901\n",
      "timestep:8, pyg_AUC: 0.4845\n",
      "timestep:9, pyg_AUC: 0.4901\n",
      "timestep:10, pyg_AUC: 0.4929\n",
      "timestep:11, pyg_AUC: 0.4845\n",
      "timestep:12, pyg_AUC: 0.4845\n",
      "timestep:13, pyg_AUC: 0.4859\n",
      "timestep:14, pyg_AUC: 0.4859\n",
      "timestep:15, pyg_AUC: 0.4859\n",
      "timestep:16, pyg_AUC: 0.4887\n",
      "timestep:17, pyg_AUC: 0.4887\n",
      "timestep:18, pyg_AUC: 0.4887\n",
      "timestep:19, pyg_AUC: 0.4915\n",
      "timestep:20, pyg_AUC: 0.4887\n",
      "timestep:21, pyg_AUC: 0.4873\n",
      "timestep:22, pyg_AUC: 0.4901\n",
      "timestep:23, pyg_AUC: 0.4901\n",
      "timestep:24, pyg_AUC: 0.4887\n",
      "timestep:25, pyg_AUC: 0.4859\n",
      "timestep:26, pyg_AUC: 0.4915\n",
      "timestep:27, pyg_AUC: 0.4901\n",
      "timestep:28, pyg_AUC: 0.4845\n",
      "timestep:29, pyg_AUC: 0.4887\n",
      "timestep:30, pyg_AUC: 0.4929\n",
      "timestep:31, pyg_AUC: 0.4901\n",
      "timestep:32, pyg_AUC: 0.4873\n",
      "timestep:33, pyg_AUC: 0.4901\n",
      "timestep:34, pyg_AUC: 0.4901\n",
      "timestep:35, pyg_AUC: 0.4929\n",
      "timestep:36, pyg_AUC: 0.4887\n",
      "timestep:37, pyg_AUC: 0.4816\n",
      "timestep:38, pyg_AUC: 0.4958\n",
      "timestep:39, pyg_AUC: 0.4915\n",
      "timestep:40, pyg_AUC: 0.4944\n",
      "timestep:41, pyg_AUC: 0.4859\n",
      "timestep:42, pyg_AUC: 0.4915\n",
      "timestep:43, pyg_AUC: 0.4859\n",
      "timestep:44, pyg_AUC: 0.4831\n",
      "timestep:45, pyg_AUC: 0.4859\n",
      "timestep:46, pyg_AUC: 0.4915\n",
      "timestep:47, pyg_AUC: 0.4901\n",
      "timestep:48, pyg_AUC: 0.4901\n",
      "timestep:49, pyg_AUC: 0.4901\n",
      "timestep:50, pyg_AUC: 0.4887\n",
      "timestep:51, pyg_AUC: 0.4901\n",
      "timestep:52, pyg_AUC: 0.4887\n",
      "timestep:53, pyg_AUC: 0.4944\n",
      "timestep:54, pyg_AUC: 0.4873\n",
      "timestep:55, pyg_AUC: 0.4929\n",
      "timestep:56, pyg_AUC: 0.4929\n",
      "timestep:57, pyg_AUC: 0.4845\n",
      "timestep:58, pyg_AUC: 0.4873\n",
      "timestep:59, pyg_AUC: 0.4887\n",
      "timestep:60, pyg_AUC: 0.4873\n",
      "timestep:61, pyg_AUC: 0.4915\n",
      "timestep:62, pyg_AUC: 0.4958\n",
      "timestep:63, pyg_AUC: 0.4944\n",
      "timestep:64, pyg_AUC: 0.4915\n",
      "timestep:65, pyg_AUC: 0.4887\n",
      "timestep:66, pyg_AUC: 0.4887\n",
      "timestep:67, pyg_AUC: 0.4873\n",
      "timestep:68, pyg_AUC: 0.4859\n",
      "timestep:69, pyg_AUC: 0.4915\n",
      "timestep:70, pyg_AUC: 0.4929\n",
      "timestep:71, pyg_AUC: 0.4845\n",
      "timestep:72, pyg_AUC: 0.4944\n",
      "timestep:73, pyg_AUC: 0.4816\n",
      "timestep:74, pyg_AUC: 0.4845\n",
      "timestep:75, pyg_AUC: 0.4915\n",
      "timestep:76, pyg_AUC: 0.4915\n",
      "timestep:77, pyg_AUC: 0.4929\n",
      "timestep:78, pyg_AUC: 0.4887\n",
      "timestep:79, pyg_AUC: 0.4901\n",
      "timestep:80, pyg_AUC: 0.4887\n",
      "timestep:81, pyg_AUC: 0.4887\n",
      "timestep:82, pyg_AUC: 0.4915\n",
      "timestep:83, pyg_AUC: 0.4944\n",
      "timestep:84, pyg_AUC: 0.4915\n",
      "timestep:85, pyg_AUC: 0.4859\n",
      "timestep:86, pyg_AUC: 0.4887\n",
      "timestep:87, pyg_AUC: 0.4901\n",
      "timestep:88, pyg_AUC: 0.4873\n",
      "timestep:89, pyg_AUC: 0.4887\n",
      "timestep:90, pyg_AUC: 0.4816\n",
      "timestep:91, pyg_AUC: 0.4929\n",
      "timestep:92, pyg_AUC: 0.4873\n",
      "timestep:93, pyg_AUC: 0.4944\n",
      "timestep:94, pyg_AUC: 0.4859\n",
      "timestep:95, pyg_AUC: 0.4816\n",
      "timestep:96, pyg_AUC: 0.4831\n",
      "timestep:97, pyg_AUC: 0.4845\n",
      "timestep:98, pyg_AUC: 0.4887\n",
      "timestep:99, pyg_AUC: 0.4887\n",
      "timestep:100, pyg_AUC: 0.4901\n",
      "timestep:101, pyg_AUC: 0.4845\n",
      "timestep:102, pyg_AUC: 0.4887\n",
      "timestep:103, pyg_AUC: 0.4915\n",
      "timestep:104, pyg_AUC: 0.4845\n",
      "timestep:105, pyg_AUC: 0.4901\n",
      "timestep:106, pyg_AUC: 0.4873\n",
      "timestep:107, pyg_AUC: 0.4901\n",
      "timestep:108, pyg_AUC: 0.4859\n",
      "timestep:109, pyg_AUC: 0.4845\n",
      "timestep:110, pyg_AUC: 0.4929\n",
      "timestep:111, pyg_AUC: 0.4887\n",
      "timestep:112, pyg_AUC: 0.4915\n",
      "timestep:113, pyg_AUC: 0.4859\n",
      "timestep:114, pyg_AUC: 0.4887\n",
      "timestep:115, pyg_AUC: 0.4859\n",
      "timestep:116, pyg_AUC: 0.4901\n",
      "timestep:117, pyg_AUC: 0.4887\n",
      "timestep:118, pyg_AUC: 0.4873\n",
      "timestep:119, pyg_AUC: 0.4915\n",
      "timestep:120, pyg_AUC: 0.4873\n",
      "timestep:121, pyg_AUC: 0.4901\n",
      "timestep:122, pyg_AUC: 0.4915\n",
      "timestep:123, pyg_AUC: 0.4845\n",
      "timestep:124, pyg_AUC: 0.4887\n",
      "timestep:125, pyg_AUC: 0.4929\n",
      "timestep:126, pyg_AUC: 0.4901\n",
      "timestep:127, pyg_AUC: 0.4915\n",
      "timestep:128, pyg_AUC: 0.4958\n",
      "timestep:129, pyg_AUC: 0.4901\n",
      "timestep:130, pyg_AUC: 0.4887\n",
      "timestep:131, pyg_AUC: 0.4944\n",
      "timestep:132, pyg_AUC: 0.4887\n",
      "timestep:133, pyg_AUC: 0.4887\n",
      "timestep:134, pyg_AUC: 0.4873\n",
      "timestep:135, pyg_AUC: 0.4873\n",
      "timestep:136, pyg_AUC: 0.4929\n",
      "timestep:137, pyg_AUC: 0.4873\n",
      "timestep:138, pyg_AUC: 0.4901\n",
      "timestep:139, pyg_AUC: 0.4915\n",
      "timestep:140, pyg_AUC: 0.4915\n",
      "timestep:141, pyg_AUC: 0.4873\n",
      "timestep:142, pyg_AUC: 0.4901\n",
      "timestep:143, pyg_AUC: 0.4901\n",
      "timestep:144, pyg_AUC: 0.4831\n",
      "timestep:145, pyg_AUC: 0.4915\n",
      "timestep:146, pyg_AUC: 0.4901\n",
      "timestep:147, pyg_AUC: 0.4873\n",
      "timestep:148, pyg_AUC: 0.4901\n",
      "timestep:149, pyg_AUC: 0.4901\n",
      "timestep:150, pyg_AUC: 0.4915\n",
      "timestep:151, pyg_AUC: 0.4901\n",
      "timestep:152, pyg_AUC: 0.4929\n",
      "timestep:153, pyg_AUC: 0.4915\n",
      "timestep:154, pyg_AUC: 0.4901\n",
      "timestep:155, pyg_AUC: 0.4944\n",
      "timestep:156, pyg_AUC: 0.4915\n",
      "timestep:157, pyg_AUC: 0.4901\n",
      "timestep:158, pyg_AUC: 0.4944\n",
      "timestep:159, pyg_AUC: 0.4901\n",
      "timestep:160, pyg_AUC: 0.4944\n",
      "timestep:161, pyg_AUC: 0.4831\n",
      "timestep:162, pyg_AUC: 0.4887\n",
      "timestep:163, pyg_AUC: 0.4873\n",
      "timestep:164, pyg_AUC: 0.4873\n",
      "timestep:165, pyg_AUC: 0.4831\n",
      "timestep:166, pyg_AUC: 0.4845\n",
      "timestep:167, pyg_AUC: 0.4845\n",
      "timestep:168, pyg_AUC: 0.4873\n",
      "timestep:169, pyg_AUC: 0.4887\n",
      "timestep:170, pyg_AUC: 0.4887\n",
      "timestep:171, pyg_AUC: 0.4887\n",
      "timestep:172, pyg_AUC: 0.4901\n",
      "timestep:173, pyg_AUC: 0.4929\n",
      "timestep:174, pyg_AUC: 0.4901\n",
      "timestep:175, pyg_AUC: 0.4901\n",
      "timestep:176, pyg_AUC: 0.4915\n",
      "timestep:177, pyg_AUC: 0.4873\n",
      "timestep:178, pyg_AUC: 0.4915\n",
      "timestep:179, pyg_AUC: 0.4915\n",
      "timestep:180, pyg_AUC: 0.4873\n",
      "timestep:181, pyg_AUC: 0.4887\n",
      "timestep:182, pyg_AUC: 0.4873\n",
      "timestep:183, pyg_AUC: 0.4887\n",
      "timestep:184, pyg_AUC: 0.4887\n",
      "timestep:185, pyg_AUC: 0.4845\n",
      "timestep:186, pyg_AUC: 0.4816\n",
      "timestep:187, pyg_AUC: 0.4831\n",
      "timestep:188, pyg_AUC: 0.4873\n",
      "timestep:189, pyg_AUC: 0.4831\n",
      "timestep:190, pyg_AUC: 0.4873\n",
      "timestep:191, pyg_AUC: 0.4887\n",
      "timestep:192, pyg_AUC: 0.4901\n",
      "timestep:193, pyg_AUC: 0.4873\n",
      "timestep:194, pyg_AUC: 0.4873\n",
      "timestep:195, pyg_AUC: 0.4915\n",
      "timestep:196, pyg_AUC: 0.4887\n",
      "timestep:197, pyg_AUC: 0.4915\n",
      "timestep:198, pyg_AUC: 0.4873\n",
      "timestep:199, pyg_AUC: 0.4873\n",
      "timestep:200, pyg_AUC: 0.4901\n",
      "timestep:201, pyg_AUC: 0.4873\n",
      "timestep:202, pyg_AUC: 0.4887\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4901\n",
      "timestep:205, pyg_AUC: 0.4845\n",
      "timestep:206, pyg_AUC: 0.4901\n",
      "timestep:207, pyg_AUC: 0.4901\n",
      "timestep:208, pyg_AUC: 0.4802\n",
      "timestep:209, pyg_AUC: 0.4901\n",
      "timestep:210, pyg_AUC: 0.4887\n",
      "timestep:211, pyg_AUC: 0.4901\n",
      "timestep:212, pyg_AUC: 0.4873\n",
      "timestep:213, pyg_AUC: 0.4873\n",
      "timestep:214, pyg_AUC: 0.4845\n",
      "timestep:215, pyg_AUC: 0.4901\n",
      "timestep:216, pyg_AUC: 0.4831\n",
      "timestep:217, pyg_AUC: 0.4873\n",
      "timestep:218, pyg_AUC: 0.4915\n",
      "timestep:219, pyg_AUC: 0.4887\n",
      "timestep:220, pyg_AUC: 0.4901\n",
      "timestep:221, pyg_AUC: 0.4831\n",
      "timestep:222, pyg_AUC: 0.4873\n",
      "timestep:223, pyg_AUC: 0.4873\n",
      "timestep:224, pyg_AUC: 0.4845\n",
      "timestep:225, pyg_AUC: 0.4802\n",
      "timestep:226, pyg_AUC: 0.4831\n",
      "timestep:227, pyg_AUC: 0.4901\n",
      "timestep:228, pyg_AUC: 0.4901\n",
      "timestep:229, pyg_AUC: 0.4873\n",
      "timestep:230, pyg_AUC: 0.4859\n",
      "timestep:231, pyg_AUC: 0.4887\n",
      "timestep:232, pyg_AUC: 0.4915\n",
      "timestep:233, pyg_AUC: 0.4887\n",
      "timestep:234, pyg_AUC: 0.4816\n",
      "timestep:235, pyg_AUC: 0.4901\n",
      "timestep:236, pyg_AUC: 0.4859\n",
      "timestep:237, pyg_AUC: 0.4873\n",
      "timestep:238, pyg_AUC: 0.4873\n",
      "timestep:239, pyg_AUC: 0.4915\n",
      "timestep:240, pyg_AUC: 0.4831\n",
      "timestep:241, pyg_AUC: 0.4915\n",
      "timestep:242, pyg_AUC: 0.4816\n",
      "timestep:243, pyg_AUC: 0.4915\n",
      "timestep:244, pyg_AUC: 0.4845\n",
      "timestep:245, pyg_AUC: 0.4873\n",
      "timestep:246, pyg_AUC: 0.4859\n",
      "timestep:247, pyg_AUC: 0.4873\n",
      "timestep:248, pyg_AUC: 0.4845\n",
      "timestep:249, pyg_AUC: 0.4859\n",
      "timestep:250, pyg_AUC: 0.4859\n",
      "timestep:251, pyg_AUC: 0.4873\n",
      "timestep:252, pyg_AUC: 0.4859\n",
      "timestep:253, pyg_AUC: 0.4873\n",
      "timestep:254, pyg_AUC: 0.4873\n",
      "timestep:255, pyg_AUC: 0.4901\n",
      "timestep:256, pyg_AUC: 0.4873\n",
      "timestep:257, pyg_AUC: 0.4859\n",
      "timestep:258, pyg_AUC: 0.4873\n",
      "timestep:259, pyg_AUC: 0.4873\n",
      "timestep:260, pyg_AUC: 0.4873\n",
      "timestep:261, pyg_AUC: 0.4887\n",
      "timestep:262, pyg_AUC: 0.4873\n",
      "timestep:263, pyg_AUC: 0.4816\n",
      "timestep:264, pyg_AUC: 0.4887\n",
      "timestep:265, pyg_AUC: 0.4859\n",
      "timestep:266, pyg_AUC: 0.4831\n",
      "timestep:267, pyg_AUC: 0.4845\n",
      "timestep:268, pyg_AUC: 0.4887\n",
      "timestep:269, pyg_AUC: 0.4859\n",
      "timestep:270, pyg_AUC: 0.4887\n",
      "timestep:271, pyg_AUC: 0.4901\n",
      "timestep:272, pyg_AUC: 0.4859\n",
      "timestep:273, pyg_AUC: 0.4859\n",
      "timestep:274, pyg_AUC: 0.4845\n",
      "timestep:275, pyg_AUC: 0.4887\n",
      "timestep:276, pyg_AUC: 0.4873\n",
      "timestep:277, pyg_AUC: 0.4873\n",
      "timestep:278, pyg_AUC: 0.4859\n",
      "timestep:279, pyg_AUC: 0.4845\n",
      "timestep:280, pyg_AUC: 0.4859\n",
      "timestep:281, pyg_AUC: 0.4831\n",
      "timestep:282, pyg_AUC: 0.4859\n",
      "timestep:283, pyg_AUC: 0.4859\n",
      "timestep:284, pyg_AUC: 0.4887\n",
      "timestep:285, pyg_AUC: 0.4802\n",
      "timestep:286, pyg_AUC: 0.4901\n",
      "timestep:287, pyg_AUC: 0.4887\n",
      "timestep:288, pyg_AUC: 0.4873\n",
      "timestep:289, pyg_AUC: 0.4887\n",
      "timestep:290, pyg_AUC: 0.4831\n",
      "timestep:291, pyg_AUC: 0.4802\n",
      "timestep:292, pyg_AUC: 0.4887\n",
      "timestep:293, pyg_AUC: 0.4873\n",
      "timestep:294, pyg_AUC: 0.4901\n",
      "timestep:295, pyg_AUC: 0.4845\n",
      "timestep:296, pyg_AUC: 0.4816\n",
      "timestep:297, pyg_AUC: 0.4887\n",
      "timestep:298, pyg_AUC: 0.4859\n",
      "timestep:299, pyg_AUC: 0.4845\n",
      "timestep:300, pyg_AUC: 0.4873\n",
      "timestep:301, pyg_AUC: 0.4845\n",
      "timestep:302, pyg_AUC: 0.4901\n",
      "timestep:303, pyg_AUC: 0.4887\n",
      "timestep:304, pyg_AUC: 0.4845\n",
      "timestep:305, pyg_AUC: 0.4873\n",
      "timestep:306, pyg_AUC: 0.4831\n",
      "timestep:307, pyg_AUC: 0.4845\n",
      "timestep:308, pyg_AUC: 0.4887\n",
      "timestep:309, pyg_AUC: 0.4831\n",
      "timestep:310, pyg_AUC: 0.4816\n",
      "timestep:311, pyg_AUC: 0.4873\n",
      "timestep:312, pyg_AUC: 0.4873\n",
      "timestep:313, pyg_AUC: 0.4845\n",
      "timestep:314, pyg_AUC: 0.4845\n",
      "timestep:315, pyg_AUC: 0.4831\n",
      "timestep:316, pyg_AUC: 0.4831\n",
      "timestep:317, pyg_AUC: 0.4845\n",
      "timestep:318, pyg_AUC: 0.4873\n",
      "timestep:319, pyg_AUC: 0.4873\n",
      "timestep:320, pyg_AUC: 0.4859\n",
      "timestep:321, pyg_AUC: 0.4859\n",
      "timestep:322, pyg_AUC: 0.4859\n",
      "timestep:323, pyg_AUC: 0.4845\n",
      "timestep:324, pyg_AUC: 0.4873\n",
      "timestep:325, pyg_AUC: 0.4873\n",
      "timestep:326, pyg_AUC: 0.4859\n",
      "timestep:327, pyg_AUC: 0.4845\n",
      "timestep:328, pyg_AUC: 0.4845\n",
      "timestep:329, pyg_AUC: 0.4873\n",
      "timestep:330, pyg_AUC: 0.4845\n",
      "timestep:331, pyg_AUC: 0.4816\n",
      "timestep:332, pyg_AUC: 0.4831\n",
      "timestep:333, pyg_AUC: 0.4845\n",
      "timestep:334, pyg_AUC: 0.4873\n",
      "timestep:335, pyg_AUC: 0.4859\n",
      "timestep:336, pyg_AUC: 0.4859\n",
      "timestep:337, pyg_AUC: 0.4901\n",
      "timestep:338, pyg_AUC: 0.4887\n",
      "timestep:339, pyg_AUC: 0.4873\n",
      "timestep:340, pyg_AUC: 0.4845\n",
      "timestep:341, pyg_AUC: 0.4859\n",
      "timestep:342, pyg_AUC: 0.4845\n",
      "timestep:343, pyg_AUC: 0.4887\n",
      "timestep:344, pyg_AUC: 0.4859\n",
      "timestep:345, pyg_AUC: 0.4845\n",
      "timestep:346, pyg_AUC: 0.4901\n",
      "timestep:347, pyg_AUC: 0.4859\n",
      "timestep:348, pyg_AUC: 0.4859\n",
      "timestep:349, pyg_AUC: 0.4873\n",
      "timestep:350, pyg_AUC: 0.4873\n",
      "timestep:351, pyg_AUC: 0.4845\n",
      "timestep:352, pyg_AUC: 0.4873\n",
      "timestep:353, pyg_AUC: 0.4887\n",
      "timestep:354, pyg_AUC: 0.4859\n",
      "timestep:355, pyg_AUC: 0.4859\n",
      "timestep:356, pyg_AUC: 0.4873\n",
      "timestep:357, pyg_AUC: 0.4845\n",
      "timestep:358, pyg_AUC: 0.4859\n",
      "timestep:359, pyg_AUC: 0.4901\n",
      "timestep:360, pyg_AUC: 0.4887\n",
      "timestep:361, pyg_AUC: 0.4845\n",
      "timestep:362, pyg_AUC: 0.4887\n",
      "timestep:363, pyg_AUC: 0.4887\n",
      "timestep:364, pyg_AUC: 0.4887\n",
      "timestep:365, pyg_AUC: 0.4901\n",
      "timestep:366, pyg_AUC: 0.4859\n",
      "timestep:367, pyg_AUC: 0.4887\n",
      "timestep:368, pyg_AUC: 0.4887\n",
      "timestep:369, pyg_AUC: 0.4873\n",
      "timestep:370, pyg_AUC: 0.4873\n",
      "timestep:371, pyg_AUC: 0.4887\n",
      "timestep:372, pyg_AUC: 0.4873\n",
      "timestep:373, pyg_AUC: 0.4831\n",
      "timestep:374, pyg_AUC: 0.4901\n",
      "timestep:375, pyg_AUC: 0.4887\n",
      "timestep:376, pyg_AUC: 0.4845\n",
      "timestep:377, pyg_AUC: 0.4831\n",
      "timestep:378, pyg_AUC: 0.4859\n",
      "timestep:379, pyg_AUC: 0.4901\n",
      "timestep:380, pyg_AUC: 0.4873\n",
      "timestep:381, pyg_AUC: 0.4845\n",
      "timestep:382, pyg_AUC: 0.4873\n",
      "timestep:383, pyg_AUC: 0.4873\n",
      "timestep:384, pyg_AUC: 0.4929\n",
      "timestep:385, pyg_AUC: 0.4859\n",
      "timestep:386, pyg_AUC: 0.4845\n",
      "timestep:387, pyg_AUC: 0.4887\n",
      "timestep:388, pyg_AUC: 0.4859\n",
      "timestep:389, pyg_AUC: 0.4887\n",
      "timestep:390, pyg_AUC: 0.4873\n",
      "timestep:391, pyg_AUC: 0.4845\n",
      "timestep:392, pyg_AUC: 0.4859\n",
      "timestep:393, pyg_AUC: 0.4887\n",
      "timestep:394, pyg_AUC: 0.4859\n",
      "timestep:395, pyg_AUC: 0.4859\n",
      "timestep:396, pyg_AUC: 0.4845\n",
      "timestep:397, pyg_AUC: 0.4915\n",
      "timestep:398, pyg_AUC: 0.4915\n",
      "timestep:399, pyg_AUC: 0.4915\n",
      "timestep:400, pyg_AUC: 0.4845\n",
      "timestep:401, pyg_AUC: 0.4859\n",
      "timestep:402, pyg_AUC: 0.4887\n",
      "timestep:403, pyg_AUC: 0.4915\n",
      "timestep:404, pyg_AUC: 0.4845\n",
      "timestep:405, pyg_AUC: 0.4859\n",
      "timestep:406, pyg_AUC: 0.4901\n",
      "timestep:407, pyg_AUC: 0.4873\n",
      "timestep:408, pyg_AUC: 0.4873\n",
      "timestep:409, pyg_AUC: 0.4887\n",
      "timestep:410, pyg_AUC: 0.4873\n",
      "timestep:411, pyg_AUC: 0.4887\n",
      "timestep:412, pyg_AUC: 0.4901\n",
      "timestep:413, pyg_AUC: 0.4873\n",
      "timestep:414, pyg_AUC: 0.4859\n",
      "timestep:415, pyg_AUC: 0.4887\n",
      "timestep:416, pyg_AUC: 0.4859\n",
      "timestep:417, pyg_AUC: 0.4887\n",
      "timestep:418, pyg_AUC: 0.4887\n",
      "timestep:419, pyg_AUC: 0.4901\n",
      "timestep:420, pyg_AUC: 0.4887\n",
      "timestep:421, pyg_AUC: 0.4901\n",
      "timestep:422, pyg_AUC: 0.4887\n",
      "timestep:423, pyg_AUC: 0.4845\n",
      "timestep:424, pyg_AUC: 0.4859\n",
      "timestep:425, pyg_AUC: 0.4845\n",
      "timestep:426, pyg_AUC: 0.4859\n",
      "timestep:427, pyg_AUC: 0.4901\n",
      "timestep:428, pyg_AUC: 0.4873\n",
      "timestep:429, pyg_AUC: 0.4901\n",
      "timestep:430, pyg_AUC: 0.4873\n",
      "timestep:431, pyg_AUC: 0.4845\n",
      "timestep:432, pyg_AUC: 0.4816\n",
      "timestep:433, pyg_AUC: 0.4901\n",
      "timestep:434, pyg_AUC: 0.4901\n",
      "timestep:435, pyg_AUC: 0.4859\n",
      "timestep:436, pyg_AUC: 0.4901\n",
      "timestep:437, pyg_AUC: 0.4887\n",
      "timestep:438, pyg_AUC: 0.4887\n",
      "timestep:439, pyg_AUC: 0.4873\n",
      "timestep:440, pyg_AUC: 0.4901\n",
      "timestep:441, pyg_AUC: 0.4873\n",
      "timestep:442, pyg_AUC: 0.4845\n",
      "timestep:443, pyg_AUC: 0.4831\n",
      "timestep:444, pyg_AUC: 0.4816\n",
      "timestep:445, pyg_AUC: 0.4831\n",
      "timestep:446, pyg_AUC: 0.4873\n",
      "timestep:447, pyg_AUC: 0.4887\n",
      "timestep:448, pyg_AUC: 0.4831\n",
      "timestep:449, pyg_AUC: 0.4915\n",
      "timestep:450, pyg_AUC: 0.4859\n",
      "timestep:451, pyg_AUC: 0.4915\n",
      "timestep:452, pyg_AUC: 0.4859\n",
      "timestep:453, pyg_AUC: 0.4901\n",
      "timestep:454, pyg_AUC: 0.4915\n",
      "timestep:455, pyg_AUC: 0.4859\n",
      "timestep:456, pyg_AUC: 0.4873\n",
      "timestep:457, pyg_AUC: 0.4901\n",
      "timestep:458, pyg_AUC: 0.4915\n",
      "timestep:459, pyg_AUC: 0.4859\n",
      "timestep:460, pyg_AUC: 0.4901\n",
      "timestep:461, pyg_AUC: 0.4873\n",
      "timestep:462, pyg_AUC: 0.4887\n",
      "timestep:463, pyg_AUC: 0.4901\n",
      "timestep:464, pyg_AUC: 0.4859\n",
      "timestep:465, pyg_AUC: 0.4887\n",
      "timestep:466, pyg_AUC: 0.4887\n",
      "timestep:467, pyg_AUC: 0.4859\n",
      "timestep:468, pyg_AUC: 0.4887\n",
      "timestep:469, pyg_AUC: 0.4859\n",
      "timestep:470, pyg_AUC: 0.4873\n",
      "timestep:471, pyg_AUC: 0.4873\n",
      "timestep:472, pyg_AUC: 0.4915\n",
      "timestep:473, pyg_AUC: 0.4929\n",
      "timestep:474, pyg_AUC: 0.4873\n",
      "timestep:475, pyg_AUC: 0.4887\n",
      "timestep:476, pyg_AUC: 0.4859\n",
      "timestep:477, pyg_AUC: 0.4859\n",
      "timestep:478, pyg_AUC: 0.4887\n",
      "timestep:479, pyg_AUC: 0.4859\n",
      "timestep:480, pyg_AUC: 0.4887\n",
      "timestep:481, pyg_AUC: 0.4845\n",
      "timestep:482, pyg_AUC: 0.4859\n",
      "timestep:483, pyg_AUC: 0.4887\n",
      "timestep:484, pyg_AUC: 0.4901\n",
      "timestep:485, pyg_AUC: 0.4887\n",
      "timestep:486, pyg_AUC: 0.4873\n",
      "timestep:487, pyg_AUC: 0.4887\n",
      "timestep:488, pyg_AUC: 0.4901\n",
      "timestep:489, pyg_AUC: 0.4901\n",
      "timestep:490, pyg_AUC: 0.4887\n",
      "timestep:491, pyg_AUC: 0.4901\n",
      "timestep:492, pyg_AUC: 0.4831\n",
      "timestep:493, pyg_AUC: 0.4873\n",
      "timestep:494, pyg_AUC: 0.4929\n",
      "timestep:495, pyg_AUC: 0.4845\n",
      "timestep:496, pyg_AUC: 0.4887\n",
      "timestep:497, pyg_AUC: 0.4915\n",
      "timestep:498, pyg_AUC: 0.4915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [10:02<23:28, 100.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:499, pyg_AUC: 0.4901\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 39.36583\n",
      "Epoch: 0010 loss= 24.33446\n",
      "Epoch: 0020 loss= 23.34266\n",
      "Epoch: 0030 loss= 16.61592\n",
      "Epoch: 0040 loss= 13.98286\n",
      "Epoch: 0050 loss= 8.16397\n",
      "Epoch: 0060 loss= 1.66162\n",
      "Epoch: 0070 loss= 0.66561\n",
      "Epoch: 0080 loss= 0.70240\n",
      "Epoch: 0090 loss= 0.77302\n",
      "Epoch: 0100 loss= 0.78796\n",
      "Epoch: 0110 loss= 0.68809\n",
      "Epoch: 0120 loss= 0.57944\n",
      "Epoch: 0130 loss= 0.60001\n",
      "Epoch: 0140 loss= 0.60059\n",
      "Epoch: 0150 loss= 0.63417\n",
      "Epoch: 0160 loss= 0.55416\n",
      "Epoch: 0170 loss= 0.66118\n",
      "Epoch: 0180 loss= 0.62555\n",
      "Epoch: 0190 loss= 0.60469\n",
      "Epoch: 0200 loss= 0.68234\n",
      "Epoch: 0210 loss= 0.65110\n",
      "Epoch: 0220 loss= 0.58275\n",
      "Epoch: 0230 loss= 0.60707\n",
      "Epoch: 0240 loss= 0.55465\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common feature: tensor([[-4.8299,  4.7370, -5.3792, -4.6727, -4.4175,  5.6911,  5.5365, -5.3083]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 39.87531\n",
      "Epoch: 0010 loss= 19.38387\n",
      "Epoch: 0020 loss= 13.05482\n",
      "Epoch: 0030 loss= 8.89157\n",
      "Epoch: 0040 loss= 1.90042\n",
      "Epoch: 0050 loss= 1.07729\n",
      "Epoch: 0060 loss= 1.18685\n",
      "Epoch: 0070 loss= 0.72021\n",
      "Epoch: 0080 loss= 0.65817\n",
      "Epoch: 0090 loss= 0.65867\n",
      "Epoch: 0100 loss= 0.51590\n",
      "Epoch: 0110 loss= 0.54971\n",
      "Epoch: 0120 loss= 0.71047\n",
      "Epoch: 0130 loss= 0.62737\n",
      "Epoch: 0140 loss= 0.57290\n",
      "Epoch: 0150 loss= 0.68967\n",
      "Epoch: 0160 loss= 0.56217\n",
      "Epoch: 0170 loss= 0.56579\n",
      "Epoch: 0180 loss= 0.70108\n",
      "Epoch: 0190 loss= 0.55479\n",
      "Epoch: 0200 loss= 0.64591\n",
      "Epoch: 0210 loss= 0.63127\n",
      "Epoch: 0220 loss= 0.53420\n",
      "Epoch: 0230 loss= 0.60165\n",
      "Epoch: 0240 loss= 0.63206\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4944\n",
      "timestep:1, pyg_AUC: 0.4859\n",
      "timestep:2, pyg_AUC: 0.4929\n",
      "timestep:3, pyg_AUC: 0.4929\n",
      "timestep:4, pyg_AUC: 0.4845\n",
      "timestep:5, pyg_AUC: 0.4831\n",
      "timestep:6, pyg_AUC: 0.4859\n",
      "timestep:7, pyg_AUC: 0.4887\n",
      "timestep:8, pyg_AUC: 0.4831\n",
      "timestep:9, pyg_AUC: 0.4944\n",
      "timestep:10, pyg_AUC: 0.4929\n",
      "timestep:11, pyg_AUC: 0.4901\n",
      "timestep:12, pyg_AUC: 0.4958\n",
      "timestep:13, pyg_AUC: 0.4929\n",
      "timestep:14, pyg_AUC: 0.4915\n",
      "timestep:15, pyg_AUC: 0.4901\n",
      "timestep:16, pyg_AUC: 0.4944\n",
      "timestep:17, pyg_AUC: 0.4873\n",
      "timestep:18, pyg_AUC: 0.4845\n",
      "timestep:19, pyg_AUC: 0.4859\n",
      "timestep:20, pyg_AUC: 0.4915\n",
      "timestep:21, pyg_AUC: 0.4901\n",
      "timestep:22, pyg_AUC: 0.4915\n",
      "timestep:23, pyg_AUC: 0.4944\n",
      "timestep:24, pyg_AUC: 0.4845\n",
      "timestep:25, pyg_AUC: 0.4887\n",
      "timestep:26, pyg_AUC: 0.4958\n",
      "timestep:27, pyg_AUC: 0.4873\n",
      "timestep:28, pyg_AUC: 0.4901\n",
      "timestep:29, pyg_AUC: 0.4929\n",
      "timestep:30, pyg_AUC: 0.4859\n",
      "timestep:31, pyg_AUC: 0.4929\n",
      "timestep:32, pyg_AUC: 0.4845\n",
      "timestep:33, pyg_AUC: 0.4901\n",
      "timestep:34, pyg_AUC: 0.4944\n",
      "timestep:35, pyg_AUC: 0.4887\n",
      "timestep:36, pyg_AUC: 0.4802\n",
      "timestep:37, pyg_AUC: 0.4901\n",
      "timestep:38, pyg_AUC: 0.4944\n",
      "timestep:39, pyg_AUC: 0.4901\n",
      "timestep:40, pyg_AUC: 0.4915\n",
      "timestep:41, pyg_AUC: 0.4901\n",
      "timestep:42, pyg_AUC: 0.4859\n",
      "timestep:43, pyg_AUC: 0.4859\n",
      "timestep:44, pyg_AUC: 0.4859\n",
      "timestep:45, pyg_AUC: 0.4901\n",
      "timestep:46, pyg_AUC: 0.4887\n",
      "timestep:47, pyg_AUC: 0.4887\n",
      "timestep:48, pyg_AUC: 0.4929\n",
      "timestep:49, pyg_AUC: 0.4929\n",
      "timestep:50, pyg_AUC: 0.4887\n",
      "timestep:51, pyg_AUC: 0.4887\n",
      "timestep:52, pyg_AUC: 0.4915\n",
      "timestep:53, pyg_AUC: 0.4929\n",
      "timestep:54, pyg_AUC: 0.4915\n",
      "timestep:55, pyg_AUC: 0.4873\n",
      "timestep:56, pyg_AUC: 0.4859\n",
      "timestep:57, pyg_AUC: 0.4873\n",
      "timestep:58, pyg_AUC: 0.4887\n",
      "timestep:59, pyg_AUC: 0.4845\n",
      "timestep:60, pyg_AUC: 0.4845\n",
      "timestep:61, pyg_AUC: 0.4831\n",
      "timestep:62, pyg_AUC: 0.4972\n",
      "timestep:63, pyg_AUC: 0.4859\n",
      "timestep:64, pyg_AUC: 0.4816\n",
      "timestep:65, pyg_AUC: 0.4944\n",
      "timestep:66, pyg_AUC: 0.4901\n",
      "timestep:67, pyg_AUC: 0.4816\n",
      "timestep:68, pyg_AUC: 0.4929\n",
      "timestep:69, pyg_AUC: 0.4887\n",
      "timestep:70, pyg_AUC: 0.4915\n",
      "timestep:71, pyg_AUC: 0.4859\n",
      "timestep:72, pyg_AUC: 0.4859\n",
      "timestep:73, pyg_AUC: 0.4873\n",
      "timestep:74, pyg_AUC: 0.4929\n",
      "timestep:75, pyg_AUC: 0.4873\n",
      "timestep:76, pyg_AUC: 0.4845\n",
      "timestep:77, pyg_AUC: 0.4859\n",
      "timestep:78, pyg_AUC: 0.4887\n",
      "timestep:79, pyg_AUC: 0.4944\n",
      "timestep:80, pyg_AUC: 0.4887\n",
      "timestep:81, pyg_AUC: 0.4901\n",
      "timestep:82, pyg_AUC: 0.4944\n",
      "timestep:83, pyg_AUC: 0.4901\n",
      "timestep:84, pyg_AUC: 0.4873\n",
      "timestep:85, pyg_AUC: 0.4887\n",
      "timestep:86, pyg_AUC: 0.4845\n",
      "timestep:87, pyg_AUC: 0.4929\n",
      "timestep:88, pyg_AUC: 0.4887\n",
      "timestep:89, pyg_AUC: 0.4901\n",
      "timestep:90, pyg_AUC: 0.4887\n",
      "timestep:91, pyg_AUC: 0.4845\n",
      "timestep:92, pyg_AUC: 0.4845\n",
      "timestep:93, pyg_AUC: 0.4915\n",
      "timestep:94, pyg_AUC: 0.4901\n",
      "timestep:95, pyg_AUC: 0.4859\n",
      "timestep:96, pyg_AUC: 0.4915\n",
      "timestep:97, pyg_AUC: 0.4887\n",
      "timestep:98, pyg_AUC: 0.4887\n",
      "timestep:99, pyg_AUC: 0.4873\n",
      "timestep:100, pyg_AUC: 0.4901\n",
      "timestep:101, pyg_AUC: 0.4859\n",
      "timestep:102, pyg_AUC: 0.4873\n",
      "timestep:103, pyg_AUC: 0.4958\n",
      "timestep:104, pyg_AUC: 0.4901\n",
      "timestep:105, pyg_AUC: 0.4831\n",
      "timestep:106, pyg_AUC: 0.4887\n",
      "timestep:107, pyg_AUC: 0.4915\n",
      "timestep:108, pyg_AUC: 0.4944\n",
      "timestep:109, pyg_AUC: 0.4929\n",
      "timestep:110, pyg_AUC: 0.4915\n",
      "timestep:111, pyg_AUC: 0.4901\n",
      "timestep:112, pyg_AUC: 0.4831\n",
      "timestep:113, pyg_AUC: 0.4845\n",
      "timestep:114, pyg_AUC: 0.4859\n",
      "timestep:115, pyg_AUC: 0.4859\n",
      "timestep:116, pyg_AUC: 0.4831\n",
      "timestep:117, pyg_AUC: 0.4859\n",
      "timestep:118, pyg_AUC: 0.4873\n",
      "timestep:119, pyg_AUC: 0.4901\n",
      "timestep:120, pyg_AUC: 0.4887\n",
      "timestep:121, pyg_AUC: 0.4873\n",
      "timestep:122, pyg_AUC: 0.4859\n",
      "timestep:123, pyg_AUC: 0.4887\n",
      "timestep:124, pyg_AUC: 0.4873\n",
      "timestep:125, pyg_AUC: 0.4873\n",
      "timestep:126, pyg_AUC: 0.4873\n",
      "timestep:127, pyg_AUC: 0.4929\n",
      "timestep:128, pyg_AUC: 0.4915\n",
      "timestep:129, pyg_AUC: 0.4887\n",
      "timestep:130, pyg_AUC: 0.4873\n",
      "timestep:131, pyg_AUC: 0.4816\n",
      "timestep:132, pyg_AUC: 0.4887\n",
      "timestep:133, pyg_AUC: 0.4915\n",
      "timestep:134, pyg_AUC: 0.4873\n",
      "timestep:135, pyg_AUC: 0.4929\n",
      "timestep:136, pyg_AUC: 0.4873\n",
      "timestep:137, pyg_AUC: 0.4887\n",
      "timestep:138, pyg_AUC: 0.4901\n",
      "timestep:139, pyg_AUC: 0.4887\n",
      "timestep:140, pyg_AUC: 0.4831\n",
      "timestep:141, pyg_AUC: 0.4901\n",
      "timestep:142, pyg_AUC: 0.4845\n",
      "timestep:143, pyg_AUC: 0.4802\n",
      "timestep:144, pyg_AUC: 0.4929\n",
      "timestep:145, pyg_AUC: 0.4887\n",
      "timestep:146, pyg_AUC: 0.4859\n",
      "timestep:147, pyg_AUC: 0.4887\n",
      "timestep:148, pyg_AUC: 0.4859\n",
      "timestep:149, pyg_AUC: 0.4929\n",
      "timestep:150, pyg_AUC: 0.4873\n",
      "timestep:151, pyg_AUC: 0.4901\n",
      "timestep:152, pyg_AUC: 0.4873\n",
      "timestep:153, pyg_AUC: 0.4859\n",
      "timestep:154, pyg_AUC: 0.4873\n",
      "timestep:155, pyg_AUC: 0.4845\n",
      "timestep:156, pyg_AUC: 0.4887\n",
      "timestep:157, pyg_AUC: 0.4901\n",
      "timestep:158, pyg_AUC: 0.4901\n",
      "timestep:159, pyg_AUC: 0.4901\n",
      "timestep:160, pyg_AUC: 0.4901\n",
      "timestep:161, pyg_AUC: 0.4845\n",
      "timestep:162, pyg_AUC: 0.4873\n",
      "timestep:163, pyg_AUC: 0.4901\n",
      "timestep:164, pyg_AUC: 0.4887\n",
      "timestep:165, pyg_AUC: 0.4887\n",
      "timestep:166, pyg_AUC: 0.4887\n",
      "timestep:167, pyg_AUC: 0.4816\n",
      "timestep:168, pyg_AUC: 0.4873\n",
      "timestep:169, pyg_AUC: 0.4887\n",
      "timestep:170, pyg_AUC: 0.4873\n",
      "timestep:171, pyg_AUC: 0.4859\n",
      "timestep:172, pyg_AUC: 0.4915\n",
      "timestep:173, pyg_AUC: 0.4901\n",
      "timestep:174, pyg_AUC: 0.4887\n",
      "timestep:175, pyg_AUC: 0.4944\n",
      "timestep:176, pyg_AUC: 0.4859\n",
      "timestep:177, pyg_AUC: 0.4915\n",
      "timestep:178, pyg_AUC: 0.4915\n",
      "timestep:179, pyg_AUC: 0.4915\n",
      "timestep:180, pyg_AUC: 0.4859\n",
      "timestep:181, pyg_AUC: 0.4901\n",
      "timestep:182, pyg_AUC: 0.4887\n",
      "timestep:183, pyg_AUC: 0.4887\n",
      "timestep:184, pyg_AUC: 0.4887\n",
      "timestep:185, pyg_AUC: 0.4816\n",
      "timestep:186, pyg_AUC: 0.4915\n",
      "timestep:187, pyg_AUC: 0.4915\n",
      "timestep:188, pyg_AUC: 0.4859\n",
      "timestep:189, pyg_AUC: 0.4873\n",
      "timestep:190, pyg_AUC: 0.4901\n",
      "timestep:191, pyg_AUC: 0.4845\n",
      "timestep:192, pyg_AUC: 0.4873\n",
      "timestep:193, pyg_AUC: 0.4915\n",
      "timestep:194, pyg_AUC: 0.4901\n",
      "timestep:195, pyg_AUC: 0.4901\n",
      "timestep:196, pyg_AUC: 0.4901\n",
      "timestep:197, pyg_AUC: 0.4915\n",
      "timestep:198, pyg_AUC: 0.4887\n",
      "timestep:199, pyg_AUC: 0.4887\n",
      "timestep:200, pyg_AUC: 0.4873\n",
      "timestep:201, pyg_AUC: 0.4901\n",
      "timestep:202, pyg_AUC: 0.4859\n",
      "timestep:203, pyg_AUC: 0.4873\n",
      "timestep:204, pyg_AUC: 0.4845\n",
      "timestep:205, pyg_AUC: 0.4802\n",
      "timestep:206, pyg_AUC: 0.4873\n",
      "timestep:207, pyg_AUC: 0.4929\n",
      "timestep:208, pyg_AUC: 0.4887\n",
      "timestep:209, pyg_AUC: 0.4845\n",
      "timestep:210, pyg_AUC: 0.4845\n",
      "timestep:211, pyg_AUC: 0.4901\n",
      "timestep:212, pyg_AUC: 0.4887\n",
      "timestep:213, pyg_AUC: 0.4901\n",
      "timestep:214, pyg_AUC: 0.4901\n",
      "timestep:215, pyg_AUC: 0.4845\n",
      "timestep:216, pyg_AUC: 0.4887\n",
      "timestep:217, pyg_AUC: 0.4887\n",
      "timestep:218, pyg_AUC: 0.4887\n",
      "timestep:219, pyg_AUC: 0.4859\n",
      "timestep:220, pyg_AUC: 0.4845\n",
      "timestep:221, pyg_AUC: 0.4915\n",
      "timestep:222, pyg_AUC: 0.4901\n",
      "timestep:223, pyg_AUC: 0.4887\n",
      "timestep:224, pyg_AUC: 0.4915\n",
      "timestep:225, pyg_AUC: 0.4831\n",
      "timestep:226, pyg_AUC: 0.4831\n",
      "timestep:227, pyg_AUC: 0.4887\n",
      "timestep:228, pyg_AUC: 0.4915\n",
      "timestep:229, pyg_AUC: 0.4859\n",
      "timestep:230, pyg_AUC: 0.4816\n",
      "timestep:231, pyg_AUC: 0.4901\n",
      "timestep:232, pyg_AUC: 0.4929\n",
      "timestep:233, pyg_AUC: 0.4831\n",
      "timestep:234, pyg_AUC: 0.4859\n",
      "timestep:235, pyg_AUC: 0.4859\n",
      "timestep:236, pyg_AUC: 0.4859\n",
      "timestep:237, pyg_AUC: 0.4887\n",
      "timestep:238, pyg_AUC: 0.4845\n",
      "timestep:239, pyg_AUC: 0.4831\n",
      "timestep:240, pyg_AUC: 0.4845\n",
      "timestep:241, pyg_AUC: 0.4845\n",
      "timestep:242, pyg_AUC: 0.4915\n",
      "timestep:243, pyg_AUC: 0.4845\n",
      "timestep:244, pyg_AUC: 0.4901\n",
      "timestep:245, pyg_AUC: 0.4901\n",
      "timestep:246, pyg_AUC: 0.4859\n",
      "timestep:247, pyg_AUC: 0.4845\n",
      "timestep:248, pyg_AUC: 0.4887\n",
      "timestep:249, pyg_AUC: 0.4887\n",
      "timestep:250, pyg_AUC: 0.4845\n",
      "timestep:251, pyg_AUC: 0.4859\n",
      "timestep:252, pyg_AUC: 0.4887\n",
      "timestep:253, pyg_AUC: 0.4887\n",
      "timestep:254, pyg_AUC: 0.4816\n",
      "timestep:255, pyg_AUC: 0.4859\n",
      "timestep:256, pyg_AUC: 0.4887\n",
      "timestep:257, pyg_AUC: 0.4873\n",
      "timestep:258, pyg_AUC: 0.4873\n",
      "timestep:259, pyg_AUC: 0.4873\n",
      "timestep:260, pyg_AUC: 0.4901\n",
      "timestep:261, pyg_AUC: 0.4887\n",
      "timestep:262, pyg_AUC: 0.4859\n",
      "timestep:263, pyg_AUC: 0.4887\n",
      "timestep:264, pyg_AUC: 0.4831\n",
      "timestep:265, pyg_AUC: 0.4901\n",
      "timestep:266, pyg_AUC: 0.4887\n",
      "timestep:267, pyg_AUC: 0.4859\n",
      "timestep:268, pyg_AUC: 0.4873\n",
      "timestep:269, pyg_AUC: 0.4887\n",
      "timestep:270, pyg_AUC: 0.4845\n",
      "timestep:271, pyg_AUC: 0.4845\n",
      "timestep:272, pyg_AUC: 0.4915\n",
      "timestep:273, pyg_AUC: 0.4901\n",
      "timestep:274, pyg_AUC: 0.4887\n",
      "timestep:275, pyg_AUC: 0.4887\n",
      "timestep:276, pyg_AUC: 0.4915\n",
      "timestep:277, pyg_AUC: 0.4929\n",
      "timestep:278, pyg_AUC: 0.4859\n",
      "timestep:279, pyg_AUC: 0.4859\n",
      "timestep:280, pyg_AUC: 0.4831\n",
      "timestep:281, pyg_AUC: 0.4845\n",
      "timestep:282, pyg_AUC: 0.4873\n",
      "timestep:283, pyg_AUC: 0.4816\n",
      "timestep:284, pyg_AUC: 0.4915\n",
      "timestep:285, pyg_AUC: 0.4887\n",
      "timestep:286, pyg_AUC: 0.4831\n",
      "timestep:287, pyg_AUC: 0.4845\n",
      "timestep:288, pyg_AUC: 0.4873\n",
      "timestep:289, pyg_AUC: 0.4873\n",
      "timestep:290, pyg_AUC: 0.4873\n",
      "timestep:291, pyg_AUC: 0.4901\n",
      "timestep:292, pyg_AUC: 0.4887\n",
      "timestep:293, pyg_AUC: 0.4859\n",
      "timestep:294, pyg_AUC: 0.4873\n",
      "timestep:295, pyg_AUC: 0.4887\n",
      "timestep:296, pyg_AUC: 0.4873\n",
      "timestep:297, pyg_AUC: 0.4873\n",
      "timestep:298, pyg_AUC: 0.4901\n",
      "timestep:299, pyg_AUC: 0.4845\n",
      "timestep:300, pyg_AUC: 0.4887\n",
      "timestep:301, pyg_AUC: 0.4915\n",
      "timestep:302, pyg_AUC: 0.4802\n",
      "timestep:303, pyg_AUC: 0.4873\n",
      "timestep:304, pyg_AUC: 0.4887\n",
      "timestep:305, pyg_AUC: 0.4816\n",
      "timestep:306, pyg_AUC: 0.4929\n",
      "timestep:307, pyg_AUC: 0.4901\n",
      "timestep:308, pyg_AUC: 0.4901\n",
      "timestep:309, pyg_AUC: 0.4873\n",
      "timestep:310, pyg_AUC: 0.4887\n",
      "timestep:311, pyg_AUC: 0.4859\n",
      "timestep:312, pyg_AUC: 0.4873\n",
      "timestep:313, pyg_AUC: 0.4859\n",
      "timestep:314, pyg_AUC: 0.4901\n",
      "timestep:315, pyg_AUC: 0.4873\n",
      "timestep:316, pyg_AUC: 0.4873\n",
      "timestep:317, pyg_AUC: 0.4873\n",
      "timestep:318, pyg_AUC: 0.4845\n",
      "timestep:319, pyg_AUC: 0.4915\n",
      "timestep:320, pyg_AUC: 0.4887\n",
      "timestep:321, pyg_AUC: 0.4887\n",
      "timestep:322, pyg_AUC: 0.4901\n",
      "timestep:323, pyg_AUC: 0.4901\n",
      "timestep:324, pyg_AUC: 0.4887\n",
      "timestep:325, pyg_AUC: 0.4873\n",
      "timestep:326, pyg_AUC: 0.4887\n",
      "timestep:327, pyg_AUC: 0.4873\n",
      "timestep:328, pyg_AUC: 0.4887\n",
      "timestep:329, pyg_AUC: 0.4859\n",
      "timestep:330, pyg_AUC: 0.4929\n",
      "timestep:331, pyg_AUC: 0.4831\n",
      "timestep:332, pyg_AUC: 0.4887\n",
      "timestep:333, pyg_AUC: 0.4887\n",
      "timestep:334, pyg_AUC: 0.4859\n",
      "timestep:335, pyg_AUC: 0.4859\n",
      "timestep:336, pyg_AUC: 0.4887\n",
      "timestep:337, pyg_AUC: 0.4873\n",
      "timestep:338, pyg_AUC: 0.4831\n",
      "timestep:339, pyg_AUC: 0.4901\n",
      "timestep:340, pyg_AUC: 0.4887\n",
      "timestep:341, pyg_AUC: 0.4873\n",
      "timestep:342, pyg_AUC: 0.4887\n",
      "timestep:343, pyg_AUC: 0.4887\n",
      "timestep:344, pyg_AUC: 0.4901\n",
      "timestep:345, pyg_AUC: 0.4887\n",
      "timestep:346, pyg_AUC: 0.4873\n",
      "timestep:347, pyg_AUC: 0.4901\n",
      "timestep:348, pyg_AUC: 0.4859\n",
      "timestep:349, pyg_AUC: 0.4859\n",
      "timestep:350, pyg_AUC: 0.4788\n",
      "timestep:351, pyg_AUC: 0.4901\n",
      "timestep:352, pyg_AUC: 0.4873\n",
      "timestep:353, pyg_AUC: 0.4859\n",
      "timestep:354, pyg_AUC: 0.4929\n",
      "timestep:355, pyg_AUC: 0.4845\n",
      "timestep:356, pyg_AUC: 0.4873\n",
      "timestep:357, pyg_AUC: 0.4901\n",
      "timestep:358, pyg_AUC: 0.4845\n",
      "timestep:359, pyg_AUC: 0.4901\n",
      "timestep:360, pyg_AUC: 0.4859\n",
      "timestep:361, pyg_AUC: 0.4859\n",
      "timestep:362, pyg_AUC: 0.4915\n",
      "timestep:363, pyg_AUC: 0.4901\n",
      "timestep:364, pyg_AUC: 0.4901\n",
      "timestep:365, pyg_AUC: 0.4915\n",
      "timestep:366, pyg_AUC: 0.4873\n",
      "timestep:367, pyg_AUC: 0.4901\n",
      "timestep:368, pyg_AUC: 0.4873\n",
      "timestep:369, pyg_AUC: 0.4873\n",
      "timestep:370, pyg_AUC: 0.4901\n",
      "timestep:371, pyg_AUC: 0.4887\n",
      "timestep:372, pyg_AUC: 0.4845\n",
      "timestep:373, pyg_AUC: 0.4873\n",
      "timestep:374, pyg_AUC: 0.4859\n",
      "timestep:375, pyg_AUC: 0.4887\n",
      "timestep:376, pyg_AUC: 0.4831\n",
      "timestep:377, pyg_AUC: 0.4901\n",
      "timestep:378, pyg_AUC: 0.4873\n",
      "timestep:379, pyg_AUC: 0.4859\n",
      "timestep:380, pyg_AUC: 0.4929\n",
      "timestep:381, pyg_AUC: 0.4887\n",
      "timestep:382, pyg_AUC: 0.4915\n",
      "timestep:383, pyg_AUC: 0.4873\n",
      "timestep:384, pyg_AUC: 0.4873\n",
      "timestep:385, pyg_AUC: 0.4831\n",
      "timestep:386, pyg_AUC: 0.4802\n",
      "timestep:387, pyg_AUC: 0.4845\n",
      "timestep:388, pyg_AUC: 0.4915\n",
      "timestep:389, pyg_AUC: 0.4901\n",
      "timestep:390, pyg_AUC: 0.4901\n",
      "timestep:391, pyg_AUC: 0.4901\n",
      "timestep:392, pyg_AUC: 0.4929\n",
      "timestep:393, pyg_AUC: 0.4887\n",
      "timestep:394, pyg_AUC: 0.4859\n",
      "timestep:395, pyg_AUC: 0.4873\n",
      "timestep:396, pyg_AUC: 0.4873\n",
      "timestep:397, pyg_AUC: 0.4887\n",
      "timestep:398, pyg_AUC: 0.4901\n",
      "timestep:399, pyg_AUC: 0.4929\n",
      "timestep:400, pyg_AUC: 0.4873\n",
      "timestep:401, pyg_AUC: 0.4929\n",
      "timestep:402, pyg_AUC: 0.4901\n",
      "timestep:403, pyg_AUC: 0.4887\n",
      "timestep:404, pyg_AUC: 0.4845\n",
      "timestep:405, pyg_AUC: 0.4859\n",
      "timestep:406, pyg_AUC: 0.4887\n",
      "timestep:407, pyg_AUC: 0.4845\n",
      "timestep:408, pyg_AUC: 0.4831\n",
      "timestep:409, pyg_AUC: 0.4901\n",
      "timestep:410, pyg_AUC: 0.4901\n",
      "timestep:411, pyg_AUC: 0.4901\n",
      "timestep:412, pyg_AUC: 0.4887\n",
      "timestep:413, pyg_AUC: 0.4915\n",
      "timestep:414, pyg_AUC: 0.4887\n",
      "timestep:415, pyg_AUC: 0.4845\n",
      "timestep:416, pyg_AUC: 0.4831\n",
      "timestep:417, pyg_AUC: 0.4944\n",
      "timestep:418, pyg_AUC: 0.4929\n",
      "timestep:419, pyg_AUC: 0.4859\n",
      "timestep:420, pyg_AUC: 0.4873\n",
      "timestep:421, pyg_AUC: 0.4901\n",
      "timestep:422, pyg_AUC: 0.4929\n",
      "timestep:423, pyg_AUC: 0.4873\n",
      "timestep:424, pyg_AUC: 0.4845\n",
      "timestep:425, pyg_AUC: 0.4887\n",
      "timestep:426, pyg_AUC: 0.4831\n",
      "timestep:427, pyg_AUC: 0.4873\n",
      "timestep:428, pyg_AUC: 0.4901\n",
      "timestep:429, pyg_AUC: 0.4859\n",
      "timestep:430, pyg_AUC: 0.4887\n",
      "timestep:431, pyg_AUC: 0.4859\n",
      "timestep:432, pyg_AUC: 0.4901\n",
      "timestep:433, pyg_AUC: 0.4901\n",
      "timestep:434, pyg_AUC: 0.4901\n",
      "timestep:435, pyg_AUC: 0.4958\n",
      "timestep:436, pyg_AUC: 0.4873\n",
      "timestep:437, pyg_AUC: 0.4859\n",
      "timestep:438, pyg_AUC: 0.4845\n",
      "timestep:439, pyg_AUC: 0.4929\n",
      "timestep:440, pyg_AUC: 0.4944\n",
      "timestep:441, pyg_AUC: 0.4929\n",
      "timestep:442, pyg_AUC: 0.4873\n",
      "timestep:443, pyg_AUC: 0.4901\n",
      "timestep:444, pyg_AUC: 0.4887\n",
      "timestep:445, pyg_AUC: 0.4901\n",
      "timestep:446, pyg_AUC: 0.4887\n",
      "timestep:447, pyg_AUC: 0.4859\n",
      "timestep:448, pyg_AUC: 0.4859\n",
      "timestep:449, pyg_AUC: 0.4831\n",
      "timestep:450, pyg_AUC: 0.4859\n",
      "timestep:451, pyg_AUC: 0.4831\n",
      "timestep:452, pyg_AUC: 0.4873\n",
      "timestep:453, pyg_AUC: 0.4845\n",
      "timestep:454, pyg_AUC: 0.4915\n",
      "timestep:455, pyg_AUC: 0.4845\n",
      "timestep:456, pyg_AUC: 0.4873\n",
      "timestep:457, pyg_AUC: 0.4929\n",
      "timestep:458, pyg_AUC: 0.4859\n",
      "timestep:459, pyg_AUC: 0.4859\n",
      "timestep:460, pyg_AUC: 0.4901\n",
      "timestep:461, pyg_AUC: 0.4859\n",
      "timestep:462, pyg_AUC: 0.4845\n",
      "timestep:463, pyg_AUC: 0.4887\n",
      "timestep:464, pyg_AUC: 0.4859\n",
      "timestep:465, pyg_AUC: 0.4859\n",
      "timestep:466, pyg_AUC: 0.4831\n",
      "timestep:467, pyg_AUC: 0.4831\n",
      "timestep:468, pyg_AUC: 0.4929\n",
      "timestep:469, pyg_AUC: 0.4887\n",
      "timestep:470, pyg_AUC: 0.4859\n",
      "timestep:471, pyg_AUC: 0.4901\n",
      "timestep:472, pyg_AUC: 0.4887\n",
      "timestep:473, pyg_AUC: 0.4901\n",
      "timestep:474, pyg_AUC: 0.4873\n",
      "timestep:475, pyg_AUC: 0.4802\n",
      "timestep:476, pyg_AUC: 0.4901\n",
      "timestep:477, pyg_AUC: 0.4873\n",
      "timestep:478, pyg_AUC: 0.4915\n",
      "timestep:479, pyg_AUC: 0.4859\n",
      "timestep:480, pyg_AUC: 0.4901\n",
      "timestep:481, pyg_AUC: 0.4802\n",
      "timestep:482, pyg_AUC: 0.4816\n",
      "timestep:483, pyg_AUC: 0.4901\n",
      "timestep:484, pyg_AUC: 0.4816\n",
      "timestep:485, pyg_AUC: 0.4887\n",
      "timestep:486, pyg_AUC: 0.4873\n",
      "timestep:487, pyg_AUC: 0.4887\n",
      "timestep:488, pyg_AUC: 0.4887\n",
      "timestep:489, pyg_AUC: 0.4901\n",
      "timestep:490, pyg_AUC: 0.4816\n",
      "timestep:491, pyg_AUC: 0.4901\n",
      "timestep:492, pyg_AUC: 0.4859\n",
      "timestep:493, pyg_AUC: 0.4845\n",
      "timestep:494, pyg_AUC: 0.4887\n",
      "timestep:495, pyg_AUC: 0.4788\n",
      "timestep:496, pyg_AUC: 0.4944\n",
      "timestep:497, pyg_AUC: 0.4831\n",
      "timestep:498, pyg_AUC: 0.4859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [11:41<21:43, 100.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:499, pyg_AUC: 0.4845\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 36.04852\n",
      "Epoch: 0010 loss= 21.20682\n",
      "Epoch: 0020 loss= 17.15179\n",
      "Epoch: 0030 loss= 15.52214\n",
      "Epoch: 0040 loss= 12.81039\n",
      "Epoch: 0050 loss= 7.01130\n",
      "Epoch: 0060 loss= 1.65614\n",
      "Epoch: 0070 loss= 0.89220\n",
      "Epoch: 0080 loss= 0.72031\n",
      "Epoch: 0090 loss= 0.69835\n",
      "Epoch: 0100 loss= 0.62402\n",
      "Epoch: 0110 loss= 0.68970\n",
      "Epoch: 0120 loss= 0.57058\n",
      "Epoch: 0130 loss= 0.56835\n",
      "Epoch: 0140 loss= 0.55246\n",
      "Epoch: 0150 loss= 0.64890\n",
      "Epoch: 0160 loss= 0.58354\n",
      "Epoch: 0170 loss= 0.58221\n",
      "Epoch: 0180 loss= 0.57907\n",
      "Epoch: 0190 loss= 0.58469\n",
      "Epoch: 0200 loss= 0.54268\n",
      "Epoch: 0210 loss= 0.49849\n",
      "Epoch: 0220 loss= 0.53378\n",
      "Epoch: 0230 loss= 0.56737\n",
      "Epoch: 0240 loss= 0.63000\n",
      "Epoch: 0250 loss= 0.53736\n",
      "Epoch: 0260 loss= 0.57091\n",
      "Epoch: 0270 loss= 0.59361\n",
      "Epoch: 0280 loss= 0.51767\n",
      "Epoch: 0290 loss= 0.62360\n",
      "Epoch: 0300 loss= 0.57281\n",
      "Epoch: 0310 loss= 0.51701\n",
      "Epoch: 0320 loss= 0.49488\n",
      "Epoch: 0330 loss= 0.56683\n",
      "Epoch: 0340 loss= 0.56669\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.8349,  4.8094, -5.4094, -4.7019, -4.4335,  5.7023,  5.5578, -5.3136]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 38.12880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss= 18.65797\n",
      "Epoch: 0020 loss= 12.12986\n",
      "Epoch: 0030 loss= 8.37839\n",
      "Epoch: 0040 loss= 1.58492\n",
      "Epoch: 0050 loss= 1.37858\n",
      "Epoch: 0060 loss= 0.84123\n",
      "Epoch: 0070 loss= 0.59859\n",
      "Epoch: 0080 loss= 0.62477\n",
      "Epoch: 0090 loss= 0.75054\n",
      "Epoch: 0100 loss= 0.61656\n",
      "Epoch: 0110 loss= 0.64501\n",
      "Epoch: 0120 loss= 0.78631\n",
      "Epoch: 0130 loss= 0.65359\n",
      "Epoch: 0140 loss= 0.58450\n",
      "Epoch: 0150 loss= 0.53318\n",
      "Epoch: 0160 loss= 0.54930\n",
      "Epoch: 0170 loss= 0.54437\n",
      "Epoch: 0180 loss= 0.68925\n",
      "Epoch: 0190 loss= 0.55348\n",
      "Epoch: 0200 loss= 0.56601\n",
      "Epoch: 0210 loss= 0.51698\n",
      "Epoch: 0220 loss= 0.61534\n",
      "Epoch: 0230 loss= 0.57268\n",
      "Epoch: 0240 loss= 0.53197\n",
      "Epoch: 0250 loss= 0.56855\n",
      "Epoch: 0260 loss= 0.58838\n",
      "Epoch: 0270 loss= 0.58513\n",
      "Epoch: 0280 loss= 0.58654\n",
      "Epoch: 0290 loss= 0.56222\n",
      "Epoch: 0300 loss= 0.58426\n",
      "Epoch: 0310 loss= 0.57357\n",
      "Epoch: 0320 loss= 0.56261\n",
      "Epoch: 0330 loss= 0.53348\n",
      "Epoch: 0340 loss= 0.50835\n",
      "Epoch: 0350 loss= 0.51057\n",
      "Epoch: 0360 loss= 0.61422\n",
      "Epoch: 0370 loss= 0.53901\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4915\n",
      "timestep:1, pyg_AUC: 0.4901\n",
      "timestep:2, pyg_AUC: 0.4873\n",
      "timestep:3, pyg_AUC: 0.4845\n",
      "timestep:4, pyg_AUC: 0.4929\n",
      "timestep:5, pyg_AUC: 0.4887\n",
      "timestep:6, pyg_AUC: 0.4915\n",
      "timestep:7, pyg_AUC: 0.4873\n",
      "timestep:8, pyg_AUC: 0.4929\n",
      "timestep:9, pyg_AUC: 0.4873\n",
      "timestep:10, pyg_AUC: 0.4887\n",
      "timestep:11, pyg_AUC: 0.4845\n",
      "timestep:12, pyg_AUC: 0.4929\n",
      "timestep:13, pyg_AUC: 0.4915\n",
      "timestep:14, pyg_AUC: 0.4915\n",
      "timestep:15, pyg_AUC: 0.4901\n",
      "timestep:16, pyg_AUC: 0.4845\n",
      "timestep:17, pyg_AUC: 0.4915\n",
      "timestep:18, pyg_AUC: 0.4887\n",
      "timestep:19, pyg_AUC: 0.4887\n",
      "timestep:20, pyg_AUC: 0.4859\n",
      "timestep:21, pyg_AUC: 0.4887\n",
      "timestep:22, pyg_AUC: 0.4887\n",
      "timestep:23, pyg_AUC: 0.4873\n",
      "timestep:24, pyg_AUC: 0.4887\n",
      "timestep:25, pyg_AUC: 0.4915\n",
      "timestep:26, pyg_AUC: 0.4873\n",
      "timestep:27, pyg_AUC: 0.4901\n",
      "timestep:28, pyg_AUC: 0.4859\n",
      "timestep:29, pyg_AUC: 0.4845\n",
      "timestep:30, pyg_AUC: 0.4915\n",
      "timestep:31, pyg_AUC: 0.4958\n",
      "timestep:32, pyg_AUC: 0.4915\n",
      "timestep:33, pyg_AUC: 0.4887\n",
      "timestep:34, pyg_AUC: 0.4901\n",
      "timestep:35, pyg_AUC: 0.4887\n",
      "timestep:36, pyg_AUC: 0.4845\n",
      "timestep:37, pyg_AUC: 0.4887\n",
      "timestep:38, pyg_AUC: 0.4901\n",
      "timestep:39, pyg_AUC: 0.4887\n",
      "timestep:40, pyg_AUC: 0.4859\n",
      "timestep:41, pyg_AUC: 0.4887\n",
      "timestep:42, pyg_AUC: 0.4901\n",
      "timestep:43, pyg_AUC: 0.4915\n",
      "timestep:44, pyg_AUC: 0.4887\n",
      "timestep:45, pyg_AUC: 0.4901\n",
      "timestep:46, pyg_AUC: 0.4873\n",
      "timestep:47, pyg_AUC: 0.4915\n",
      "timestep:48, pyg_AUC: 0.4831\n",
      "timestep:49, pyg_AUC: 0.4915\n",
      "timestep:50, pyg_AUC: 0.4887\n",
      "timestep:51, pyg_AUC: 0.4901\n",
      "timestep:52, pyg_AUC: 0.4887\n",
      "timestep:53, pyg_AUC: 0.4831\n",
      "timestep:54, pyg_AUC: 0.4873\n",
      "timestep:55, pyg_AUC: 0.4901\n",
      "timestep:56, pyg_AUC: 0.4887\n",
      "timestep:57, pyg_AUC: 0.4887\n",
      "timestep:58, pyg_AUC: 0.4873\n",
      "timestep:59, pyg_AUC: 0.4859\n",
      "timestep:60, pyg_AUC: 0.4845\n",
      "timestep:61, pyg_AUC: 0.4887\n",
      "timestep:62, pyg_AUC: 0.4859\n",
      "timestep:63, pyg_AUC: 0.4859\n",
      "timestep:64, pyg_AUC: 0.4901\n",
      "timestep:65, pyg_AUC: 0.4915\n",
      "timestep:66, pyg_AUC: 0.4873\n",
      "timestep:67, pyg_AUC: 0.4859\n",
      "timestep:68, pyg_AUC: 0.4901\n",
      "timestep:69, pyg_AUC: 0.4901\n",
      "timestep:70, pyg_AUC: 0.4887\n",
      "timestep:71, pyg_AUC: 0.4887\n",
      "timestep:72, pyg_AUC: 0.4873\n",
      "timestep:73, pyg_AUC: 0.4873\n",
      "timestep:74, pyg_AUC: 0.4887\n",
      "timestep:75, pyg_AUC: 0.4887\n",
      "timestep:76, pyg_AUC: 0.4859\n",
      "timestep:77, pyg_AUC: 0.4859\n",
      "timestep:78, pyg_AUC: 0.4887\n",
      "timestep:79, pyg_AUC: 0.4873\n",
      "timestep:80, pyg_AUC: 0.4873\n",
      "timestep:81, pyg_AUC: 0.4859\n",
      "timestep:82, pyg_AUC: 0.4859\n",
      "timestep:83, pyg_AUC: 0.4873\n",
      "timestep:84, pyg_AUC: 0.4944\n",
      "timestep:85, pyg_AUC: 0.4859\n",
      "timestep:86, pyg_AUC: 0.4873\n",
      "timestep:87, pyg_AUC: 0.4929\n",
      "timestep:88, pyg_AUC: 0.4915\n",
      "timestep:89, pyg_AUC: 0.4887\n",
      "timestep:90, pyg_AUC: 0.4873\n",
      "timestep:91, pyg_AUC: 0.4901\n",
      "timestep:92, pyg_AUC: 0.4859\n",
      "timestep:93, pyg_AUC: 0.4887\n",
      "timestep:94, pyg_AUC: 0.4915\n",
      "timestep:95, pyg_AUC: 0.4915\n",
      "timestep:96, pyg_AUC: 0.4887\n",
      "timestep:97, pyg_AUC: 0.4859\n",
      "timestep:98, pyg_AUC: 0.4887\n",
      "timestep:99, pyg_AUC: 0.4873\n",
      "timestep:100, pyg_AUC: 0.4859\n",
      "timestep:101, pyg_AUC: 0.4859\n",
      "timestep:102, pyg_AUC: 0.4929\n",
      "timestep:103, pyg_AUC: 0.4831\n",
      "timestep:104, pyg_AUC: 0.4873\n",
      "timestep:105, pyg_AUC: 0.4887\n",
      "timestep:106, pyg_AUC: 0.4873\n",
      "timestep:107, pyg_AUC: 0.4901\n",
      "timestep:108, pyg_AUC: 0.4887\n",
      "timestep:109, pyg_AUC: 0.4873\n",
      "timestep:110, pyg_AUC: 0.4929\n",
      "timestep:111, pyg_AUC: 0.4816\n",
      "timestep:112, pyg_AUC: 0.4873\n",
      "timestep:113, pyg_AUC: 0.4944\n",
      "timestep:114, pyg_AUC: 0.4901\n",
      "timestep:115, pyg_AUC: 0.4859\n",
      "timestep:116, pyg_AUC: 0.4873\n",
      "timestep:117, pyg_AUC: 0.4887\n",
      "timestep:118, pyg_AUC: 0.4859\n",
      "timestep:119, pyg_AUC: 0.4845\n",
      "timestep:120, pyg_AUC: 0.4887\n",
      "timestep:121, pyg_AUC: 0.4845\n",
      "timestep:122, pyg_AUC: 0.4915\n",
      "timestep:123, pyg_AUC: 0.4873\n",
      "timestep:124, pyg_AUC: 0.4901\n",
      "timestep:125, pyg_AUC: 0.4831\n",
      "timestep:126, pyg_AUC: 0.4901\n",
      "timestep:127, pyg_AUC: 0.4901\n",
      "timestep:128, pyg_AUC: 0.4859\n",
      "timestep:129, pyg_AUC: 0.4845\n",
      "timestep:130, pyg_AUC: 0.4859\n",
      "timestep:131, pyg_AUC: 0.4901\n",
      "timestep:132, pyg_AUC: 0.4929\n",
      "timestep:133, pyg_AUC: 0.4944\n",
      "timestep:134, pyg_AUC: 0.4901\n",
      "timestep:135, pyg_AUC: 0.4873\n",
      "timestep:136, pyg_AUC: 0.4845\n",
      "timestep:137, pyg_AUC: 0.4944\n",
      "timestep:138, pyg_AUC: 0.4944\n",
      "timestep:139, pyg_AUC: 0.4929\n",
      "timestep:140, pyg_AUC: 0.4901\n",
      "timestep:141, pyg_AUC: 0.4901\n",
      "timestep:142, pyg_AUC: 0.4915\n",
      "timestep:143, pyg_AUC: 0.4873\n",
      "timestep:144, pyg_AUC: 0.4915\n",
      "timestep:145, pyg_AUC: 0.4887\n",
      "timestep:146, pyg_AUC: 0.4859\n",
      "timestep:147, pyg_AUC: 0.4929\n",
      "timestep:148, pyg_AUC: 0.4915\n",
      "timestep:149, pyg_AUC: 0.4901\n",
      "timestep:150, pyg_AUC: 0.4915\n",
      "timestep:151, pyg_AUC: 0.4929\n",
      "timestep:152, pyg_AUC: 0.4929\n",
      "timestep:153, pyg_AUC: 0.4929\n",
      "timestep:154, pyg_AUC: 0.4859\n",
      "timestep:155, pyg_AUC: 0.4901\n",
      "timestep:156, pyg_AUC: 0.4901\n",
      "timestep:157, pyg_AUC: 0.4915\n",
      "timestep:158, pyg_AUC: 0.4887\n",
      "timestep:159, pyg_AUC: 0.4859\n",
      "timestep:160, pyg_AUC: 0.4887\n",
      "timestep:161, pyg_AUC: 0.4901\n",
      "timestep:162, pyg_AUC: 0.4915\n",
      "timestep:163, pyg_AUC: 0.4915\n",
      "timestep:164, pyg_AUC: 0.4915\n",
      "timestep:165, pyg_AUC: 0.4831\n",
      "timestep:166, pyg_AUC: 0.4915\n",
      "timestep:167, pyg_AUC: 0.4915\n",
      "timestep:168, pyg_AUC: 0.4859\n",
      "timestep:169, pyg_AUC: 0.4873\n",
      "timestep:170, pyg_AUC: 0.4901\n",
      "timestep:171, pyg_AUC: 0.4901\n",
      "timestep:172, pyg_AUC: 0.4831\n",
      "timestep:173, pyg_AUC: 0.4887\n",
      "timestep:174, pyg_AUC: 0.4873\n",
      "timestep:175, pyg_AUC: 0.4901\n",
      "timestep:176, pyg_AUC: 0.4873\n",
      "timestep:177, pyg_AUC: 0.4887\n",
      "timestep:178, pyg_AUC: 0.4915\n",
      "timestep:179, pyg_AUC: 0.4915\n",
      "timestep:180, pyg_AUC: 0.4915\n",
      "timestep:181, pyg_AUC: 0.4873\n",
      "timestep:182, pyg_AUC: 0.4901\n",
      "timestep:183, pyg_AUC: 0.4915\n",
      "timestep:184, pyg_AUC: 0.4929\n",
      "timestep:185, pyg_AUC: 0.4887\n",
      "timestep:186, pyg_AUC: 0.4915\n",
      "timestep:187, pyg_AUC: 0.4901\n",
      "timestep:188, pyg_AUC: 0.4915\n",
      "timestep:189, pyg_AUC: 0.4873\n",
      "timestep:190, pyg_AUC: 0.4929\n",
      "timestep:191, pyg_AUC: 0.4901\n",
      "timestep:192, pyg_AUC: 0.4915\n",
      "timestep:193, pyg_AUC: 0.4901\n",
      "timestep:194, pyg_AUC: 0.4915\n",
      "timestep:195, pyg_AUC: 0.4915\n",
      "timestep:196, pyg_AUC: 0.4915\n",
      "timestep:197, pyg_AUC: 0.4915\n",
      "timestep:198, pyg_AUC: 0.4901\n",
      "timestep:199, pyg_AUC: 0.4859\n",
      "timestep:200, pyg_AUC: 0.4915\n",
      "timestep:201, pyg_AUC: 0.4887\n",
      "timestep:202, pyg_AUC: 0.4859\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4915\n",
      "timestep:205, pyg_AUC: 0.4915\n",
      "timestep:206, pyg_AUC: 0.4901\n",
      "timestep:207, pyg_AUC: 0.4901\n",
      "timestep:208, pyg_AUC: 0.4887\n",
      "timestep:209, pyg_AUC: 0.4873\n",
      "timestep:210, pyg_AUC: 0.4915\n",
      "timestep:211, pyg_AUC: 0.4887\n",
      "timestep:212, pyg_AUC: 0.4845\n",
      "timestep:213, pyg_AUC: 0.4915\n",
      "timestep:214, pyg_AUC: 0.4873\n",
      "timestep:215, pyg_AUC: 0.4915\n",
      "timestep:216, pyg_AUC: 0.4887\n",
      "timestep:217, pyg_AUC: 0.4887\n",
      "timestep:218, pyg_AUC: 0.4873\n",
      "timestep:219, pyg_AUC: 0.4887\n",
      "timestep:220, pyg_AUC: 0.4887\n",
      "timestep:221, pyg_AUC: 0.4887\n",
      "timestep:222, pyg_AUC: 0.4873\n",
      "timestep:223, pyg_AUC: 0.4873\n",
      "timestep:224, pyg_AUC: 0.4915\n",
      "timestep:225, pyg_AUC: 0.4887\n",
      "timestep:226, pyg_AUC: 0.4915\n",
      "timestep:227, pyg_AUC: 0.4887\n",
      "timestep:228, pyg_AUC: 0.4901\n",
      "timestep:229, pyg_AUC: 0.4859\n",
      "timestep:230, pyg_AUC: 0.4859\n",
      "timestep:231, pyg_AUC: 0.4901\n",
      "timestep:232, pyg_AUC: 0.4887\n",
      "timestep:233, pyg_AUC: 0.4859\n",
      "timestep:234, pyg_AUC: 0.4901\n",
      "timestep:235, pyg_AUC: 0.4901\n",
      "timestep:236, pyg_AUC: 0.4859\n",
      "timestep:237, pyg_AUC: 0.4887\n",
      "timestep:238, pyg_AUC: 0.4887\n",
      "timestep:239, pyg_AUC: 0.4845\n",
      "timestep:240, pyg_AUC: 0.4873\n",
      "timestep:241, pyg_AUC: 0.4873\n",
      "timestep:242, pyg_AUC: 0.4873\n",
      "timestep:243, pyg_AUC: 0.4887\n",
      "timestep:244, pyg_AUC: 0.4901\n",
      "timestep:245, pyg_AUC: 0.4859\n",
      "timestep:246, pyg_AUC: 0.4887\n",
      "timestep:247, pyg_AUC: 0.4915\n",
      "timestep:248, pyg_AUC: 0.4873\n",
      "timestep:249, pyg_AUC: 0.4901\n",
      "timestep:250, pyg_AUC: 0.4845\n",
      "timestep:251, pyg_AUC: 0.4901\n",
      "timestep:252, pyg_AUC: 0.4887\n",
      "timestep:253, pyg_AUC: 0.4887\n",
      "timestep:254, pyg_AUC: 0.4845\n",
      "timestep:255, pyg_AUC: 0.4873\n",
      "timestep:256, pyg_AUC: 0.4887\n",
      "timestep:257, pyg_AUC: 0.4887\n",
      "timestep:258, pyg_AUC: 0.4887\n",
      "timestep:259, pyg_AUC: 0.4887\n",
      "timestep:260, pyg_AUC: 0.4901\n",
      "timestep:261, pyg_AUC: 0.4915\n",
      "timestep:262, pyg_AUC: 0.4901\n",
      "timestep:263, pyg_AUC: 0.4859\n",
      "timestep:264, pyg_AUC: 0.4859\n",
      "timestep:265, pyg_AUC: 0.4901\n",
      "timestep:266, pyg_AUC: 0.4887\n",
      "timestep:267, pyg_AUC: 0.4887\n",
      "timestep:268, pyg_AUC: 0.4915\n",
      "timestep:269, pyg_AUC: 0.4859\n",
      "timestep:270, pyg_AUC: 0.4845\n",
      "timestep:271, pyg_AUC: 0.4859\n",
      "timestep:272, pyg_AUC: 0.4873\n",
      "timestep:273, pyg_AUC: 0.4845\n",
      "timestep:274, pyg_AUC: 0.4831\n",
      "timestep:275, pyg_AUC: 0.4901\n",
      "timestep:276, pyg_AUC: 0.4873\n",
      "timestep:277, pyg_AUC: 0.4901\n",
      "timestep:278, pyg_AUC: 0.4901\n",
      "timestep:279, pyg_AUC: 0.4901\n",
      "timestep:280, pyg_AUC: 0.4901\n",
      "timestep:281, pyg_AUC: 0.4901\n",
      "timestep:282, pyg_AUC: 0.4859\n",
      "timestep:283, pyg_AUC: 0.4873\n",
      "timestep:284, pyg_AUC: 0.4845\n",
      "timestep:285, pyg_AUC: 0.4901\n",
      "timestep:286, pyg_AUC: 0.4873\n",
      "timestep:287, pyg_AUC: 0.4873\n",
      "timestep:288, pyg_AUC: 0.4887\n",
      "timestep:289, pyg_AUC: 0.4929\n",
      "timestep:290, pyg_AUC: 0.4887\n",
      "timestep:291, pyg_AUC: 0.4887\n",
      "timestep:292, pyg_AUC: 0.4873\n",
      "timestep:293, pyg_AUC: 0.4887\n",
      "timestep:294, pyg_AUC: 0.4887\n",
      "timestep:295, pyg_AUC: 0.4859\n",
      "timestep:296, pyg_AUC: 0.4887\n",
      "timestep:297, pyg_AUC: 0.4915\n",
      "timestep:298, pyg_AUC: 0.4887\n",
      "timestep:299, pyg_AUC: 0.4915\n",
      "timestep:300, pyg_AUC: 0.4873\n",
      "timestep:301, pyg_AUC: 0.4887\n",
      "timestep:302, pyg_AUC: 0.4887\n",
      "timestep:303, pyg_AUC: 0.4845\n",
      "timestep:304, pyg_AUC: 0.4887\n",
      "timestep:305, pyg_AUC: 0.4873\n",
      "timestep:306, pyg_AUC: 0.4873\n",
      "timestep:307, pyg_AUC: 0.4873\n",
      "timestep:308, pyg_AUC: 0.4901\n",
      "timestep:309, pyg_AUC: 0.4887\n",
      "timestep:310, pyg_AUC: 0.4873\n",
      "timestep:311, pyg_AUC: 0.4859\n",
      "timestep:312, pyg_AUC: 0.4873\n",
      "timestep:313, pyg_AUC: 0.4845\n",
      "timestep:314, pyg_AUC: 0.4831\n",
      "timestep:315, pyg_AUC: 0.4915\n",
      "timestep:316, pyg_AUC: 0.4859\n",
      "timestep:317, pyg_AUC: 0.4887\n",
      "timestep:318, pyg_AUC: 0.4859\n",
      "timestep:319, pyg_AUC: 0.4831\n",
      "timestep:320, pyg_AUC: 0.4845\n",
      "timestep:321, pyg_AUC: 0.4873\n",
      "timestep:322, pyg_AUC: 0.4887\n",
      "timestep:323, pyg_AUC: 0.4887\n",
      "timestep:324, pyg_AUC: 0.4873\n",
      "timestep:325, pyg_AUC: 0.4859\n",
      "timestep:326, pyg_AUC: 0.4873\n",
      "timestep:327, pyg_AUC: 0.4845\n",
      "timestep:328, pyg_AUC: 0.4873\n",
      "timestep:329, pyg_AUC: 0.4887\n",
      "timestep:330, pyg_AUC: 0.4873\n",
      "timestep:331, pyg_AUC: 0.4873\n",
      "timestep:332, pyg_AUC: 0.4887\n",
      "timestep:333, pyg_AUC: 0.4873\n",
      "timestep:334, pyg_AUC: 0.4887\n",
      "timestep:335, pyg_AUC: 0.4873\n",
      "timestep:336, pyg_AUC: 0.4873\n",
      "timestep:337, pyg_AUC: 0.4873\n",
      "timestep:338, pyg_AUC: 0.4873\n",
      "timestep:339, pyg_AUC: 0.4887\n",
      "timestep:340, pyg_AUC: 0.4859\n",
      "timestep:341, pyg_AUC: 0.4845\n",
      "timestep:342, pyg_AUC: 0.4873\n",
      "timestep:343, pyg_AUC: 0.4873\n",
      "timestep:344, pyg_AUC: 0.4873\n",
      "timestep:345, pyg_AUC: 0.4887\n",
      "timestep:346, pyg_AUC: 0.4887\n",
      "timestep:347, pyg_AUC: 0.4859\n",
      "timestep:348, pyg_AUC: 0.4887\n",
      "timestep:349, pyg_AUC: 0.4845\n",
      "timestep:350, pyg_AUC: 0.4887\n",
      "timestep:351, pyg_AUC: 0.4873\n",
      "timestep:352, pyg_AUC: 0.4859\n",
      "timestep:353, pyg_AUC: 0.4859\n",
      "timestep:354, pyg_AUC: 0.4859\n",
      "timestep:355, pyg_AUC: 0.4859\n",
      "timestep:356, pyg_AUC: 0.4859\n",
      "timestep:357, pyg_AUC: 0.4873\n",
      "timestep:358, pyg_AUC: 0.4859\n",
      "timestep:359, pyg_AUC: 0.4859\n",
      "timestep:360, pyg_AUC: 0.4873\n",
      "timestep:361, pyg_AUC: 0.4873\n",
      "timestep:362, pyg_AUC: 0.4873\n",
      "timestep:363, pyg_AUC: 0.4873\n",
      "timestep:364, pyg_AUC: 0.4873\n",
      "timestep:365, pyg_AUC: 0.4873\n",
      "timestep:366, pyg_AUC: 0.4873\n",
      "timestep:367, pyg_AUC: 0.4859\n",
      "timestep:368, pyg_AUC: 0.4873\n",
      "timestep:369, pyg_AUC: 0.4859\n",
      "timestep:370, pyg_AUC: 0.4859\n",
      "timestep:371, pyg_AUC: 0.4873\n",
      "timestep:372, pyg_AUC: 0.4873\n",
      "timestep:373, pyg_AUC: 0.4873\n",
      "timestep:374, pyg_AUC: 0.4873\n",
      "timestep:375, pyg_AUC: 0.4859\n",
      "timestep:376, pyg_AUC: 0.4845\n",
      "timestep:377, pyg_AUC: 0.4845\n",
      "timestep:378, pyg_AUC: 0.4873\n",
      "timestep:379, pyg_AUC: 0.4901\n",
      "timestep:380, pyg_AUC: 0.4887\n",
      "timestep:381, pyg_AUC: 0.4873\n",
      "timestep:382, pyg_AUC: 0.4859\n",
      "timestep:383, pyg_AUC: 0.4873\n",
      "timestep:384, pyg_AUC: 0.4859\n",
      "timestep:385, pyg_AUC: 0.4831\n",
      "timestep:386, pyg_AUC: 0.4873\n",
      "timestep:387, pyg_AUC: 0.4887\n",
      "timestep:388, pyg_AUC: 0.4873\n",
      "timestep:389, pyg_AUC: 0.4831\n",
      "timestep:390, pyg_AUC: 0.4859\n",
      "timestep:391, pyg_AUC: 0.4859\n",
      "timestep:392, pyg_AUC: 0.4873\n",
      "timestep:393, pyg_AUC: 0.4873\n",
      "timestep:394, pyg_AUC: 0.4887\n",
      "timestep:395, pyg_AUC: 0.4887\n",
      "timestep:396, pyg_AUC: 0.4873\n",
      "timestep:397, pyg_AUC: 0.4859\n",
      "timestep:398, pyg_AUC: 0.4831\n",
      "timestep:399, pyg_AUC: 0.4887\n",
      "timestep:400, pyg_AUC: 0.4859\n",
      "timestep:401, pyg_AUC: 0.4831\n",
      "timestep:402, pyg_AUC: 0.4859\n",
      "timestep:403, pyg_AUC: 0.4887\n",
      "timestep:404, pyg_AUC: 0.4859\n",
      "timestep:405, pyg_AUC: 0.4873\n",
      "timestep:406, pyg_AUC: 0.4873\n",
      "timestep:407, pyg_AUC: 0.4887\n",
      "timestep:408, pyg_AUC: 0.4859\n",
      "timestep:409, pyg_AUC: 0.4859\n",
      "timestep:410, pyg_AUC: 0.4845\n",
      "timestep:411, pyg_AUC: 0.4845\n",
      "timestep:412, pyg_AUC: 0.4859\n",
      "timestep:413, pyg_AUC: 0.4873\n",
      "timestep:414, pyg_AUC: 0.4845\n",
      "timestep:415, pyg_AUC: 0.4859\n",
      "timestep:416, pyg_AUC: 0.4915\n",
      "timestep:417, pyg_AUC: 0.4859\n",
      "timestep:418, pyg_AUC: 0.4831\n",
      "timestep:419, pyg_AUC: 0.4859\n",
      "timestep:420, pyg_AUC: 0.4859\n",
      "timestep:421, pyg_AUC: 0.4901\n",
      "timestep:422, pyg_AUC: 0.4873\n",
      "timestep:423, pyg_AUC: 0.4887\n",
      "timestep:424, pyg_AUC: 0.4859\n",
      "timestep:425, pyg_AUC: 0.4859\n",
      "timestep:426, pyg_AUC: 0.4845\n",
      "timestep:427, pyg_AUC: 0.4887\n",
      "timestep:428, pyg_AUC: 0.4859\n",
      "timestep:429, pyg_AUC: 0.4887\n",
      "timestep:430, pyg_AUC: 0.4873\n",
      "timestep:431, pyg_AUC: 0.4859\n",
      "timestep:432, pyg_AUC: 0.4859\n",
      "timestep:433, pyg_AUC: 0.4859\n",
      "timestep:434, pyg_AUC: 0.4859\n",
      "timestep:435, pyg_AUC: 0.4859\n",
      "timestep:436, pyg_AUC: 0.4859\n",
      "timestep:437, pyg_AUC: 0.4873\n",
      "timestep:438, pyg_AUC: 0.4845\n",
      "timestep:439, pyg_AUC: 0.4873\n",
      "timestep:440, pyg_AUC: 0.4887\n",
      "timestep:441, pyg_AUC: 0.4873\n",
      "timestep:442, pyg_AUC: 0.4873\n",
      "timestep:443, pyg_AUC: 0.4831\n",
      "timestep:444, pyg_AUC: 0.4859\n",
      "timestep:445, pyg_AUC: 0.4845\n",
      "timestep:446, pyg_AUC: 0.4887\n",
      "timestep:447, pyg_AUC: 0.4873\n",
      "timestep:448, pyg_AUC: 0.4887\n",
      "timestep:449, pyg_AUC: 0.4887\n",
      "timestep:450, pyg_AUC: 0.4887\n",
      "timestep:451, pyg_AUC: 0.4859\n",
      "timestep:452, pyg_AUC: 0.4873\n",
      "timestep:453, pyg_AUC: 0.4887\n",
      "timestep:454, pyg_AUC: 0.4873\n",
      "timestep:455, pyg_AUC: 0.4873\n",
      "timestep:456, pyg_AUC: 0.4873\n",
      "timestep:457, pyg_AUC: 0.4859\n",
      "timestep:458, pyg_AUC: 0.4831\n",
      "timestep:459, pyg_AUC: 0.4887\n",
      "timestep:460, pyg_AUC: 0.4859\n",
      "timestep:461, pyg_AUC: 0.4887\n",
      "timestep:462, pyg_AUC: 0.4859\n",
      "timestep:463, pyg_AUC: 0.4873\n",
      "timestep:464, pyg_AUC: 0.4859\n",
      "timestep:465, pyg_AUC: 0.4859\n",
      "timestep:466, pyg_AUC: 0.4887\n",
      "timestep:467, pyg_AUC: 0.4873\n",
      "timestep:468, pyg_AUC: 0.4873\n",
      "timestep:469, pyg_AUC: 0.4887\n",
      "timestep:470, pyg_AUC: 0.4887\n",
      "timestep:471, pyg_AUC: 0.4887\n",
      "timestep:472, pyg_AUC: 0.4859\n",
      "timestep:473, pyg_AUC: 0.4873\n",
      "timestep:474, pyg_AUC: 0.4873\n",
      "timestep:475, pyg_AUC: 0.4887\n",
      "timestep:476, pyg_AUC: 0.4831\n",
      "timestep:477, pyg_AUC: 0.4873\n",
      "timestep:478, pyg_AUC: 0.4873\n",
      "timestep:479, pyg_AUC: 0.4873\n",
      "timestep:480, pyg_AUC: 0.4873\n",
      "timestep:481, pyg_AUC: 0.4887\n",
      "timestep:482, pyg_AUC: 0.4859\n",
      "timestep:483, pyg_AUC: 0.4887\n",
      "timestep:484, pyg_AUC: 0.4859\n",
      "timestep:485, pyg_AUC: 0.4873\n",
      "timestep:486, pyg_AUC: 0.4887\n",
      "timestep:487, pyg_AUC: 0.4887\n",
      "timestep:488, pyg_AUC: 0.4887\n",
      "timestep:489, pyg_AUC: 0.4873\n",
      "timestep:490, pyg_AUC: 0.4873\n",
      "timestep:491, pyg_AUC: 0.4845\n",
      "timestep:492, pyg_AUC: 0.4845\n",
      "timestep:493, pyg_AUC: 0.4873\n",
      "timestep:494, pyg_AUC: 0.4901\n",
      "timestep:495, pyg_AUC: 0.4831\n",
      "timestep:496, pyg_AUC: 0.4873\n",
      "timestep:497, pyg_AUC: 0.4887\n",
      "timestep:498, pyg_AUC: 0.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [13:23<20:07, 100.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:499, pyg_AUC: 0.4887\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 40.08904\n",
      "Epoch: 0010 loss= 24.59587\n",
      "Epoch: 0020 loss= 18.23821\n",
      "Epoch: 0030 loss= 15.96839\n",
      "Epoch: 0040 loss= 10.45609\n",
      "Epoch: 0050 loss= 1.78387\n",
      "Epoch: 0060 loss= 2.07045\n",
      "Epoch: 0070 loss= 0.84090\n",
      "Epoch: 0080 loss= 0.73431\n",
      "Epoch: 0090 loss= 0.77535\n",
      "Epoch: 0100 loss= 0.69004\n",
      "Epoch: 0110 loss= 0.68497\n",
      "Epoch: 0120 loss= 0.73894\n",
      "Epoch: 0130 loss= 0.71031\n",
      "Epoch: 0140 loss= 0.69673\n",
      "Epoch: 0150 loss= 0.59125\n",
      "Epoch: 0160 loss= 0.58074\n",
      "Epoch: 0170 loss= 0.58226\n",
      "Epoch: 0180 loss= 0.63660\n",
      "Epoch: 0190 loss= 0.62512\n",
      "Epoch: 0200 loss= 0.55665\n",
      "Epoch: 0210 loss= 0.62328\n",
      "Epoch: 0220 loss= 0.61565\n",
      "Epoch: 0230 loss= 0.58649\n",
      "Epoch: 0240 loss= 0.58926\n",
      "Epoch: 0250 loss= 0.56000\n",
      "Epoch: 0260 loss= 0.58832\n",
      "Epoch: 0270 loss= 0.59169\n",
      "Epoch: 0280 loss= 0.55733\n",
      "Epoch: 0290 loss= 0.54055\n",
      "Epoch: 0300 loss= 0.60310\n",
      "Epoch: 0310 loss= 0.58121\n",
      "Epoch: 0320 loss= 0.53030\n",
      "Epoch: 0330 loss= 0.55677\n",
      "Epoch: 0340 loss= 0.58418\n",
      "Epoch: 0350 loss= 0.50872\n",
      "Epoch: 0360 loss= 0.58279\n",
      "Epoch: 0370 loss= 0.59407\n",
      "Epoch: 0380 loss= 0.49109\n",
      "Epoch: 0390 loss= 0.62480\n",
      "Epoch: 0400 loss= 0.52162\n",
      "Epoch: 0410 loss= 0.61569\n",
      "Epoch: 0420 loss= 0.59002\n",
      "Epoch: 0430 loss= 0.57384\n",
      "Epoch: 0440 loss= 0.54947\n",
      "Epoch: 0450 loss= 0.62524\n",
      "Epoch: 0460 loss= 0.60731\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7448,  4.7338, -5.3433, -4.6125, -4.3862,  5.6541,  5.4484, -5.2244]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 36.73578\n",
      "Epoch: 0010 loss= 23.66158\n",
      "Epoch: 0020 loss= 17.67880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0030 loss= 12.70227\n",
      "Epoch: 0040 loss= 5.18452\n",
      "Epoch: 0050 loss= 1.14904\n",
      "Epoch: 0060 loss= 0.97878\n",
      "Epoch: 0070 loss= 0.70795\n",
      "Epoch: 0080 loss= 0.58560\n",
      "Epoch: 0090 loss= 0.61537\n",
      "Epoch: 0100 loss= 0.60637\n",
      "Epoch: 0110 loss= 0.58919\n",
      "Epoch: 0120 loss= 0.65843\n",
      "Epoch: 0130 loss= 0.60368\n",
      "Epoch: 0140 loss= 0.63667\n",
      "Epoch: 0150 loss= 0.62997\n",
      "Epoch: 0160 loss= 0.64687\n",
      "Epoch: 0170 loss= 0.58297\n",
      "Epoch: 0180 loss= 0.54966\n",
      "Epoch: 0190 loss= 0.52281\n",
      "Epoch: 0200 loss= 0.58987\n",
      "Epoch: 0210 loss= 0.58067\n",
      "Epoch: 0220 loss= 0.61337\n",
      "Epoch: 0230 loss= 0.55011\n",
      "Epoch: 0240 loss= 0.60010\n",
      "Epoch: 0250 loss= 0.62341\n",
      "Epoch: 0260 loss= 0.67706\n",
      "Epoch: 0270 loss= 0.53032\n",
      "Epoch: 0280 loss= 0.58574\n",
      "Epoch: 0290 loss= 0.60657\n",
      "Epoch: 0300 loss= 0.50432\n",
      "Epoch: 0310 loss= 0.60256\n",
      "Epoch: 0320 loss= 0.57000\n",
      "Epoch: 0330 loss= 0.58846\n",
      "Epoch: 0340 loss= 0.55091\n",
      "Epoch: 0350 loss= 0.58578\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4845\n",
      "timestep:1, pyg_AUC: 0.4944\n",
      "timestep:2, pyg_AUC: 0.4915\n",
      "timestep:3, pyg_AUC: 0.4887\n",
      "timestep:4, pyg_AUC: 0.4887\n",
      "timestep:5, pyg_AUC: 0.4887\n",
      "timestep:6, pyg_AUC: 0.4873\n",
      "timestep:7, pyg_AUC: 0.4929\n",
      "timestep:8, pyg_AUC: 0.4915\n",
      "timestep:9, pyg_AUC: 0.4873\n",
      "timestep:10, pyg_AUC: 0.4901\n",
      "timestep:11, pyg_AUC: 0.4944\n",
      "timestep:12, pyg_AUC: 0.4859\n",
      "timestep:13, pyg_AUC: 0.4887\n",
      "timestep:14, pyg_AUC: 0.4831\n",
      "timestep:15, pyg_AUC: 0.4873\n",
      "timestep:16, pyg_AUC: 0.4915\n",
      "timestep:17, pyg_AUC: 0.4859\n",
      "timestep:18, pyg_AUC: 0.4873\n",
      "timestep:19, pyg_AUC: 0.4859\n",
      "timestep:20, pyg_AUC: 0.4859\n",
      "timestep:21, pyg_AUC: 0.4929\n",
      "timestep:22, pyg_AUC: 0.4901\n",
      "timestep:23, pyg_AUC: 0.4873\n",
      "timestep:24, pyg_AUC: 0.4859\n",
      "timestep:25, pyg_AUC: 0.4915\n",
      "timestep:26, pyg_AUC: 0.4873\n",
      "timestep:27, pyg_AUC: 0.4901\n",
      "timestep:28, pyg_AUC: 0.4915\n",
      "timestep:29, pyg_AUC: 0.4901\n",
      "timestep:30, pyg_AUC: 0.4929\n",
      "timestep:31, pyg_AUC: 0.4873\n",
      "timestep:32, pyg_AUC: 0.4816\n",
      "timestep:33, pyg_AUC: 0.4845\n",
      "timestep:34, pyg_AUC: 0.4887\n",
      "timestep:35, pyg_AUC: 0.4887\n",
      "timestep:36, pyg_AUC: 0.4873\n",
      "timestep:37, pyg_AUC: 0.4887\n",
      "timestep:38, pyg_AUC: 0.4901\n",
      "timestep:39, pyg_AUC: 0.4859\n",
      "timestep:40, pyg_AUC: 0.4859\n",
      "timestep:41, pyg_AUC: 0.4887\n",
      "timestep:42, pyg_AUC: 0.4901\n",
      "timestep:43, pyg_AUC: 0.4887\n",
      "timestep:44, pyg_AUC: 0.4901\n",
      "timestep:45, pyg_AUC: 0.4887\n",
      "timestep:46, pyg_AUC: 0.4887\n",
      "timestep:47, pyg_AUC: 0.4859\n",
      "timestep:48, pyg_AUC: 0.4901\n",
      "timestep:49, pyg_AUC: 0.4901\n",
      "timestep:50, pyg_AUC: 0.4901\n",
      "timestep:51, pyg_AUC: 0.4915\n",
      "timestep:52, pyg_AUC: 0.4816\n",
      "timestep:53, pyg_AUC: 0.4944\n",
      "timestep:54, pyg_AUC: 0.4887\n",
      "timestep:55, pyg_AUC: 0.4915\n",
      "timestep:56, pyg_AUC: 0.4873\n",
      "timestep:57, pyg_AUC: 0.4873\n",
      "timestep:58, pyg_AUC: 0.4901\n",
      "timestep:59, pyg_AUC: 0.4845\n",
      "timestep:60, pyg_AUC: 0.4873\n",
      "timestep:61, pyg_AUC: 0.4915\n",
      "timestep:62, pyg_AUC: 0.4929\n",
      "timestep:63, pyg_AUC: 0.4901\n",
      "timestep:64, pyg_AUC: 0.4845\n",
      "timestep:65, pyg_AUC: 0.4929\n",
      "timestep:66, pyg_AUC: 0.4831\n",
      "timestep:67, pyg_AUC: 0.4859\n",
      "timestep:68, pyg_AUC: 0.4859\n",
      "timestep:69, pyg_AUC: 0.4901\n",
      "timestep:70, pyg_AUC: 0.4929\n",
      "timestep:71, pyg_AUC: 0.4873\n",
      "timestep:72, pyg_AUC: 0.4873\n",
      "timestep:73, pyg_AUC: 0.4887\n",
      "timestep:74, pyg_AUC: 0.4929\n",
      "timestep:75, pyg_AUC: 0.4873\n",
      "timestep:76, pyg_AUC: 0.4887\n",
      "timestep:77, pyg_AUC: 0.4929\n",
      "timestep:78, pyg_AUC: 0.4831\n",
      "timestep:79, pyg_AUC: 0.4873\n",
      "timestep:80, pyg_AUC: 0.4873\n",
      "timestep:81, pyg_AUC: 0.4972\n",
      "timestep:82, pyg_AUC: 0.4901\n",
      "timestep:83, pyg_AUC: 0.4887\n",
      "timestep:84, pyg_AUC: 0.4901\n",
      "timestep:85, pyg_AUC: 0.4958\n",
      "timestep:86, pyg_AUC: 0.4873\n",
      "timestep:87, pyg_AUC: 0.4944\n",
      "timestep:88, pyg_AUC: 0.4845\n",
      "timestep:89, pyg_AUC: 0.4887\n",
      "timestep:90, pyg_AUC: 0.4929\n",
      "timestep:91, pyg_AUC: 0.4901\n",
      "timestep:92, pyg_AUC: 0.4915\n",
      "timestep:93, pyg_AUC: 0.4873\n",
      "timestep:94, pyg_AUC: 0.4915\n",
      "timestep:95, pyg_AUC: 0.4929\n",
      "timestep:96, pyg_AUC: 0.4816\n",
      "timestep:97, pyg_AUC: 0.4915\n",
      "timestep:98, pyg_AUC: 0.4929\n",
      "timestep:99, pyg_AUC: 0.4901\n",
      "timestep:100, pyg_AUC: 0.4887\n",
      "timestep:101, pyg_AUC: 0.4831\n",
      "timestep:102, pyg_AUC: 0.4887\n",
      "timestep:103, pyg_AUC: 0.4845\n",
      "timestep:104, pyg_AUC: 0.4901\n",
      "timestep:105, pyg_AUC: 0.4873\n",
      "timestep:106, pyg_AUC: 0.4944\n",
      "timestep:107, pyg_AUC: 0.4915\n",
      "timestep:108, pyg_AUC: 0.4915\n",
      "timestep:109, pyg_AUC: 0.4816\n",
      "timestep:110, pyg_AUC: 0.4859\n",
      "timestep:111, pyg_AUC: 0.4929\n",
      "timestep:112, pyg_AUC: 0.4887\n",
      "timestep:113, pyg_AUC: 0.4944\n",
      "timestep:114, pyg_AUC: 0.4887\n",
      "timestep:115, pyg_AUC: 0.4873\n",
      "timestep:116, pyg_AUC: 0.4873\n",
      "timestep:117, pyg_AUC: 0.4887\n",
      "timestep:118, pyg_AUC: 0.4887\n",
      "timestep:119, pyg_AUC: 0.4915\n",
      "timestep:120, pyg_AUC: 0.4929\n",
      "timestep:121, pyg_AUC: 0.4859\n",
      "timestep:122, pyg_AUC: 0.4887\n",
      "timestep:123, pyg_AUC: 0.4915\n",
      "timestep:124, pyg_AUC: 0.4929\n",
      "timestep:125, pyg_AUC: 0.4845\n",
      "timestep:126, pyg_AUC: 0.4929\n",
      "timestep:127, pyg_AUC: 0.4915\n",
      "timestep:128, pyg_AUC: 0.4901\n",
      "timestep:129, pyg_AUC: 0.4887\n",
      "timestep:130, pyg_AUC: 0.4901\n",
      "timestep:131, pyg_AUC: 0.4873\n",
      "timestep:132, pyg_AUC: 0.4845\n",
      "timestep:133, pyg_AUC: 0.4901\n",
      "timestep:134, pyg_AUC: 0.4887\n",
      "timestep:135, pyg_AUC: 0.4929\n",
      "timestep:136, pyg_AUC: 0.4873\n",
      "timestep:137, pyg_AUC: 0.4929\n",
      "timestep:138, pyg_AUC: 0.4901\n",
      "timestep:139, pyg_AUC: 0.4887\n",
      "timestep:140, pyg_AUC: 0.4845\n",
      "timestep:141, pyg_AUC: 0.4901\n",
      "timestep:142, pyg_AUC: 0.4873\n",
      "timestep:143, pyg_AUC: 0.4901\n",
      "timestep:144, pyg_AUC: 0.4901\n",
      "timestep:145, pyg_AUC: 0.4929\n",
      "timestep:146, pyg_AUC: 0.4887\n",
      "timestep:147, pyg_AUC: 0.4929\n",
      "timestep:148, pyg_AUC: 0.4915\n",
      "timestep:149, pyg_AUC: 0.4901\n",
      "timestep:150, pyg_AUC: 0.4901\n",
      "timestep:151, pyg_AUC: 0.4929\n",
      "timestep:152, pyg_AUC: 0.4887\n",
      "timestep:153, pyg_AUC: 0.4802\n",
      "timestep:154, pyg_AUC: 0.4915\n",
      "timestep:155, pyg_AUC: 0.4915\n",
      "timestep:156, pyg_AUC: 0.4901\n",
      "timestep:157, pyg_AUC: 0.4873\n",
      "timestep:158, pyg_AUC: 0.4859\n",
      "timestep:159, pyg_AUC: 0.4845\n",
      "timestep:160, pyg_AUC: 0.4944\n",
      "timestep:161, pyg_AUC: 0.4929\n",
      "timestep:162, pyg_AUC: 0.4887\n",
      "timestep:163, pyg_AUC: 0.4915\n",
      "timestep:164, pyg_AUC: 0.4915\n",
      "timestep:165, pyg_AUC: 0.4887\n",
      "timestep:166, pyg_AUC: 0.4915\n",
      "timestep:167, pyg_AUC: 0.4873\n",
      "timestep:168, pyg_AUC: 0.4845\n",
      "timestep:169, pyg_AUC: 0.4873\n",
      "timestep:170, pyg_AUC: 0.4887\n",
      "timestep:171, pyg_AUC: 0.4901\n",
      "timestep:172, pyg_AUC: 0.4845\n",
      "timestep:173, pyg_AUC: 0.4929\n",
      "timestep:174, pyg_AUC: 0.4859\n",
      "timestep:175, pyg_AUC: 0.4887\n",
      "timestep:176, pyg_AUC: 0.4901\n",
      "timestep:177, pyg_AUC: 0.4901\n",
      "timestep:178, pyg_AUC: 0.4929\n",
      "timestep:179, pyg_AUC: 0.4929\n",
      "timestep:180, pyg_AUC: 0.4887\n",
      "timestep:181, pyg_AUC: 0.4873\n",
      "timestep:182, pyg_AUC: 0.4901\n",
      "timestep:183, pyg_AUC: 0.4859\n",
      "timestep:184, pyg_AUC: 0.4873\n",
      "timestep:185, pyg_AUC: 0.4929\n",
      "timestep:186, pyg_AUC: 0.4929\n",
      "timestep:187, pyg_AUC: 0.4901\n",
      "timestep:188, pyg_AUC: 0.4929\n",
      "timestep:189, pyg_AUC: 0.4901\n",
      "timestep:190, pyg_AUC: 0.4887\n",
      "timestep:191, pyg_AUC: 0.4901\n",
      "timestep:192, pyg_AUC: 0.4816\n",
      "timestep:193, pyg_AUC: 0.4887\n",
      "timestep:194, pyg_AUC: 0.4859\n",
      "timestep:195, pyg_AUC: 0.4929\n",
      "timestep:196, pyg_AUC: 0.4915\n",
      "timestep:197, pyg_AUC: 0.4944\n",
      "timestep:198, pyg_AUC: 0.4859\n",
      "timestep:199, pyg_AUC: 0.4958\n",
      "timestep:200, pyg_AUC: 0.4901\n",
      "timestep:201, pyg_AUC: 0.4873\n",
      "timestep:202, pyg_AUC: 0.4873\n",
      "timestep:203, pyg_AUC: 0.4901\n",
      "timestep:204, pyg_AUC: 0.4859\n",
      "timestep:205, pyg_AUC: 0.4901\n",
      "timestep:206, pyg_AUC: 0.4816\n",
      "timestep:207, pyg_AUC: 0.4915\n",
      "timestep:208, pyg_AUC: 0.4816\n",
      "timestep:209, pyg_AUC: 0.4873\n",
      "timestep:210, pyg_AUC: 0.4845\n",
      "timestep:211, pyg_AUC: 0.4887\n",
      "timestep:212, pyg_AUC: 0.4887\n",
      "timestep:213, pyg_AUC: 0.4873\n",
      "timestep:214, pyg_AUC: 0.4915\n",
      "timestep:215, pyg_AUC: 0.4873\n",
      "timestep:216, pyg_AUC: 0.4915\n",
      "timestep:217, pyg_AUC: 0.4887\n",
      "timestep:218, pyg_AUC: 0.4845\n",
      "timestep:219, pyg_AUC: 0.4929\n",
      "timestep:220, pyg_AUC: 0.4929\n",
      "timestep:221, pyg_AUC: 0.4901\n",
      "timestep:222, pyg_AUC: 0.4873\n",
      "timestep:223, pyg_AUC: 0.4887\n",
      "timestep:224, pyg_AUC: 0.4901\n",
      "timestep:225, pyg_AUC: 0.4873\n",
      "timestep:226, pyg_AUC: 0.4859\n",
      "timestep:227, pyg_AUC: 0.4915\n",
      "timestep:228, pyg_AUC: 0.4845\n",
      "timestep:229, pyg_AUC: 0.4887\n",
      "timestep:230, pyg_AUC: 0.4873\n",
      "timestep:231, pyg_AUC: 0.4901\n",
      "timestep:232, pyg_AUC: 0.4873\n",
      "timestep:233, pyg_AUC: 0.4915\n",
      "timestep:234, pyg_AUC: 0.4901\n",
      "timestep:235, pyg_AUC: 0.4887\n",
      "timestep:236, pyg_AUC: 0.4859\n",
      "timestep:237, pyg_AUC: 0.4873\n",
      "timestep:238, pyg_AUC: 0.4915\n",
      "timestep:239, pyg_AUC: 0.4901\n",
      "timestep:240, pyg_AUC: 0.4873\n",
      "timestep:241, pyg_AUC: 0.4859\n",
      "timestep:242, pyg_AUC: 0.4845\n",
      "timestep:243, pyg_AUC: 0.4873\n",
      "timestep:244, pyg_AUC: 0.4887\n",
      "timestep:245, pyg_AUC: 0.4887\n",
      "timestep:246, pyg_AUC: 0.4831\n",
      "timestep:247, pyg_AUC: 0.4845\n",
      "timestep:248, pyg_AUC: 0.4887\n",
      "timestep:249, pyg_AUC: 0.4929\n",
      "timestep:250, pyg_AUC: 0.4873\n",
      "timestep:251, pyg_AUC: 0.4845\n",
      "timestep:252, pyg_AUC: 0.4873\n",
      "timestep:253, pyg_AUC: 0.4873\n",
      "timestep:254, pyg_AUC: 0.4887\n",
      "timestep:255, pyg_AUC: 0.4873\n",
      "timestep:256, pyg_AUC: 0.4901\n",
      "timestep:257, pyg_AUC: 0.4901\n",
      "timestep:258, pyg_AUC: 0.4944\n",
      "timestep:259, pyg_AUC: 0.4873\n",
      "timestep:260, pyg_AUC: 0.4873\n",
      "timestep:261, pyg_AUC: 0.4845\n",
      "timestep:262, pyg_AUC: 0.4901\n",
      "timestep:263, pyg_AUC: 0.4831\n",
      "timestep:264, pyg_AUC: 0.4901\n",
      "timestep:265, pyg_AUC: 0.4859\n",
      "timestep:266, pyg_AUC: 0.4859\n",
      "timestep:267, pyg_AUC: 0.4915\n",
      "timestep:268, pyg_AUC: 0.4873\n",
      "timestep:269, pyg_AUC: 0.4845\n",
      "timestep:270, pyg_AUC: 0.4887\n",
      "timestep:271, pyg_AUC: 0.4831\n",
      "timestep:272, pyg_AUC: 0.4887\n",
      "timestep:273, pyg_AUC: 0.4887\n",
      "timestep:274, pyg_AUC: 0.4816\n",
      "timestep:275, pyg_AUC: 0.4845\n",
      "timestep:276, pyg_AUC: 0.4887\n",
      "timestep:277, pyg_AUC: 0.4859\n",
      "timestep:278, pyg_AUC: 0.4859\n",
      "timestep:279, pyg_AUC: 0.4901\n",
      "timestep:280, pyg_AUC: 0.4859\n",
      "timestep:281, pyg_AUC: 0.4873\n",
      "timestep:282, pyg_AUC: 0.4816\n",
      "timestep:283, pyg_AUC: 0.4873\n",
      "timestep:284, pyg_AUC: 0.4915\n",
      "timestep:285, pyg_AUC: 0.4887\n",
      "timestep:286, pyg_AUC: 0.4873\n",
      "timestep:287, pyg_AUC: 0.4859\n",
      "timestep:288, pyg_AUC: 0.4873\n",
      "timestep:289, pyg_AUC: 0.4816\n",
      "timestep:290, pyg_AUC: 0.4859\n",
      "timestep:291, pyg_AUC: 0.4901\n",
      "timestep:292, pyg_AUC: 0.4873\n",
      "timestep:293, pyg_AUC: 0.4873\n",
      "timestep:294, pyg_AUC: 0.4901\n",
      "timestep:295, pyg_AUC: 0.4873\n",
      "timestep:296, pyg_AUC: 0.4887\n",
      "timestep:297, pyg_AUC: 0.4915\n",
      "timestep:298, pyg_AUC: 0.4887\n",
      "timestep:299, pyg_AUC: 0.4845\n",
      "timestep:300, pyg_AUC: 0.4873\n",
      "timestep:301, pyg_AUC: 0.4859\n",
      "timestep:302, pyg_AUC: 0.4859\n",
      "timestep:303, pyg_AUC: 0.4859\n",
      "timestep:304, pyg_AUC: 0.4887\n",
      "timestep:305, pyg_AUC: 0.4873\n",
      "timestep:306, pyg_AUC: 0.4859\n",
      "timestep:307, pyg_AUC: 0.4859\n",
      "timestep:308, pyg_AUC: 0.4873\n",
      "timestep:309, pyg_AUC: 0.4887\n",
      "timestep:310, pyg_AUC: 0.4845\n",
      "timestep:311, pyg_AUC: 0.4859\n",
      "timestep:312, pyg_AUC: 0.4901\n",
      "timestep:313, pyg_AUC: 0.4873\n",
      "timestep:314, pyg_AUC: 0.4831\n",
      "timestep:315, pyg_AUC: 0.4887\n",
      "timestep:316, pyg_AUC: 0.4845\n",
      "timestep:317, pyg_AUC: 0.4859\n",
      "timestep:318, pyg_AUC: 0.4901\n",
      "timestep:319, pyg_AUC: 0.4887\n",
      "timestep:320, pyg_AUC: 0.4873\n",
      "timestep:321, pyg_AUC: 0.4873\n",
      "timestep:322, pyg_AUC: 0.4887\n",
      "timestep:323, pyg_AUC: 0.4873\n",
      "timestep:324, pyg_AUC: 0.4873\n",
      "timestep:325, pyg_AUC: 0.4873\n",
      "timestep:326, pyg_AUC: 0.4887\n",
      "timestep:327, pyg_AUC: 0.4873\n",
      "timestep:328, pyg_AUC: 0.4887\n",
      "timestep:329, pyg_AUC: 0.4915\n",
      "timestep:330, pyg_AUC: 0.4901\n",
      "timestep:331, pyg_AUC: 0.4845\n",
      "timestep:332, pyg_AUC: 0.4873\n",
      "timestep:333, pyg_AUC: 0.4859\n",
      "timestep:334, pyg_AUC: 0.4901\n",
      "timestep:335, pyg_AUC: 0.4873\n",
      "timestep:336, pyg_AUC: 0.4887\n",
      "timestep:337, pyg_AUC: 0.4915\n",
      "timestep:338, pyg_AUC: 0.4873\n",
      "timestep:339, pyg_AUC: 0.4887\n",
      "timestep:340, pyg_AUC: 0.4873\n",
      "timestep:341, pyg_AUC: 0.4873\n",
      "timestep:342, pyg_AUC: 0.4816\n",
      "timestep:343, pyg_AUC: 0.4873\n",
      "timestep:344, pyg_AUC: 0.4859\n",
      "timestep:345, pyg_AUC: 0.4901\n",
      "timestep:346, pyg_AUC: 0.4915\n",
      "timestep:347, pyg_AUC: 0.4859\n",
      "timestep:348, pyg_AUC: 0.4873\n",
      "timestep:349, pyg_AUC: 0.4859\n",
      "timestep:350, pyg_AUC: 0.4887\n",
      "timestep:351, pyg_AUC: 0.4887\n",
      "timestep:352, pyg_AUC: 0.4915\n",
      "timestep:353, pyg_AUC: 0.4859\n",
      "timestep:354, pyg_AUC: 0.4873\n",
      "timestep:355, pyg_AUC: 0.4873\n",
      "timestep:356, pyg_AUC: 0.4901\n",
      "timestep:357, pyg_AUC: 0.4859\n",
      "timestep:358, pyg_AUC: 0.4859\n",
      "timestep:359, pyg_AUC: 0.4845\n",
      "timestep:360, pyg_AUC: 0.4887\n",
      "timestep:361, pyg_AUC: 0.4873\n",
      "timestep:362, pyg_AUC: 0.4845\n",
      "timestep:363, pyg_AUC: 0.4873\n",
      "timestep:364, pyg_AUC: 0.4873\n",
      "timestep:365, pyg_AUC: 0.4901\n",
      "timestep:366, pyg_AUC: 0.4831\n",
      "timestep:367, pyg_AUC: 0.4873\n",
      "timestep:368, pyg_AUC: 0.4901\n",
      "timestep:369, pyg_AUC: 0.4859\n",
      "timestep:370, pyg_AUC: 0.4887\n",
      "timestep:371, pyg_AUC: 0.4816\n",
      "timestep:372, pyg_AUC: 0.4901\n",
      "timestep:373, pyg_AUC: 0.4859\n",
      "timestep:374, pyg_AUC: 0.4915\n",
      "timestep:375, pyg_AUC: 0.4873\n",
      "timestep:376, pyg_AUC: 0.4859\n",
      "timestep:377, pyg_AUC: 0.4831\n",
      "timestep:378, pyg_AUC: 0.4887\n",
      "timestep:379, pyg_AUC: 0.4873\n",
      "timestep:380, pyg_AUC: 0.4845\n",
      "timestep:381, pyg_AUC: 0.4845\n",
      "timestep:382, pyg_AUC: 0.4859\n",
      "timestep:383, pyg_AUC: 0.4873\n",
      "timestep:384, pyg_AUC: 0.4901\n",
      "timestep:385, pyg_AUC: 0.4859\n",
      "timestep:386, pyg_AUC: 0.4859\n",
      "timestep:387, pyg_AUC: 0.4873\n",
      "timestep:388, pyg_AUC: 0.4873\n",
      "timestep:389, pyg_AUC: 0.4887\n",
      "timestep:390, pyg_AUC: 0.4887\n",
      "timestep:391, pyg_AUC: 0.4873\n",
      "timestep:392, pyg_AUC: 0.4845\n",
      "timestep:393, pyg_AUC: 0.4901\n",
      "timestep:394, pyg_AUC: 0.4859\n",
      "timestep:395, pyg_AUC: 0.4887\n",
      "timestep:396, pyg_AUC: 0.4873\n",
      "timestep:397, pyg_AUC: 0.4873\n",
      "timestep:398, pyg_AUC: 0.4887\n",
      "timestep:399, pyg_AUC: 0.4901\n",
      "timestep:400, pyg_AUC: 0.4859\n",
      "timestep:401, pyg_AUC: 0.4901\n",
      "timestep:402, pyg_AUC: 0.4873\n",
      "timestep:403, pyg_AUC: 0.4887\n",
      "timestep:404, pyg_AUC: 0.4887\n",
      "timestep:405, pyg_AUC: 0.4859\n",
      "timestep:406, pyg_AUC: 0.4845\n",
      "timestep:407, pyg_AUC: 0.4845\n",
      "timestep:408, pyg_AUC: 0.4901\n",
      "timestep:409, pyg_AUC: 0.4887\n",
      "timestep:410, pyg_AUC: 0.4915\n",
      "timestep:411, pyg_AUC: 0.4859\n",
      "timestep:412, pyg_AUC: 0.4901\n",
      "timestep:413, pyg_AUC: 0.4915\n",
      "timestep:414, pyg_AUC: 0.4929\n",
      "timestep:415, pyg_AUC: 0.4887\n",
      "timestep:416, pyg_AUC: 0.4859\n",
      "timestep:417, pyg_AUC: 0.4887\n",
      "timestep:418, pyg_AUC: 0.4901\n",
      "timestep:419, pyg_AUC: 0.4845\n",
      "timestep:420, pyg_AUC: 0.4887\n",
      "timestep:421, pyg_AUC: 0.4887\n",
      "timestep:422, pyg_AUC: 0.4915\n",
      "timestep:423, pyg_AUC: 0.4887\n",
      "timestep:424, pyg_AUC: 0.4901\n",
      "timestep:425, pyg_AUC: 0.4887\n",
      "timestep:426, pyg_AUC: 0.4915\n",
      "timestep:427, pyg_AUC: 0.4887\n",
      "timestep:428, pyg_AUC: 0.4831\n",
      "timestep:429, pyg_AUC: 0.4873\n",
      "timestep:430, pyg_AUC: 0.4859\n",
      "timestep:431, pyg_AUC: 0.4901\n",
      "timestep:432, pyg_AUC: 0.4873\n",
      "timestep:433, pyg_AUC: 0.4873\n",
      "timestep:434, pyg_AUC: 0.4901\n",
      "timestep:435, pyg_AUC: 0.4859\n",
      "timestep:436, pyg_AUC: 0.4873\n",
      "timestep:437, pyg_AUC: 0.4887\n",
      "timestep:438, pyg_AUC: 0.4859\n",
      "timestep:439, pyg_AUC: 0.4845\n",
      "timestep:440, pyg_AUC: 0.4915\n",
      "timestep:441, pyg_AUC: 0.4915\n",
      "timestep:442, pyg_AUC: 0.4873\n",
      "timestep:443, pyg_AUC: 0.4859\n",
      "timestep:444, pyg_AUC: 0.4873\n",
      "timestep:445, pyg_AUC: 0.4873\n",
      "timestep:446, pyg_AUC: 0.4873\n",
      "timestep:447, pyg_AUC: 0.4859\n",
      "timestep:448, pyg_AUC: 0.4859\n",
      "timestep:449, pyg_AUC: 0.4859\n",
      "timestep:450, pyg_AUC: 0.4915\n",
      "timestep:451, pyg_AUC: 0.4873\n",
      "timestep:452, pyg_AUC: 0.4901\n",
      "timestep:453, pyg_AUC: 0.4859\n",
      "timestep:454, pyg_AUC: 0.4901\n",
      "timestep:455, pyg_AUC: 0.4887\n",
      "timestep:456, pyg_AUC: 0.4915\n",
      "timestep:457, pyg_AUC: 0.4845\n",
      "timestep:458, pyg_AUC: 0.4873\n",
      "timestep:459, pyg_AUC: 0.4845\n",
      "timestep:460, pyg_AUC: 0.4901\n",
      "timestep:461, pyg_AUC: 0.4915\n",
      "timestep:462, pyg_AUC: 0.4887\n",
      "timestep:463, pyg_AUC: 0.4873\n",
      "timestep:464, pyg_AUC: 0.4901\n",
      "timestep:465, pyg_AUC: 0.4831\n",
      "timestep:466, pyg_AUC: 0.4887\n",
      "timestep:467, pyg_AUC: 0.4873\n",
      "timestep:468, pyg_AUC: 0.4929\n",
      "timestep:469, pyg_AUC: 0.4859\n",
      "timestep:470, pyg_AUC: 0.4887\n",
      "timestep:471, pyg_AUC: 0.4901\n",
      "timestep:472, pyg_AUC: 0.4859\n",
      "timestep:473, pyg_AUC: 0.4915\n",
      "timestep:474, pyg_AUC: 0.4873\n",
      "timestep:475, pyg_AUC: 0.4859\n",
      "timestep:476, pyg_AUC: 0.4901\n",
      "timestep:477, pyg_AUC: 0.4915\n",
      "timestep:478, pyg_AUC: 0.4887\n",
      "timestep:479, pyg_AUC: 0.4901\n",
      "timestep:480, pyg_AUC: 0.4873\n",
      "timestep:481, pyg_AUC: 0.4901\n",
      "timestep:482, pyg_AUC: 0.4901\n",
      "timestep:483, pyg_AUC: 0.4929\n",
      "timestep:484, pyg_AUC: 0.4845\n",
      "timestep:485, pyg_AUC: 0.4873\n",
      "timestep:486, pyg_AUC: 0.4915\n",
      "timestep:487, pyg_AUC: 0.4887\n",
      "timestep:488, pyg_AUC: 0.4901\n",
      "timestep:489, pyg_AUC: 0.4901\n",
      "timestep:490, pyg_AUC: 0.4901\n",
      "timestep:491, pyg_AUC: 0.4901\n",
      "timestep:492, pyg_AUC: 0.4873\n",
      "timestep:493, pyg_AUC: 0.4887\n",
      "timestep:494, pyg_AUC: 0.4873\n",
      "timestep:495, pyg_AUC: 0.4859\n",
      "timestep:496, pyg_AUC: 0.4887\n",
      "timestep:497, pyg_AUC: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [15:05<18:31, 101.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:498, pyg_AUC: 0.4915\n",
      "timestep:499, pyg_AUC: 0.4887\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 40.26085\n",
      "Epoch: 0010 loss= 22.15419\n",
      "Epoch: 0020 loss= 20.86200\n",
      "Epoch: 0030 loss= 18.15471\n",
      "Epoch: 0040 loss= 16.44251\n",
      "Epoch: 0050 loss= 11.18345\n",
      "Epoch: 0060 loss= 6.28638\n",
      "Epoch: 0070 loss= 1.63418\n",
      "Epoch: 0080 loss= 1.74932\n",
      "Epoch: 0090 loss= 0.85999\n",
      "Epoch: 0100 loss= 0.68743\n",
      "Epoch: 0110 loss= 0.55744\n",
      "Epoch: 0120 loss= 0.63184\n",
      "Epoch: 0130 loss= 0.62867\n",
      "Epoch: 0140 loss= 0.67911\n",
      "Epoch: 0150 loss= 0.63033\n",
      "Epoch: 0160 loss= 0.61746\n",
      "Epoch: 0170 loss= 0.59930\n",
      "Epoch: 0180 loss= 0.52183\n",
      "Epoch: 0190 loss= 0.62963\n",
      "Epoch: 0200 loss= 0.62968\n",
      "Epoch: 0210 loss= 0.60948\n",
      "Epoch: 0220 loss= 0.60556\n",
      "Epoch: 0230 loss= 0.58215\n",
      "Epoch: 0240 loss= 0.53245\n",
      "Epoch: 0250 loss= 0.55564\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.8127,  4.7619, -5.4034, -4.6698, -4.4210,  5.6952,  5.4990, -5.2999]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 37.85318\n",
      "Epoch: 0010 loss= 25.58685\n",
      "Epoch: 0020 loss= 16.22158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0030 loss= 11.97665\n",
      "Epoch: 0040 loss= 5.09608\n",
      "Epoch: 0050 loss= 2.03420\n",
      "Epoch: 0060 loss= 1.02793\n",
      "Epoch: 0070 loss= 0.79431\n",
      "Epoch: 0080 loss= 0.64497\n",
      "Epoch: 0090 loss= 0.57750\n",
      "Epoch: 0100 loss= 0.82892\n",
      "Epoch: 0110 loss= 0.68237\n",
      "Epoch: 0120 loss= 0.62658\n",
      "Epoch: 0130 loss= 0.54531\n",
      "Epoch: 0140 loss= 0.65706\n",
      "Epoch: 0150 loss= 0.69874\n",
      "Epoch: 0160 loss= 0.64743\n",
      "Epoch: 0170 loss= 0.59881\n",
      "Epoch: 0180 loss= 0.65388\n",
      "Epoch: 0190 loss= 0.53005\n",
      "Epoch: 0200 loss= 0.55560\n",
      "Epoch: 0210 loss= 0.55213\n",
      "Epoch: 0220 loss= 0.56445\n",
      "Epoch: 0230 loss= 0.61492\n",
      "Epoch: 0240 loss= 0.62763\n",
      "Epoch: 0250 loss= 0.59007\n",
      "Epoch: 0260 loss= 0.61574\n",
      "Epoch: 0270 loss= 0.53579\n",
      "Epoch: 0280 loss= 0.62249\n",
      "Epoch: 0290 loss= 0.57000\n",
      "Epoch: 0300 loss= 0.57561\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4831\n",
      "timestep:1, pyg_AUC: 0.4816\n",
      "timestep:2, pyg_AUC: 0.4929\n",
      "timestep:3, pyg_AUC: 0.4873\n",
      "timestep:4, pyg_AUC: 0.4901\n",
      "timestep:5, pyg_AUC: 0.4873\n",
      "timestep:6, pyg_AUC: 0.4944\n",
      "timestep:7, pyg_AUC: 0.4887\n",
      "timestep:8, pyg_AUC: 0.4887\n",
      "timestep:9, pyg_AUC: 0.4859\n",
      "timestep:10, pyg_AUC: 0.4873\n",
      "timestep:11, pyg_AUC: 0.4929\n",
      "timestep:12, pyg_AUC: 0.4887\n",
      "timestep:13, pyg_AUC: 0.4901\n",
      "timestep:14, pyg_AUC: 0.4901\n",
      "timestep:15, pyg_AUC: 0.4873\n",
      "timestep:16, pyg_AUC: 0.4887\n",
      "timestep:17, pyg_AUC: 0.4887\n",
      "timestep:18, pyg_AUC: 0.4831\n",
      "timestep:19, pyg_AUC: 0.4901\n",
      "timestep:20, pyg_AUC: 0.4887\n",
      "timestep:21, pyg_AUC: 0.4845\n",
      "timestep:22, pyg_AUC: 0.4901\n",
      "timestep:23, pyg_AUC: 0.4915\n",
      "timestep:24, pyg_AUC: 0.4915\n",
      "timestep:25, pyg_AUC: 0.4901\n",
      "timestep:26, pyg_AUC: 0.4929\n",
      "timestep:27, pyg_AUC: 0.4845\n",
      "timestep:28, pyg_AUC: 0.4774\n",
      "timestep:29, pyg_AUC: 0.4901\n",
      "timestep:30, pyg_AUC: 0.4887\n",
      "timestep:31, pyg_AUC: 0.4887\n",
      "timestep:32, pyg_AUC: 0.4901\n",
      "timestep:33, pyg_AUC: 0.4873\n",
      "timestep:34, pyg_AUC: 0.4901\n",
      "timestep:35, pyg_AUC: 0.4915\n",
      "timestep:36, pyg_AUC: 0.4859\n",
      "timestep:37, pyg_AUC: 0.4831\n",
      "timestep:38, pyg_AUC: 0.4887\n",
      "timestep:39, pyg_AUC: 0.4915\n",
      "timestep:40, pyg_AUC: 0.4859\n",
      "timestep:41, pyg_AUC: 0.4873\n",
      "timestep:42, pyg_AUC: 0.4845\n",
      "timestep:43, pyg_AUC: 0.4901\n",
      "timestep:44, pyg_AUC: 0.4915\n",
      "timestep:45, pyg_AUC: 0.4859\n",
      "timestep:46, pyg_AUC: 0.4901\n",
      "timestep:47, pyg_AUC: 0.4901\n",
      "timestep:48, pyg_AUC: 0.4929\n",
      "timestep:49, pyg_AUC: 0.4929\n",
      "timestep:50, pyg_AUC: 0.4859\n",
      "timestep:51, pyg_AUC: 0.4915\n",
      "timestep:52, pyg_AUC: 0.4887\n",
      "timestep:53, pyg_AUC: 0.4901\n",
      "timestep:54, pyg_AUC: 0.4887\n",
      "timestep:55, pyg_AUC: 0.4901\n",
      "timestep:56, pyg_AUC: 0.4873\n",
      "timestep:57, pyg_AUC: 0.4887\n",
      "timestep:58, pyg_AUC: 0.4901\n",
      "timestep:59, pyg_AUC: 0.4887\n",
      "timestep:60, pyg_AUC: 0.4887\n",
      "timestep:61, pyg_AUC: 0.4901\n",
      "timestep:62, pyg_AUC: 0.4873\n",
      "timestep:63, pyg_AUC: 0.4845\n",
      "timestep:64, pyg_AUC: 0.4887\n",
      "timestep:65, pyg_AUC: 0.4915\n",
      "timestep:66, pyg_AUC: 0.4887\n",
      "timestep:67, pyg_AUC: 0.4929\n",
      "timestep:68, pyg_AUC: 0.4901\n",
      "timestep:69, pyg_AUC: 0.4845\n",
      "timestep:70, pyg_AUC: 0.4901\n",
      "timestep:71, pyg_AUC: 0.4887\n",
      "timestep:72, pyg_AUC: 0.4873\n",
      "timestep:73, pyg_AUC: 0.4873\n",
      "timestep:74, pyg_AUC: 0.4859\n",
      "timestep:75, pyg_AUC: 0.4873\n",
      "timestep:76, pyg_AUC: 0.4901\n",
      "timestep:77, pyg_AUC: 0.4873\n",
      "timestep:78, pyg_AUC: 0.4887\n",
      "timestep:79, pyg_AUC: 0.4915\n",
      "timestep:80, pyg_AUC: 0.4929\n",
      "timestep:81, pyg_AUC: 0.4873\n",
      "timestep:82, pyg_AUC: 0.4859\n",
      "timestep:83, pyg_AUC: 0.4873\n",
      "timestep:84, pyg_AUC: 0.4873\n",
      "timestep:85, pyg_AUC: 0.4873\n",
      "timestep:86, pyg_AUC: 0.4915\n",
      "timestep:87, pyg_AUC: 0.4873\n",
      "timestep:88, pyg_AUC: 0.4873\n",
      "timestep:89, pyg_AUC: 0.4901\n",
      "timestep:90, pyg_AUC: 0.4859\n",
      "timestep:91, pyg_AUC: 0.4873\n",
      "timestep:92, pyg_AUC: 0.4915\n",
      "timestep:93, pyg_AUC: 0.4901\n",
      "timestep:94, pyg_AUC: 0.4873\n",
      "timestep:95, pyg_AUC: 0.4887\n",
      "timestep:96, pyg_AUC: 0.4901\n",
      "timestep:97, pyg_AUC: 0.4901\n",
      "timestep:98, pyg_AUC: 0.4873\n",
      "timestep:99, pyg_AUC: 0.4901\n",
      "timestep:100, pyg_AUC: 0.4845\n",
      "timestep:101, pyg_AUC: 0.4901\n",
      "timestep:102, pyg_AUC: 0.4915\n",
      "timestep:103, pyg_AUC: 0.4887\n",
      "timestep:104, pyg_AUC: 0.4944\n",
      "timestep:105, pyg_AUC: 0.4873\n",
      "timestep:106, pyg_AUC: 0.4887\n",
      "timestep:107, pyg_AUC: 0.4845\n",
      "timestep:108, pyg_AUC: 0.4887\n",
      "timestep:109, pyg_AUC: 0.4873\n",
      "timestep:110, pyg_AUC: 0.4901\n",
      "timestep:111, pyg_AUC: 0.4859\n",
      "timestep:112, pyg_AUC: 0.4887\n",
      "timestep:113, pyg_AUC: 0.4887\n",
      "timestep:114, pyg_AUC: 0.4859\n",
      "timestep:115, pyg_AUC: 0.4859\n",
      "timestep:116, pyg_AUC: 0.4901\n",
      "timestep:117, pyg_AUC: 0.4901\n",
      "timestep:118, pyg_AUC: 0.4873\n",
      "timestep:119, pyg_AUC: 0.4901\n",
      "timestep:120, pyg_AUC: 0.4901\n",
      "timestep:121, pyg_AUC: 0.4887\n",
      "timestep:122, pyg_AUC: 0.4873\n",
      "timestep:123, pyg_AUC: 0.4901\n",
      "timestep:124, pyg_AUC: 0.4873\n",
      "timestep:125, pyg_AUC: 0.4901\n",
      "timestep:126, pyg_AUC: 0.4845\n",
      "timestep:127, pyg_AUC: 0.4873\n",
      "timestep:128, pyg_AUC: 0.4845\n",
      "timestep:129, pyg_AUC: 0.4873\n",
      "timestep:130, pyg_AUC: 0.4859\n",
      "timestep:131, pyg_AUC: 0.4873\n",
      "timestep:132, pyg_AUC: 0.4887\n",
      "timestep:133, pyg_AUC: 0.4887\n",
      "timestep:134, pyg_AUC: 0.4845\n",
      "timestep:135, pyg_AUC: 0.4859\n",
      "timestep:136, pyg_AUC: 0.4873\n",
      "timestep:137, pyg_AUC: 0.4887\n",
      "timestep:138, pyg_AUC: 0.4901\n",
      "timestep:139, pyg_AUC: 0.4915\n",
      "timestep:140, pyg_AUC: 0.4887\n",
      "timestep:141, pyg_AUC: 0.4859\n",
      "timestep:142, pyg_AUC: 0.4831\n",
      "timestep:143, pyg_AUC: 0.4901\n",
      "timestep:144, pyg_AUC: 0.4929\n",
      "timestep:145, pyg_AUC: 0.4845\n",
      "timestep:146, pyg_AUC: 0.4915\n",
      "timestep:147, pyg_AUC: 0.4845\n",
      "timestep:148, pyg_AUC: 0.4887\n",
      "timestep:149, pyg_AUC: 0.4887\n",
      "timestep:150, pyg_AUC: 0.4873\n",
      "timestep:151, pyg_AUC: 0.4887\n",
      "timestep:152, pyg_AUC: 0.4845\n",
      "timestep:153, pyg_AUC: 0.4845\n",
      "timestep:154, pyg_AUC: 0.4887\n",
      "timestep:155, pyg_AUC: 0.4887\n",
      "timestep:156, pyg_AUC: 0.4859\n",
      "timestep:157, pyg_AUC: 0.4901\n",
      "timestep:158, pyg_AUC: 0.4887\n",
      "timestep:159, pyg_AUC: 0.4901\n",
      "timestep:160, pyg_AUC: 0.4901\n",
      "timestep:161, pyg_AUC: 0.4845\n",
      "timestep:162, pyg_AUC: 0.4873\n",
      "timestep:163, pyg_AUC: 0.4887\n",
      "timestep:164, pyg_AUC: 0.4901\n",
      "timestep:165, pyg_AUC: 0.4873\n",
      "timestep:166, pyg_AUC: 0.4958\n",
      "timestep:167, pyg_AUC: 0.4901\n",
      "timestep:168, pyg_AUC: 0.4901\n",
      "timestep:169, pyg_AUC: 0.4887\n",
      "timestep:170, pyg_AUC: 0.4887\n",
      "timestep:171, pyg_AUC: 0.4901\n",
      "timestep:172, pyg_AUC: 0.4915\n",
      "timestep:173, pyg_AUC: 0.4873\n",
      "timestep:174, pyg_AUC: 0.4859\n",
      "timestep:175, pyg_AUC: 0.4845\n",
      "timestep:176, pyg_AUC: 0.4887\n",
      "timestep:177, pyg_AUC: 0.4915\n",
      "timestep:178, pyg_AUC: 0.4915\n",
      "timestep:179, pyg_AUC: 0.4901\n",
      "timestep:180, pyg_AUC: 0.4859\n",
      "timestep:181, pyg_AUC: 0.4901\n",
      "timestep:182, pyg_AUC: 0.4831\n",
      "timestep:183, pyg_AUC: 0.4845\n",
      "timestep:184, pyg_AUC: 0.4831\n",
      "timestep:185, pyg_AUC: 0.4915\n",
      "timestep:186, pyg_AUC: 0.4859\n",
      "timestep:187, pyg_AUC: 0.4901\n",
      "timestep:188, pyg_AUC: 0.4901\n",
      "timestep:189, pyg_AUC: 0.4887\n",
      "timestep:190, pyg_AUC: 0.4873\n",
      "timestep:191, pyg_AUC: 0.4873\n",
      "timestep:192, pyg_AUC: 0.4859\n",
      "timestep:193, pyg_AUC: 0.4887\n",
      "timestep:194, pyg_AUC: 0.4873\n",
      "timestep:195, pyg_AUC: 0.4873\n",
      "timestep:196, pyg_AUC: 0.4901\n",
      "timestep:197, pyg_AUC: 0.4901\n",
      "timestep:198, pyg_AUC: 0.4915\n",
      "timestep:199, pyg_AUC: 0.4873\n",
      "timestep:200, pyg_AUC: 0.4901\n",
      "timestep:201, pyg_AUC: 0.4859\n",
      "timestep:202, pyg_AUC: 0.4873\n",
      "timestep:203, pyg_AUC: 0.4845\n",
      "timestep:204, pyg_AUC: 0.4915\n",
      "timestep:205, pyg_AUC: 0.4873\n",
      "timestep:206, pyg_AUC: 0.4873\n",
      "timestep:207, pyg_AUC: 0.4845\n",
      "timestep:208, pyg_AUC: 0.4901\n",
      "timestep:209, pyg_AUC: 0.4887\n",
      "timestep:210, pyg_AUC: 0.4873\n",
      "timestep:211, pyg_AUC: 0.4873\n",
      "timestep:212, pyg_AUC: 0.4887\n",
      "timestep:213, pyg_AUC: 0.4873\n",
      "timestep:214, pyg_AUC: 0.4887\n",
      "timestep:215, pyg_AUC: 0.4887\n",
      "timestep:216, pyg_AUC: 0.4901\n",
      "timestep:217, pyg_AUC: 0.4859\n",
      "timestep:218, pyg_AUC: 0.4873\n",
      "timestep:219, pyg_AUC: 0.4831\n",
      "timestep:220, pyg_AUC: 0.4873\n",
      "timestep:221, pyg_AUC: 0.4831\n",
      "timestep:222, pyg_AUC: 0.4887\n",
      "timestep:223, pyg_AUC: 0.4873\n",
      "timestep:224, pyg_AUC: 0.4887\n",
      "timestep:225, pyg_AUC: 0.4901\n",
      "timestep:226, pyg_AUC: 0.4873\n",
      "timestep:227, pyg_AUC: 0.4802\n",
      "timestep:228, pyg_AUC: 0.4873\n",
      "timestep:229, pyg_AUC: 0.4873\n",
      "timestep:230, pyg_AUC: 0.4901\n",
      "timestep:231, pyg_AUC: 0.4873\n",
      "timestep:232, pyg_AUC: 0.4831\n",
      "timestep:233, pyg_AUC: 0.4887\n",
      "timestep:234, pyg_AUC: 0.4831\n",
      "timestep:235, pyg_AUC: 0.4873\n",
      "timestep:236, pyg_AUC: 0.4901\n",
      "timestep:237, pyg_AUC: 0.4859\n",
      "timestep:238, pyg_AUC: 0.4845\n",
      "timestep:239, pyg_AUC: 0.4873\n",
      "timestep:240, pyg_AUC: 0.4873\n",
      "timestep:241, pyg_AUC: 0.4845\n",
      "timestep:242, pyg_AUC: 0.4859\n",
      "timestep:243, pyg_AUC: 0.4887\n",
      "timestep:244, pyg_AUC: 0.4887\n",
      "timestep:245, pyg_AUC: 0.4887\n",
      "timestep:246, pyg_AUC: 0.4845\n",
      "timestep:247, pyg_AUC: 0.4887\n",
      "timestep:248, pyg_AUC: 0.4859\n",
      "timestep:249, pyg_AUC: 0.4873\n",
      "timestep:250, pyg_AUC: 0.4873\n",
      "timestep:251, pyg_AUC: 0.4901\n",
      "timestep:252, pyg_AUC: 0.4887\n",
      "timestep:253, pyg_AUC: 0.4915\n",
      "timestep:254, pyg_AUC: 0.4859\n",
      "timestep:255, pyg_AUC: 0.4901\n",
      "timestep:256, pyg_AUC: 0.4859\n",
      "timestep:257, pyg_AUC: 0.4887\n",
      "timestep:258, pyg_AUC: 0.4859\n",
      "timestep:259, pyg_AUC: 0.4873\n",
      "timestep:260, pyg_AUC: 0.4873\n",
      "timestep:261, pyg_AUC: 0.4873\n",
      "timestep:262, pyg_AUC: 0.4859\n",
      "timestep:263, pyg_AUC: 0.4901\n",
      "timestep:264, pyg_AUC: 0.4887\n",
      "timestep:265, pyg_AUC: 0.4901\n",
      "timestep:266, pyg_AUC: 0.4901\n",
      "timestep:267, pyg_AUC: 0.4873\n",
      "timestep:268, pyg_AUC: 0.4887\n",
      "timestep:269, pyg_AUC: 0.4845\n",
      "timestep:270, pyg_AUC: 0.4873\n",
      "timestep:271, pyg_AUC: 0.4859\n",
      "timestep:272, pyg_AUC: 0.4859\n",
      "timestep:273, pyg_AUC: 0.4873\n",
      "timestep:274, pyg_AUC: 0.4873\n",
      "timestep:275, pyg_AUC: 0.4887\n",
      "timestep:276, pyg_AUC: 0.4887\n",
      "timestep:277, pyg_AUC: 0.4859\n",
      "timestep:278, pyg_AUC: 0.4859\n",
      "timestep:279, pyg_AUC: 0.4887\n",
      "timestep:280, pyg_AUC: 0.4887\n",
      "timestep:281, pyg_AUC: 0.4873\n",
      "timestep:282, pyg_AUC: 0.4873\n",
      "timestep:283, pyg_AUC: 0.4873\n",
      "timestep:284, pyg_AUC: 0.4887\n",
      "timestep:285, pyg_AUC: 0.4859\n",
      "timestep:286, pyg_AUC: 0.4873\n",
      "timestep:287, pyg_AUC: 0.4887\n",
      "timestep:288, pyg_AUC: 0.4845\n",
      "timestep:289, pyg_AUC: 0.4915\n",
      "timestep:290, pyg_AUC: 0.4845\n",
      "timestep:291, pyg_AUC: 0.4873\n",
      "timestep:292, pyg_AUC: 0.4887\n",
      "timestep:293, pyg_AUC: 0.4859\n",
      "timestep:294, pyg_AUC: 0.4859\n",
      "timestep:295, pyg_AUC: 0.4887\n",
      "timestep:296, pyg_AUC: 0.4887\n",
      "timestep:297, pyg_AUC: 0.4873\n",
      "timestep:298, pyg_AUC: 0.4873\n",
      "timestep:299, pyg_AUC: 0.4887\n",
      "timestep:300, pyg_AUC: 0.4887\n",
      "timestep:301, pyg_AUC: 0.4915\n",
      "timestep:302, pyg_AUC: 0.4873\n",
      "timestep:303, pyg_AUC: 0.4873\n",
      "timestep:304, pyg_AUC: 0.4887\n",
      "timestep:305, pyg_AUC: 0.4887\n",
      "timestep:306, pyg_AUC: 0.4845\n",
      "timestep:307, pyg_AUC: 0.4845\n",
      "timestep:308, pyg_AUC: 0.4887\n",
      "timestep:309, pyg_AUC: 0.4915\n",
      "timestep:310, pyg_AUC: 0.4873\n",
      "timestep:311, pyg_AUC: 0.4929\n",
      "timestep:312, pyg_AUC: 0.4859\n",
      "timestep:313, pyg_AUC: 0.4887\n",
      "timestep:314, pyg_AUC: 0.4845\n",
      "timestep:315, pyg_AUC: 0.4873\n",
      "timestep:316, pyg_AUC: 0.4915\n",
      "timestep:317, pyg_AUC: 0.4859\n",
      "timestep:318, pyg_AUC: 0.4859\n",
      "timestep:319, pyg_AUC: 0.4901\n",
      "timestep:320, pyg_AUC: 0.4887\n",
      "timestep:321, pyg_AUC: 0.4887\n",
      "timestep:322, pyg_AUC: 0.4887\n",
      "timestep:323, pyg_AUC: 0.4901\n",
      "timestep:324, pyg_AUC: 0.4859\n",
      "timestep:325, pyg_AUC: 0.4873\n",
      "timestep:326, pyg_AUC: 0.4859\n",
      "timestep:327, pyg_AUC: 0.4901\n",
      "timestep:328, pyg_AUC: 0.4859\n",
      "timestep:329, pyg_AUC: 0.4901\n",
      "timestep:330, pyg_AUC: 0.4887\n",
      "timestep:331, pyg_AUC: 0.4887\n",
      "timestep:332, pyg_AUC: 0.4915\n",
      "timestep:333, pyg_AUC: 0.4901\n",
      "timestep:334, pyg_AUC: 0.4901\n",
      "timestep:335, pyg_AUC: 0.4873\n",
      "timestep:336, pyg_AUC: 0.4873\n",
      "timestep:337, pyg_AUC: 0.4887\n",
      "timestep:338, pyg_AUC: 0.4887\n",
      "timestep:339, pyg_AUC: 0.4873\n",
      "timestep:340, pyg_AUC: 0.4887\n",
      "timestep:341, pyg_AUC: 0.4873\n",
      "timestep:342, pyg_AUC: 0.4901\n",
      "timestep:343, pyg_AUC: 0.4816\n",
      "timestep:344, pyg_AUC: 0.4873\n",
      "timestep:345, pyg_AUC: 0.4859\n",
      "timestep:346, pyg_AUC: 0.4873\n",
      "timestep:347, pyg_AUC: 0.4915\n",
      "timestep:348, pyg_AUC: 0.4915\n",
      "timestep:349, pyg_AUC: 0.4915\n",
      "timestep:350, pyg_AUC: 0.4901\n",
      "timestep:351, pyg_AUC: 0.4887\n",
      "timestep:352, pyg_AUC: 0.4887\n",
      "timestep:353, pyg_AUC: 0.4901\n",
      "timestep:354, pyg_AUC: 0.4901\n",
      "timestep:355, pyg_AUC: 0.4873\n",
      "timestep:356, pyg_AUC: 0.4915\n",
      "timestep:357, pyg_AUC: 0.4887\n",
      "timestep:358, pyg_AUC: 0.4859\n",
      "timestep:359, pyg_AUC: 0.4859\n",
      "timestep:360, pyg_AUC: 0.4901\n",
      "timestep:361, pyg_AUC: 0.4816\n",
      "timestep:362, pyg_AUC: 0.4845\n",
      "timestep:363, pyg_AUC: 0.4859\n",
      "timestep:364, pyg_AUC: 0.4845\n",
      "timestep:365, pyg_AUC: 0.4887\n",
      "timestep:366, pyg_AUC: 0.4887\n",
      "timestep:367, pyg_AUC: 0.4859\n",
      "timestep:368, pyg_AUC: 0.4887\n",
      "timestep:369, pyg_AUC: 0.4915\n",
      "timestep:370, pyg_AUC: 0.4816\n",
      "timestep:371, pyg_AUC: 0.4873\n",
      "timestep:372, pyg_AUC: 0.4901\n",
      "timestep:373, pyg_AUC: 0.4859\n",
      "timestep:374, pyg_AUC: 0.4887\n",
      "timestep:375, pyg_AUC: 0.4887\n",
      "timestep:376, pyg_AUC: 0.4901\n",
      "timestep:377, pyg_AUC: 0.4887\n",
      "timestep:378, pyg_AUC: 0.4873\n",
      "timestep:379, pyg_AUC: 0.4915\n",
      "timestep:380, pyg_AUC: 0.4887\n",
      "timestep:381, pyg_AUC: 0.4915\n",
      "timestep:382, pyg_AUC: 0.4915\n",
      "timestep:383, pyg_AUC: 0.4901\n",
      "timestep:384, pyg_AUC: 0.4873\n",
      "timestep:385, pyg_AUC: 0.4901\n",
      "timestep:386, pyg_AUC: 0.4901\n",
      "timestep:387, pyg_AUC: 0.4901\n",
      "timestep:388, pyg_AUC: 0.4887\n",
      "timestep:389, pyg_AUC: 0.4831\n",
      "timestep:390, pyg_AUC: 0.4887\n",
      "timestep:391, pyg_AUC: 0.4887\n",
      "timestep:392, pyg_AUC: 0.4873\n",
      "timestep:393, pyg_AUC: 0.4901\n",
      "timestep:394, pyg_AUC: 0.4873\n",
      "timestep:395, pyg_AUC: 0.4901\n",
      "timestep:396, pyg_AUC: 0.4887\n",
      "timestep:397, pyg_AUC: 0.4901\n",
      "timestep:398, pyg_AUC: 0.4859\n",
      "timestep:399, pyg_AUC: 0.4901\n",
      "timestep:400, pyg_AUC: 0.4845\n",
      "timestep:401, pyg_AUC: 0.4873\n",
      "timestep:402, pyg_AUC: 0.4887\n",
      "timestep:403, pyg_AUC: 0.4901\n",
      "timestep:404, pyg_AUC: 0.4901\n",
      "timestep:405, pyg_AUC: 0.4873\n",
      "timestep:406, pyg_AUC: 0.4901\n",
      "timestep:407, pyg_AUC: 0.4845\n",
      "timestep:408, pyg_AUC: 0.4859\n",
      "timestep:409, pyg_AUC: 0.4859\n",
      "timestep:410, pyg_AUC: 0.4859\n",
      "timestep:411, pyg_AUC: 0.4845\n",
      "timestep:412, pyg_AUC: 0.4816\n",
      "timestep:413, pyg_AUC: 0.4887\n",
      "timestep:414, pyg_AUC: 0.4831\n",
      "timestep:415, pyg_AUC: 0.4901\n",
      "timestep:416, pyg_AUC: 0.4845\n",
      "timestep:417, pyg_AUC: 0.4873\n",
      "timestep:418, pyg_AUC: 0.4859\n",
      "timestep:419, pyg_AUC: 0.4901\n",
      "timestep:420, pyg_AUC: 0.4929\n",
      "timestep:421, pyg_AUC: 0.4887\n",
      "timestep:422, pyg_AUC: 0.4873\n",
      "timestep:423, pyg_AUC: 0.4831\n",
      "timestep:424, pyg_AUC: 0.4915\n",
      "timestep:425, pyg_AUC: 0.4873\n",
      "timestep:426, pyg_AUC: 0.4915\n",
      "timestep:427, pyg_AUC: 0.4929\n",
      "timestep:428, pyg_AUC: 0.4887\n",
      "timestep:429, pyg_AUC: 0.4915\n",
      "timestep:430, pyg_AUC: 0.4887\n",
      "timestep:431, pyg_AUC: 0.4887\n",
      "timestep:432, pyg_AUC: 0.4887\n",
      "timestep:433, pyg_AUC: 0.4845\n",
      "timestep:434, pyg_AUC: 0.4859\n",
      "timestep:435, pyg_AUC: 0.4831\n",
      "timestep:436, pyg_AUC: 0.4915\n",
      "timestep:437, pyg_AUC: 0.4845\n",
      "timestep:438, pyg_AUC: 0.4915\n",
      "timestep:439, pyg_AUC: 0.4873\n",
      "timestep:440, pyg_AUC: 0.4873\n",
      "timestep:441, pyg_AUC: 0.4859\n",
      "timestep:442, pyg_AUC: 0.4845\n",
      "timestep:443, pyg_AUC: 0.4845\n",
      "timestep:444, pyg_AUC: 0.4915\n",
      "timestep:445, pyg_AUC: 0.4901\n",
      "timestep:446, pyg_AUC: 0.4944\n",
      "timestep:447, pyg_AUC: 0.4901\n",
      "timestep:448, pyg_AUC: 0.4845\n",
      "timestep:449, pyg_AUC: 0.4901\n",
      "timestep:450, pyg_AUC: 0.4873\n",
      "timestep:451, pyg_AUC: 0.4859\n",
      "timestep:452, pyg_AUC: 0.4831\n",
      "timestep:453, pyg_AUC: 0.4929\n",
      "timestep:454, pyg_AUC: 0.4859\n",
      "timestep:455, pyg_AUC: 0.4859\n",
      "timestep:456, pyg_AUC: 0.4901\n",
      "timestep:457, pyg_AUC: 0.4845\n",
      "timestep:458, pyg_AUC: 0.4887\n",
      "timestep:459, pyg_AUC: 0.4887\n",
      "timestep:460, pyg_AUC: 0.4859\n",
      "timestep:461, pyg_AUC: 0.4915\n",
      "timestep:462, pyg_AUC: 0.4887\n",
      "timestep:463, pyg_AUC: 0.4873\n",
      "timestep:464, pyg_AUC: 0.4915\n",
      "timestep:465, pyg_AUC: 0.4859\n",
      "timestep:466, pyg_AUC: 0.4859\n",
      "timestep:467, pyg_AUC: 0.4873\n",
      "timestep:468, pyg_AUC: 0.4831\n",
      "timestep:469, pyg_AUC: 0.4901\n",
      "timestep:470, pyg_AUC: 0.4915\n",
      "timestep:471, pyg_AUC: 0.4859\n",
      "timestep:472, pyg_AUC: 0.4859\n",
      "timestep:473, pyg_AUC: 0.4887\n",
      "timestep:474, pyg_AUC: 0.4901\n",
      "timestep:475, pyg_AUC: 0.4859\n",
      "timestep:476, pyg_AUC: 0.4859\n",
      "timestep:477, pyg_AUC: 0.4887\n",
      "timestep:478, pyg_AUC: 0.4901\n",
      "timestep:479, pyg_AUC: 0.4873\n",
      "timestep:480, pyg_AUC: 0.4873\n",
      "timestep:481, pyg_AUC: 0.4887\n",
      "timestep:482, pyg_AUC: 0.4887\n",
      "timestep:483, pyg_AUC: 0.4873\n",
      "timestep:484, pyg_AUC: 0.4859\n",
      "timestep:485, pyg_AUC: 0.4887\n",
      "timestep:486, pyg_AUC: 0.4845\n",
      "timestep:487, pyg_AUC: 0.4816\n",
      "timestep:488, pyg_AUC: 0.4901\n",
      "timestep:489, pyg_AUC: 0.4873\n",
      "timestep:490, pyg_AUC: 0.4845\n",
      "timestep:491, pyg_AUC: 0.4831\n",
      "timestep:492, pyg_AUC: 0.4887\n",
      "timestep:493, pyg_AUC: 0.4859\n",
      "timestep:494, pyg_AUC: 0.4816\n",
      "timestep:495, pyg_AUC: 0.4901\n",
      "timestep:496, pyg_AUC: 0.4845\n",
      "timestep:497, pyg_AUC: 0.4873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [16:46<16:52, 101.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:498, pyg_AUC: 0.4873\n",
      "timestep:499, pyg_AUC: 0.4859\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 41.35893\n",
      "Epoch: 0010 loss= 24.60151\n",
      "Epoch: 0020 loss= 18.33657\n",
      "Epoch: 0030 loss= 16.52004\n",
      "Epoch: 0040 loss= 13.95027\n",
      "Epoch: 0050 loss= 5.52830\n",
      "Epoch: 0060 loss= 1.37850\n",
      "Epoch: 0070 loss= 0.81585\n",
      "Epoch: 0080 loss= 0.75493\n",
      "Epoch: 0090 loss= 0.77594\n",
      "Epoch: 0100 loss= 0.68952\n",
      "Epoch: 0110 loss= 0.63322\n",
      "Epoch: 0120 loss= 0.62033\n",
      "Epoch: 0130 loss= 0.60450\n",
      "Epoch: 0140 loss= 0.63107\n",
      "Epoch: 0150 loss= 0.50966\n",
      "Epoch: 0160 loss= 0.56602\n",
      "Epoch: 0170 loss= 0.61068\n",
      "Epoch: 0180 loss= 0.66395\n",
      "Epoch: 0190 loss= 0.58228\n",
      "Epoch: 0200 loss= 0.61942\n",
      "Epoch: 0210 loss= 0.54799\n",
      "Epoch: 0220 loss= 0.56241\n",
      "Epoch: 0230 loss= 0.50945\n",
      "Epoch: 0240 loss= 0.60310\n",
      "Epoch: 0250 loss= 0.65515\n",
      "Epoch: 0260 loss= 0.61181\n",
      "Epoch: 0270 loss= 0.57547\n",
      "Epoch: 0280 loss= 0.50513\n",
      "Epoch: 0290 loss= 0.51688\n",
      "Epoch: 0300 loss= 0.55942\n",
      "Epoch: 0310 loss= 0.68189\n",
      "Epoch: 0320 loss= 0.54025\n",
      "Epoch: 0330 loss= 0.54704\n",
      "Epoch: 0340 loss= 0.55064\n",
      "Epoch: 0350 loss= 0.52639\n",
      "Epoch: 0360 loss= 0.52319\n",
      "Epoch: 0370 loss= 0.55905\n",
      "Epoch: 0380 loss= 0.51637\n",
      "Epoch: 0390 loss= 0.57848\n",
      "Epoch: 0400 loss= 0.54539\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7992,  4.7097, -5.3460, -4.6135, -4.3883,  5.6580,  5.4867, -5.2439]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 35.14780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss= 36.01245\n",
      "Epoch: 0020 loss= 22.90716\n",
      "Epoch: 0030 loss= 16.17031\n",
      "Epoch: 0040 loss= 7.57759\n",
      "Epoch: 0050 loss= 2.18238\n",
      "Epoch: 0060 loss= 0.72347\n",
      "Epoch: 0070 loss= 0.68237\n",
      "Epoch: 0080 loss= 1.38942\n",
      "Epoch: 0090 loss= 0.68676\n",
      "Epoch: 0100 loss= 1.04547\n",
      "Epoch: 0110 loss= 0.79572\n",
      "Epoch: 0120 loss= 0.62468\n",
      "Epoch: 0130 loss= 0.58597\n",
      "Epoch: 0140 loss= 0.57311\n",
      "Epoch: 0150 loss= 0.58915\n",
      "Epoch: 0160 loss= 0.60219\n",
      "Epoch: 0170 loss= 0.58091\n",
      "Epoch: 0180 loss= 0.67272\n",
      "Epoch: 0190 loss= 0.56651\n",
      "Epoch: 0200 loss= 0.60999\n",
      "Epoch: 0210 loss= 0.55475\n",
      "Epoch: 0220 loss= 0.65925\n",
      "Epoch: 0230 loss= 0.53609\n",
      "Epoch: 0240 loss= 0.57257\n",
      "Epoch: 0250 loss= 0.62324\n",
      "Epoch: 0260 loss= 0.55636\n",
      "Epoch: 0270 loss= 0.64556\n",
      "Epoch: 0280 loss= 0.50953\n",
      "Epoch: 0290 loss= 0.62862\n",
      "Epoch: 0300 loss= 0.57529\n",
      "Epoch: 0310 loss= 0.54115\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4887\n",
      "timestep:1, pyg_AUC: 0.4929\n",
      "timestep:2, pyg_AUC: 0.4929\n",
      "timestep:3, pyg_AUC: 0.4901\n",
      "timestep:4, pyg_AUC: 0.4901\n",
      "timestep:5, pyg_AUC: 0.4915\n",
      "timestep:6, pyg_AUC: 0.4929\n",
      "timestep:7, pyg_AUC: 0.4873\n",
      "timestep:8, pyg_AUC: 0.4901\n",
      "timestep:9, pyg_AUC: 0.4929\n",
      "timestep:10, pyg_AUC: 0.4859\n",
      "timestep:11, pyg_AUC: 0.4873\n",
      "timestep:12, pyg_AUC: 0.4901\n",
      "timestep:13, pyg_AUC: 0.4887\n",
      "timestep:14, pyg_AUC: 0.4901\n",
      "timestep:15, pyg_AUC: 0.4915\n",
      "timestep:16, pyg_AUC: 0.4915\n",
      "timestep:17, pyg_AUC: 0.4929\n",
      "timestep:18, pyg_AUC: 0.4929\n",
      "timestep:19, pyg_AUC: 0.4901\n",
      "timestep:20, pyg_AUC: 0.4901\n",
      "timestep:21, pyg_AUC: 0.4915\n",
      "timestep:22, pyg_AUC: 0.4901\n",
      "timestep:23, pyg_AUC: 0.4901\n",
      "timestep:24, pyg_AUC: 0.4873\n",
      "timestep:25, pyg_AUC: 0.4873\n",
      "timestep:26, pyg_AUC: 0.4929\n",
      "timestep:27, pyg_AUC: 0.4831\n",
      "timestep:28, pyg_AUC: 0.4929\n",
      "timestep:29, pyg_AUC: 0.4901\n",
      "timestep:30, pyg_AUC: 0.4929\n",
      "timestep:31, pyg_AUC: 0.4887\n",
      "timestep:32, pyg_AUC: 0.4887\n",
      "timestep:33, pyg_AUC: 0.4901\n",
      "timestep:34, pyg_AUC: 0.4901\n",
      "timestep:35, pyg_AUC: 0.4901\n",
      "timestep:36, pyg_AUC: 0.4915\n",
      "timestep:37, pyg_AUC: 0.4901\n",
      "timestep:38, pyg_AUC: 0.4859\n",
      "timestep:39, pyg_AUC: 0.4929\n",
      "timestep:40, pyg_AUC: 0.4845\n",
      "timestep:41, pyg_AUC: 0.4915\n",
      "timestep:42, pyg_AUC: 0.4944\n",
      "timestep:43, pyg_AUC: 0.4887\n",
      "timestep:44, pyg_AUC: 0.4929\n",
      "timestep:45, pyg_AUC: 0.4944\n",
      "timestep:46, pyg_AUC: 0.4887\n",
      "timestep:47, pyg_AUC: 0.4929\n",
      "timestep:48, pyg_AUC: 0.4929\n",
      "timestep:49, pyg_AUC: 0.4831\n",
      "timestep:50, pyg_AUC: 0.4915\n",
      "timestep:51, pyg_AUC: 0.4887\n",
      "timestep:52, pyg_AUC: 0.4929\n",
      "timestep:53, pyg_AUC: 0.4887\n",
      "timestep:54, pyg_AUC: 0.4845\n",
      "timestep:55, pyg_AUC: 0.4831\n",
      "timestep:56, pyg_AUC: 0.4873\n",
      "timestep:57, pyg_AUC: 0.4901\n",
      "timestep:58, pyg_AUC: 0.4859\n",
      "timestep:59, pyg_AUC: 0.4887\n",
      "timestep:60, pyg_AUC: 0.4887\n",
      "timestep:61, pyg_AUC: 0.4929\n",
      "timestep:62, pyg_AUC: 0.4915\n",
      "timestep:63, pyg_AUC: 0.4873\n",
      "timestep:64, pyg_AUC: 0.4944\n",
      "timestep:65, pyg_AUC: 0.4887\n",
      "timestep:66, pyg_AUC: 0.4887\n",
      "timestep:67, pyg_AUC: 0.4915\n",
      "timestep:68, pyg_AUC: 0.4887\n",
      "timestep:69, pyg_AUC: 0.4901\n",
      "timestep:70, pyg_AUC: 0.4915\n",
      "timestep:71, pyg_AUC: 0.4915\n",
      "timestep:72, pyg_AUC: 0.4887\n",
      "timestep:73, pyg_AUC: 0.4873\n",
      "timestep:74, pyg_AUC: 0.4831\n",
      "timestep:75, pyg_AUC: 0.4873\n",
      "timestep:76, pyg_AUC: 0.4887\n",
      "timestep:77, pyg_AUC: 0.4901\n",
      "timestep:78, pyg_AUC: 0.4845\n",
      "timestep:79, pyg_AUC: 0.4887\n",
      "timestep:80, pyg_AUC: 0.4929\n",
      "timestep:81, pyg_AUC: 0.4887\n",
      "timestep:82, pyg_AUC: 0.4901\n",
      "timestep:83, pyg_AUC: 0.4887\n",
      "timestep:84, pyg_AUC: 0.4887\n",
      "timestep:85, pyg_AUC: 0.4929\n",
      "timestep:86, pyg_AUC: 0.4845\n",
      "timestep:87, pyg_AUC: 0.4873\n",
      "timestep:88, pyg_AUC: 0.4915\n",
      "timestep:89, pyg_AUC: 0.4901\n",
      "timestep:90, pyg_AUC: 0.4915\n",
      "timestep:91, pyg_AUC: 0.4887\n",
      "timestep:92, pyg_AUC: 0.4873\n",
      "timestep:93, pyg_AUC: 0.4915\n",
      "timestep:94, pyg_AUC: 0.4901\n",
      "timestep:95, pyg_AUC: 0.4901\n",
      "timestep:96, pyg_AUC: 0.4845\n",
      "timestep:97, pyg_AUC: 0.4901\n",
      "timestep:98, pyg_AUC: 0.4887\n",
      "timestep:99, pyg_AUC: 0.4873\n",
      "timestep:100, pyg_AUC: 0.4887\n",
      "timestep:101, pyg_AUC: 0.4887\n",
      "timestep:102, pyg_AUC: 0.4915\n",
      "timestep:103, pyg_AUC: 0.4887\n",
      "timestep:104, pyg_AUC: 0.4901\n",
      "timestep:105, pyg_AUC: 0.4859\n",
      "timestep:106, pyg_AUC: 0.4887\n",
      "timestep:107, pyg_AUC: 0.4915\n",
      "timestep:108, pyg_AUC: 0.4845\n",
      "timestep:109, pyg_AUC: 0.4915\n",
      "timestep:110, pyg_AUC: 0.4859\n",
      "timestep:111, pyg_AUC: 0.4887\n",
      "timestep:112, pyg_AUC: 0.4859\n",
      "timestep:113, pyg_AUC: 0.4887\n",
      "timestep:114, pyg_AUC: 0.4887\n",
      "timestep:115, pyg_AUC: 0.4859\n",
      "timestep:116, pyg_AUC: 0.4873\n",
      "timestep:117, pyg_AUC: 0.4901\n",
      "timestep:118, pyg_AUC: 0.4859\n",
      "timestep:119, pyg_AUC: 0.4802\n",
      "timestep:120, pyg_AUC: 0.4901\n",
      "timestep:121, pyg_AUC: 0.4831\n",
      "timestep:122, pyg_AUC: 0.4845\n",
      "timestep:123, pyg_AUC: 0.4845\n",
      "timestep:124, pyg_AUC: 0.4915\n",
      "timestep:125, pyg_AUC: 0.4831\n",
      "timestep:126, pyg_AUC: 0.4901\n",
      "timestep:127, pyg_AUC: 0.4873\n",
      "timestep:128, pyg_AUC: 0.4859\n",
      "timestep:129, pyg_AUC: 0.4845\n",
      "timestep:130, pyg_AUC: 0.4901\n",
      "timestep:131, pyg_AUC: 0.4859\n",
      "timestep:132, pyg_AUC: 0.4887\n",
      "timestep:133, pyg_AUC: 0.4873\n",
      "timestep:134, pyg_AUC: 0.4873\n",
      "timestep:135, pyg_AUC: 0.4887\n",
      "timestep:136, pyg_AUC: 0.4887\n",
      "timestep:137, pyg_AUC: 0.4887\n",
      "timestep:138, pyg_AUC: 0.4887\n",
      "timestep:139, pyg_AUC: 0.4831\n",
      "timestep:140, pyg_AUC: 0.4873\n",
      "timestep:141, pyg_AUC: 0.4831\n",
      "timestep:142, pyg_AUC: 0.4887\n",
      "timestep:143, pyg_AUC: 0.4887\n",
      "timestep:144, pyg_AUC: 0.4873\n",
      "timestep:145, pyg_AUC: 0.4873\n",
      "timestep:146, pyg_AUC: 0.4901\n",
      "timestep:147, pyg_AUC: 0.4831\n",
      "timestep:148, pyg_AUC: 0.4845\n",
      "timestep:149, pyg_AUC: 0.4802\n",
      "timestep:150, pyg_AUC: 0.4887\n",
      "timestep:151, pyg_AUC: 0.4873\n",
      "timestep:152, pyg_AUC: 0.4887\n",
      "timestep:153, pyg_AUC: 0.4873\n",
      "timestep:154, pyg_AUC: 0.4845\n",
      "timestep:155, pyg_AUC: 0.4845\n",
      "timestep:156, pyg_AUC: 0.4901\n",
      "timestep:157, pyg_AUC: 0.4873\n",
      "timestep:158, pyg_AUC: 0.4901\n",
      "timestep:159, pyg_AUC: 0.4859\n",
      "timestep:160, pyg_AUC: 0.4816\n",
      "timestep:161, pyg_AUC: 0.4859\n",
      "timestep:162, pyg_AUC: 0.4859\n",
      "timestep:163, pyg_AUC: 0.4887\n",
      "timestep:164, pyg_AUC: 0.4887\n",
      "timestep:165, pyg_AUC: 0.4873\n",
      "timestep:166, pyg_AUC: 0.4887\n",
      "timestep:167, pyg_AUC: 0.4845\n",
      "timestep:168, pyg_AUC: 0.4859\n",
      "timestep:169, pyg_AUC: 0.4873\n",
      "timestep:170, pyg_AUC: 0.4915\n",
      "timestep:171, pyg_AUC: 0.4845\n",
      "timestep:172, pyg_AUC: 0.4845\n",
      "timestep:173, pyg_AUC: 0.4887\n",
      "timestep:174, pyg_AUC: 0.4873\n",
      "timestep:175, pyg_AUC: 0.4873\n",
      "timestep:176, pyg_AUC: 0.4887\n",
      "timestep:177, pyg_AUC: 0.4831\n",
      "timestep:178, pyg_AUC: 0.4915\n",
      "timestep:179, pyg_AUC: 0.4887\n",
      "timestep:180, pyg_AUC: 0.4859\n",
      "timestep:181, pyg_AUC: 0.4859\n",
      "timestep:182, pyg_AUC: 0.4831\n",
      "timestep:183, pyg_AUC: 0.4845\n",
      "timestep:184, pyg_AUC: 0.4873\n",
      "timestep:185, pyg_AUC: 0.4873\n",
      "timestep:186, pyg_AUC: 0.4845\n",
      "timestep:187, pyg_AUC: 0.4859\n",
      "timestep:188, pyg_AUC: 0.4901\n",
      "timestep:189, pyg_AUC: 0.4887\n",
      "timestep:190, pyg_AUC: 0.4887\n",
      "timestep:191, pyg_AUC: 0.4873\n",
      "timestep:192, pyg_AUC: 0.4845\n",
      "timestep:193, pyg_AUC: 0.4887\n",
      "timestep:194, pyg_AUC: 0.4816\n",
      "timestep:195, pyg_AUC: 0.4887\n",
      "timestep:196, pyg_AUC: 0.4887\n",
      "timestep:197, pyg_AUC: 0.4901\n",
      "timestep:198, pyg_AUC: 0.4887\n",
      "timestep:199, pyg_AUC: 0.4845\n",
      "timestep:200, pyg_AUC: 0.4915\n",
      "timestep:201, pyg_AUC: 0.4901\n",
      "timestep:202, pyg_AUC: 0.4873\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4859\n",
      "timestep:205, pyg_AUC: 0.4831\n",
      "timestep:206, pyg_AUC: 0.4873\n",
      "timestep:207, pyg_AUC: 0.4873\n",
      "timestep:208, pyg_AUC: 0.4901\n",
      "timestep:209, pyg_AUC: 0.4887\n",
      "timestep:210, pyg_AUC: 0.4887\n",
      "timestep:211, pyg_AUC: 0.4859\n",
      "timestep:212, pyg_AUC: 0.4845\n",
      "timestep:213, pyg_AUC: 0.4887\n",
      "timestep:214, pyg_AUC: 0.4845\n",
      "timestep:215, pyg_AUC: 0.4887\n",
      "timestep:216, pyg_AUC: 0.4831\n",
      "timestep:217, pyg_AUC: 0.4859\n",
      "timestep:218, pyg_AUC: 0.4859\n",
      "timestep:219, pyg_AUC: 0.4887\n",
      "timestep:220, pyg_AUC: 0.4859\n",
      "timestep:221, pyg_AUC: 0.4887\n",
      "timestep:222, pyg_AUC: 0.4859\n",
      "timestep:223, pyg_AUC: 0.4859\n",
      "timestep:224, pyg_AUC: 0.4859\n",
      "timestep:225, pyg_AUC: 0.4873\n",
      "timestep:226, pyg_AUC: 0.4887\n",
      "timestep:227, pyg_AUC: 0.4873\n",
      "timestep:228, pyg_AUC: 0.4859\n",
      "timestep:229, pyg_AUC: 0.4859\n",
      "timestep:230, pyg_AUC: 0.4845\n",
      "timestep:231, pyg_AUC: 0.4873\n",
      "timestep:232, pyg_AUC: 0.4831\n",
      "timestep:233, pyg_AUC: 0.4887\n",
      "timestep:234, pyg_AUC: 0.4845\n",
      "timestep:235, pyg_AUC: 0.4831\n",
      "timestep:236, pyg_AUC: 0.4873\n",
      "timestep:237, pyg_AUC: 0.4845\n",
      "timestep:238, pyg_AUC: 0.4845\n",
      "timestep:239, pyg_AUC: 0.4845\n",
      "timestep:240, pyg_AUC: 0.4859\n",
      "timestep:241, pyg_AUC: 0.4831\n",
      "timestep:242, pyg_AUC: 0.4887\n",
      "timestep:243, pyg_AUC: 0.4887\n",
      "timestep:244, pyg_AUC: 0.4845\n",
      "timestep:245, pyg_AUC: 0.4887\n",
      "timestep:246, pyg_AUC: 0.4873\n",
      "timestep:247, pyg_AUC: 0.4859\n",
      "timestep:248, pyg_AUC: 0.4816\n",
      "timestep:249, pyg_AUC: 0.4873\n",
      "timestep:250, pyg_AUC: 0.4845\n",
      "timestep:251, pyg_AUC: 0.4873\n",
      "timestep:252, pyg_AUC: 0.4831\n",
      "timestep:253, pyg_AUC: 0.4873\n",
      "timestep:254, pyg_AUC: 0.4831\n",
      "timestep:255, pyg_AUC: 0.4845\n",
      "timestep:256, pyg_AUC: 0.4873\n",
      "timestep:257, pyg_AUC: 0.4845\n",
      "timestep:258, pyg_AUC: 0.4831\n",
      "timestep:259, pyg_AUC: 0.4845\n",
      "timestep:260, pyg_AUC: 0.4859\n",
      "timestep:261, pyg_AUC: 0.4887\n",
      "timestep:262, pyg_AUC: 0.4859\n",
      "timestep:263, pyg_AUC: 0.4802\n",
      "timestep:264, pyg_AUC: 0.4845\n",
      "timestep:265, pyg_AUC: 0.4845\n",
      "timestep:266, pyg_AUC: 0.4845\n",
      "timestep:267, pyg_AUC: 0.4859\n",
      "timestep:268, pyg_AUC: 0.4831\n",
      "timestep:269, pyg_AUC: 0.4859\n",
      "timestep:270, pyg_AUC: 0.4831\n",
      "timestep:271, pyg_AUC: 0.4873\n",
      "timestep:272, pyg_AUC: 0.4873\n",
      "timestep:273, pyg_AUC: 0.4873\n",
      "timestep:274, pyg_AUC: 0.4859\n",
      "timestep:275, pyg_AUC: 0.4873\n",
      "timestep:276, pyg_AUC: 0.4831\n",
      "timestep:277, pyg_AUC: 0.4873\n",
      "timestep:278, pyg_AUC: 0.4845\n",
      "timestep:279, pyg_AUC: 0.4859\n",
      "timestep:280, pyg_AUC: 0.4873\n",
      "timestep:281, pyg_AUC: 0.4873\n",
      "timestep:282, pyg_AUC: 0.4887\n",
      "timestep:283, pyg_AUC: 0.4845\n",
      "timestep:284, pyg_AUC: 0.4831\n",
      "timestep:285, pyg_AUC: 0.4887\n",
      "timestep:286, pyg_AUC: 0.4859\n",
      "timestep:287, pyg_AUC: 0.4859\n",
      "timestep:288, pyg_AUC: 0.4845\n",
      "timestep:289, pyg_AUC: 0.4845\n",
      "timestep:290, pyg_AUC: 0.4859\n",
      "timestep:291, pyg_AUC: 0.4887\n",
      "timestep:292, pyg_AUC: 0.4831\n",
      "timestep:293, pyg_AUC: 0.4845\n",
      "timestep:294, pyg_AUC: 0.4831\n",
      "timestep:295, pyg_AUC: 0.4873\n",
      "timestep:296, pyg_AUC: 0.4816\n",
      "timestep:297, pyg_AUC: 0.4873\n",
      "timestep:298, pyg_AUC: 0.4873\n",
      "timestep:299, pyg_AUC: 0.4859\n",
      "timestep:300, pyg_AUC: 0.4845\n",
      "timestep:301, pyg_AUC: 0.4845\n",
      "timestep:302, pyg_AUC: 0.4845\n",
      "timestep:303, pyg_AUC: 0.4831\n",
      "timestep:304, pyg_AUC: 0.4859\n",
      "timestep:305, pyg_AUC: 0.4845\n",
      "timestep:306, pyg_AUC: 0.4887\n",
      "timestep:307, pyg_AUC: 0.4859\n",
      "timestep:308, pyg_AUC: 0.4845\n",
      "timestep:309, pyg_AUC: 0.4831\n",
      "timestep:310, pyg_AUC: 0.4873\n",
      "timestep:311, pyg_AUC: 0.4887\n",
      "timestep:312, pyg_AUC: 0.4845\n",
      "timestep:313, pyg_AUC: 0.4831\n",
      "timestep:314, pyg_AUC: 0.4901\n",
      "timestep:315, pyg_AUC: 0.4831\n",
      "timestep:316, pyg_AUC: 0.4816\n",
      "timestep:317, pyg_AUC: 0.4887\n",
      "timestep:318, pyg_AUC: 0.4873\n",
      "timestep:319, pyg_AUC: 0.4845\n",
      "timestep:320, pyg_AUC: 0.4816\n",
      "timestep:321, pyg_AUC: 0.4845\n",
      "timestep:322, pyg_AUC: 0.4873\n",
      "timestep:323, pyg_AUC: 0.4859\n",
      "timestep:324, pyg_AUC: 0.4831\n",
      "timestep:325, pyg_AUC: 0.4859\n",
      "timestep:326, pyg_AUC: 0.4845\n",
      "timestep:327, pyg_AUC: 0.4845\n",
      "timestep:328, pyg_AUC: 0.4887\n",
      "timestep:329, pyg_AUC: 0.4859\n",
      "timestep:330, pyg_AUC: 0.4859\n",
      "timestep:331, pyg_AUC: 0.4859\n",
      "timestep:332, pyg_AUC: 0.4887\n",
      "timestep:333, pyg_AUC: 0.4845\n",
      "timestep:334, pyg_AUC: 0.4873\n",
      "timestep:335, pyg_AUC: 0.4859\n",
      "timestep:336, pyg_AUC: 0.4845\n",
      "timestep:337, pyg_AUC: 0.4873\n",
      "timestep:338, pyg_AUC: 0.4859\n",
      "timestep:339, pyg_AUC: 0.4831\n",
      "timestep:340, pyg_AUC: 0.4873\n",
      "timestep:341, pyg_AUC: 0.4816\n",
      "timestep:342, pyg_AUC: 0.4859\n",
      "timestep:343, pyg_AUC: 0.4901\n",
      "timestep:344, pyg_AUC: 0.4887\n",
      "timestep:345, pyg_AUC: 0.4859\n",
      "timestep:346, pyg_AUC: 0.4873\n",
      "timestep:347, pyg_AUC: 0.4859\n",
      "timestep:348, pyg_AUC: 0.4873\n",
      "timestep:349, pyg_AUC: 0.4887\n",
      "timestep:350, pyg_AUC: 0.4887\n",
      "timestep:351, pyg_AUC: 0.4859\n",
      "timestep:352, pyg_AUC: 0.4845\n",
      "timestep:353, pyg_AUC: 0.4845\n",
      "timestep:354, pyg_AUC: 0.4873\n",
      "timestep:355, pyg_AUC: 0.4873\n",
      "timestep:356, pyg_AUC: 0.4873\n",
      "timestep:357, pyg_AUC: 0.4915\n",
      "timestep:358, pyg_AUC: 0.4887\n",
      "timestep:359, pyg_AUC: 0.4887\n",
      "timestep:360, pyg_AUC: 0.4873\n",
      "timestep:361, pyg_AUC: 0.4887\n",
      "timestep:362, pyg_AUC: 0.4929\n",
      "timestep:363, pyg_AUC: 0.4859\n",
      "timestep:364, pyg_AUC: 0.4859\n",
      "timestep:365, pyg_AUC: 0.4859\n",
      "timestep:366, pyg_AUC: 0.4873\n",
      "timestep:367, pyg_AUC: 0.4887\n",
      "timestep:368, pyg_AUC: 0.4901\n",
      "timestep:369, pyg_AUC: 0.4915\n",
      "timestep:370, pyg_AUC: 0.4887\n",
      "timestep:371, pyg_AUC: 0.4887\n",
      "timestep:372, pyg_AUC: 0.4873\n",
      "timestep:373, pyg_AUC: 0.4887\n",
      "timestep:374, pyg_AUC: 0.4859\n",
      "timestep:375, pyg_AUC: 0.4845\n",
      "timestep:376, pyg_AUC: 0.4887\n",
      "timestep:377, pyg_AUC: 0.4859\n",
      "timestep:378, pyg_AUC: 0.4831\n",
      "timestep:379, pyg_AUC: 0.4873\n",
      "timestep:380, pyg_AUC: 0.4859\n",
      "timestep:381, pyg_AUC: 0.4859\n",
      "timestep:382, pyg_AUC: 0.4859\n",
      "timestep:383, pyg_AUC: 0.4887\n",
      "timestep:384, pyg_AUC: 0.4873\n",
      "timestep:385, pyg_AUC: 0.4873\n",
      "timestep:386, pyg_AUC: 0.4845\n",
      "timestep:387, pyg_AUC: 0.4831\n",
      "timestep:388, pyg_AUC: 0.4887\n",
      "timestep:389, pyg_AUC: 0.4887\n",
      "timestep:390, pyg_AUC: 0.4873\n",
      "timestep:391, pyg_AUC: 0.4845\n",
      "timestep:392, pyg_AUC: 0.4887\n",
      "timestep:393, pyg_AUC: 0.4816\n",
      "timestep:394, pyg_AUC: 0.4831\n",
      "timestep:395, pyg_AUC: 0.4859\n",
      "timestep:396, pyg_AUC: 0.4859\n",
      "timestep:397, pyg_AUC: 0.4859\n",
      "timestep:398, pyg_AUC: 0.4887\n",
      "timestep:399, pyg_AUC: 0.4873\n",
      "timestep:400, pyg_AUC: 0.4887\n",
      "timestep:401, pyg_AUC: 0.4802\n",
      "timestep:402, pyg_AUC: 0.4859\n",
      "timestep:403, pyg_AUC: 0.4887\n",
      "timestep:404, pyg_AUC: 0.4873\n",
      "timestep:405, pyg_AUC: 0.4887\n",
      "timestep:406, pyg_AUC: 0.4845\n",
      "timestep:407, pyg_AUC: 0.4859\n",
      "timestep:408, pyg_AUC: 0.4873\n",
      "timestep:409, pyg_AUC: 0.4859\n",
      "timestep:410, pyg_AUC: 0.4887\n",
      "timestep:411, pyg_AUC: 0.4873\n",
      "timestep:412, pyg_AUC: 0.4831\n",
      "timestep:413, pyg_AUC: 0.4915\n",
      "timestep:414, pyg_AUC: 0.4873\n",
      "timestep:415, pyg_AUC: 0.4887\n",
      "timestep:416, pyg_AUC: 0.4859\n",
      "timestep:417, pyg_AUC: 0.4873\n",
      "timestep:418, pyg_AUC: 0.4887\n",
      "timestep:419, pyg_AUC: 0.4873\n",
      "timestep:420, pyg_AUC: 0.4873\n",
      "timestep:421, pyg_AUC: 0.4887\n",
      "timestep:422, pyg_AUC: 0.4887\n",
      "timestep:423, pyg_AUC: 0.4901\n",
      "timestep:424, pyg_AUC: 0.4887\n",
      "timestep:425, pyg_AUC: 0.4901\n",
      "timestep:426, pyg_AUC: 0.4901\n",
      "timestep:427, pyg_AUC: 0.4873\n",
      "timestep:428, pyg_AUC: 0.4831\n",
      "timestep:429, pyg_AUC: 0.4859\n",
      "timestep:430, pyg_AUC: 0.4859\n",
      "timestep:431, pyg_AUC: 0.4873\n",
      "timestep:432, pyg_AUC: 0.4887\n",
      "timestep:433, pyg_AUC: 0.4873\n",
      "timestep:434, pyg_AUC: 0.4873\n",
      "timestep:435, pyg_AUC: 0.4901\n",
      "timestep:436, pyg_AUC: 0.4887\n",
      "timestep:437, pyg_AUC: 0.4887\n",
      "timestep:438, pyg_AUC: 0.4901\n",
      "timestep:439, pyg_AUC: 0.4887\n",
      "timestep:440, pyg_AUC: 0.4901\n",
      "timestep:441, pyg_AUC: 0.4873\n",
      "timestep:442, pyg_AUC: 0.4901\n",
      "timestep:443, pyg_AUC: 0.4873\n",
      "timestep:444, pyg_AUC: 0.4887\n",
      "timestep:445, pyg_AUC: 0.4831\n",
      "timestep:446, pyg_AUC: 0.4887\n",
      "timestep:447, pyg_AUC: 0.4901\n",
      "timestep:448, pyg_AUC: 0.4845\n",
      "timestep:449, pyg_AUC: 0.4887\n",
      "timestep:450, pyg_AUC: 0.4845\n",
      "timestep:451, pyg_AUC: 0.4887\n",
      "timestep:452, pyg_AUC: 0.4887\n",
      "timestep:453, pyg_AUC: 0.4901\n",
      "timestep:454, pyg_AUC: 0.4887\n",
      "timestep:455, pyg_AUC: 0.4859\n",
      "timestep:456, pyg_AUC: 0.4887\n",
      "timestep:457, pyg_AUC: 0.4859\n",
      "timestep:458, pyg_AUC: 0.4873\n",
      "timestep:459, pyg_AUC: 0.4887\n",
      "timestep:460, pyg_AUC: 0.4901\n",
      "timestep:461, pyg_AUC: 0.4859\n",
      "timestep:462, pyg_AUC: 0.4901\n",
      "timestep:463, pyg_AUC: 0.4901\n",
      "timestep:464, pyg_AUC: 0.4887\n",
      "timestep:465, pyg_AUC: 0.4873\n",
      "timestep:466, pyg_AUC: 0.4859\n",
      "timestep:467, pyg_AUC: 0.4901\n",
      "timestep:468, pyg_AUC: 0.4887\n",
      "timestep:469, pyg_AUC: 0.4859\n",
      "timestep:470, pyg_AUC: 0.4831\n",
      "timestep:471, pyg_AUC: 0.4873\n",
      "timestep:472, pyg_AUC: 0.4873\n",
      "timestep:473, pyg_AUC: 0.4859\n",
      "timestep:474, pyg_AUC: 0.4915\n",
      "timestep:475, pyg_AUC: 0.4901\n",
      "timestep:476, pyg_AUC: 0.4887\n",
      "timestep:477, pyg_AUC: 0.4859\n",
      "timestep:478, pyg_AUC: 0.4887\n",
      "timestep:479, pyg_AUC: 0.4845\n",
      "timestep:480, pyg_AUC: 0.4859\n",
      "timestep:481, pyg_AUC: 0.4859\n",
      "timestep:482, pyg_AUC: 0.4873\n",
      "timestep:483, pyg_AUC: 0.4873\n",
      "timestep:484, pyg_AUC: 0.4859\n",
      "timestep:485, pyg_AUC: 0.4859\n",
      "timestep:486, pyg_AUC: 0.4859\n",
      "timestep:487, pyg_AUC: 0.4901\n",
      "timestep:488, pyg_AUC: 0.4873\n",
      "timestep:489, pyg_AUC: 0.4887\n",
      "timestep:490, pyg_AUC: 0.4901\n",
      "timestep:491, pyg_AUC: 0.4873\n",
      "timestep:492, pyg_AUC: 0.4887\n",
      "timestep:493, pyg_AUC: 0.4901\n",
      "timestep:494, pyg_AUC: 0.4887\n",
      "timestep:495, pyg_AUC: 0.4859\n",
      "timestep:496, pyg_AUC: 0.4887\n",
      "timestep:497, pyg_AUC: 0.4887\n",
      "timestep:498, pyg_AUC: 0.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [18:30<15:17, 101.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:499, pyg_AUC: 0.4915\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 39.35904\n",
      "Epoch: 0010 loss= 29.17165\n",
      "Epoch: 0020 loss= 19.43969\n",
      "Epoch: 0030 loss= 18.19963\n",
      "Epoch: 0040 loss= 12.00288\n",
      "Epoch: 0050 loss= 6.20477\n",
      "Epoch: 0060 loss= 2.22260\n",
      "Epoch: 0070 loss= 1.02518\n",
      "Epoch: 0080 loss= 0.95035\n",
      "Epoch: 0090 loss= 0.93086\n",
      "Epoch: 0100 loss= 0.73464\n",
      "Epoch: 0110 loss= 0.62667\n",
      "Epoch: 0120 loss= 0.58846\n",
      "Epoch: 0130 loss= 0.64864\n",
      "Epoch: 0140 loss= 0.65306\n",
      "Epoch: 0150 loss= 0.58918\n",
      "Epoch: 0160 loss= 0.53883\n",
      "Epoch: 0170 loss= 0.62951\n",
      "Epoch: 0180 loss= 0.59401\n",
      "Epoch: 0190 loss= 0.52517\n",
      "Epoch: 0200 loss= 0.64239\n",
      "Epoch: 0210 loss= 0.61863\n",
      "Epoch: 0220 loss= 0.61715\n",
      "Epoch: 0230 loss= 0.66073\n",
      "Epoch: 0240 loss= 0.63735\n",
      "Epoch: 0250 loss= 0.55691\n",
      "Epoch: 0260 loss= 0.64955\n",
      "Epoch: 0270 loss= 0.50248\n",
      "Epoch: 0280 loss= 0.58078\n",
      "Epoch: 0290 loss= 0.59196\n",
      "Epoch: 0300 loss= 0.62263\n",
      "Epoch: 0310 loss= 0.57856\n",
      "Epoch: 0320 loss= 0.52537\n",
      "Epoch: 0330 loss= 0.50644\n",
      "Epoch: 0340 loss= 0.54605\n",
      "Epoch: 0350 loss= 0.70242\n",
      "Epoch: 0360 loss= 0.65520\n",
      "Epoch: 0370 loss= 0.61701\n",
      "Epoch: 0380 loss= 0.57459\n",
      "Epoch: 0390 loss= 0.55978\n",
      "Epoch: 0400 loss= 0.58930\n",
      "Epoch: 0410 loss= 0.58731\n",
      "Epoch: 0420 loss= 0.54249\n",
      "Epoch: 0430 loss= 0.55281\n",
      "Epoch: 0440 loss= 0.52838\n",
      "Epoch: 0450 loss= 0.56844\n",
      "Epoch: 0460 loss= 0.53207\n",
      "Epoch: 0470 loss= 0.58170\n",
      "Epoch: 0480 loss= 0.53206\n",
      "Epoch: 0490 loss= 0.54436\n",
      "Epoch: 0500 loss= 0.55533\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.8072,  4.6982, -5.3783, -4.6276, -4.3834,  5.6597,  5.4700, -5.2512]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 43.35785\n",
      "Epoch: 0010 loss= 29.34479\n",
      "Epoch: 0020 loss= 16.27497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0030 loss= 12.92207\n",
      "Epoch: 0040 loss= 3.58091\n",
      "Epoch: 0050 loss= 1.32048\n",
      "Epoch: 0060 loss= 1.11041\n",
      "Epoch: 0070 loss= 0.70642\n",
      "Epoch: 0080 loss= 0.66843\n",
      "Epoch: 0090 loss= 0.89318\n",
      "Epoch: 0100 loss= 0.70181\n",
      "Epoch: 0110 loss= 0.67280\n",
      "Epoch: 0120 loss= 0.64324\n",
      "Epoch: 0130 loss= 0.51843\n",
      "Epoch: 0140 loss= 0.62808\n",
      "Epoch: 0150 loss= 0.66568\n",
      "Epoch: 0160 loss= 0.56307\n",
      "Epoch: 0170 loss= 0.63642\n",
      "Epoch: 0180 loss= 0.71387\n",
      "Epoch: 0190 loss= 0.51330\n",
      "Epoch: 0200 loss= 0.54778\n",
      "Epoch: 0210 loss= 0.64114\n",
      "Epoch: 0220 loss= 0.59166\n",
      "Epoch: 0230 loss= 0.54896\n",
      "Epoch: 0240 loss= 0.60114\n",
      "Epoch: 0250 loss= 0.56157\n",
      "Epoch: 0260 loss= 0.56298\n",
      "Epoch: 0270 loss= 0.56409\n",
      "Epoch: 0280 loss= 0.61256\n",
      "Epoch: 0290 loss= 0.55272\n",
      "Epoch: 0300 loss= 0.55279\n",
      "Epoch: 0310 loss= 0.57982\n",
      "Epoch: 0320 loss= 0.55660\n",
      "Epoch: 0330 loss= 0.49976\n",
      "Epoch: 0340 loss= 0.61634\n",
      "Epoch: 0350 loss= 0.52848\n",
      "Epoch: 0360 loss= 0.64468\n",
      "Epoch: 0370 loss= 0.62946\n",
      "Epoch: 0380 loss= 0.55514\n",
      "Epoch: 0390 loss= 0.60773\n",
      "Epoch: 0400 loss= 0.52353\n",
      "Epoch: 0410 loss= 0.53474\n",
      "Epoch: 0420 loss= 0.61786\n",
      "Epoch: 0430 loss= 0.57684\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4901\n",
      "timestep:1, pyg_AUC: 0.4929\n",
      "timestep:2, pyg_AUC: 0.4873\n",
      "timestep:3, pyg_AUC: 0.4915\n",
      "timestep:4, pyg_AUC: 0.4887\n",
      "timestep:5, pyg_AUC: 0.4887\n",
      "timestep:6, pyg_AUC: 0.4915\n",
      "timestep:7, pyg_AUC: 0.4929\n",
      "timestep:8, pyg_AUC: 0.4873\n",
      "timestep:9, pyg_AUC: 0.4873\n",
      "timestep:10, pyg_AUC: 0.4915\n",
      "timestep:11, pyg_AUC: 0.4901\n",
      "timestep:12, pyg_AUC: 0.4887\n",
      "timestep:13, pyg_AUC: 0.4901\n",
      "timestep:14, pyg_AUC: 0.4901\n",
      "timestep:15, pyg_AUC: 0.4915\n",
      "timestep:16, pyg_AUC: 0.4915\n",
      "timestep:17, pyg_AUC: 0.4845\n",
      "timestep:18, pyg_AUC: 0.4915\n",
      "timestep:19, pyg_AUC: 0.4887\n",
      "timestep:20, pyg_AUC: 0.4944\n",
      "timestep:21, pyg_AUC: 0.4873\n",
      "timestep:22, pyg_AUC: 0.4887\n",
      "timestep:23, pyg_AUC: 0.4901\n",
      "timestep:24, pyg_AUC: 0.4887\n",
      "timestep:25, pyg_AUC: 0.4901\n",
      "timestep:26, pyg_AUC: 0.4901\n",
      "timestep:27, pyg_AUC: 0.4915\n",
      "timestep:28, pyg_AUC: 0.4859\n",
      "timestep:29, pyg_AUC: 0.4929\n",
      "timestep:30, pyg_AUC: 0.4859\n",
      "timestep:31, pyg_AUC: 0.4915\n",
      "timestep:32, pyg_AUC: 0.4873\n",
      "timestep:33, pyg_AUC: 0.4887\n",
      "timestep:34, pyg_AUC: 0.4901\n",
      "timestep:35, pyg_AUC: 0.4873\n",
      "timestep:36, pyg_AUC: 0.4915\n",
      "timestep:37, pyg_AUC: 0.4887\n",
      "timestep:38, pyg_AUC: 0.4929\n",
      "timestep:39, pyg_AUC: 0.4873\n",
      "timestep:40, pyg_AUC: 0.4887\n",
      "timestep:41, pyg_AUC: 0.4901\n",
      "timestep:42, pyg_AUC: 0.4929\n",
      "timestep:43, pyg_AUC: 0.4887\n",
      "timestep:44, pyg_AUC: 0.4901\n",
      "timestep:45, pyg_AUC: 0.4887\n",
      "timestep:46, pyg_AUC: 0.4915\n",
      "timestep:47, pyg_AUC: 0.4915\n",
      "timestep:48, pyg_AUC: 0.4915\n",
      "timestep:49, pyg_AUC: 0.4915\n",
      "timestep:50, pyg_AUC: 0.4873\n",
      "timestep:51, pyg_AUC: 0.4929\n",
      "timestep:52, pyg_AUC: 0.4901\n",
      "timestep:53, pyg_AUC: 0.4901\n",
      "timestep:54, pyg_AUC: 0.4901\n",
      "timestep:55, pyg_AUC: 0.4887\n",
      "timestep:56, pyg_AUC: 0.4859\n",
      "timestep:57, pyg_AUC: 0.4887\n",
      "timestep:58, pyg_AUC: 0.4901\n",
      "timestep:59, pyg_AUC: 0.4915\n",
      "timestep:60, pyg_AUC: 0.4901\n",
      "timestep:61, pyg_AUC: 0.4901\n",
      "timestep:62, pyg_AUC: 0.4887\n",
      "timestep:63, pyg_AUC: 0.4901\n",
      "timestep:64, pyg_AUC: 0.4887\n",
      "timestep:65, pyg_AUC: 0.4944\n",
      "timestep:66, pyg_AUC: 0.4887\n",
      "timestep:67, pyg_AUC: 0.4901\n",
      "timestep:68, pyg_AUC: 0.4915\n",
      "timestep:69, pyg_AUC: 0.4887\n",
      "timestep:70, pyg_AUC: 0.4915\n",
      "timestep:71, pyg_AUC: 0.4929\n",
      "timestep:72, pyg_AUC: 0.4915\n",
      "timestep:73, pyg_AUC: 0.4915\n",
      "timestep:74, pyg_AUC: 0.4901\n",
      "timestep:75, pyg_AUC: 0.4887\n",
      "timestep:76, pyg_AUC: 0.4901\n",
      "timestep:77, pyg_AUC: 0.4887\n",
      "timestep:78, pyg_AUC: 0.4887\n",
      "timestep:79, pyg_AUC: 0.4887\n",
      "timestep:80, pyg_AUC: 0.4901\n",
      "timestep:81, pyg_AUC: 0.4929\n",
      "timestep:82, pyg_AUC: 0.4859\n",
      "timestep:83, pyg_AUC: 0.4887\n",
      "timestep:84, pyg_AUC: 0.4887\n",
      "timestep:85, pyg_AUC: 0.4873\n",
      "timestep:86, pyg_AUC: 0.4901\n",
      "timestep:87, pyg_AUC: 0.4873\n",
      "timestep:88, pyg_AUC: 0.4901\n",
      "timestep:89, pyg_AUC: 0.4901\n",
      "timestep:90, pyg_AUC: 0.4915\n",
      "timestep:91, pyg_AUC: 0.4887\n",
      "timestep:92, pyg_AUC: 0.4929\n",
      "timestep:93, pyg_AUC: 0.4887\n",
      "timestep:94, pyg_AUC: 0.4901\n",
      "timestep:95, pyg_AUC: 0.4887\n",
      "timestep:96, pyg_AUC: 0.4901\n",
      "timestep:97, pyg_AUC: 0.4887\n",
      "timestep:98, pyg_AUC: 0.4901\n",
      "timestep:99, pyg_AUC: 0.4887\n",
      "timestep:100, pyg_AUC: 0.4901\n",
      "timestep:101, pyg_AUC: 0.4887\n",
      "timestep:102, pyg_AUC: 0.4873\n",
      "timestep:103, pyg_AUC: 0.4873\n",
      "timestep:104, pyg_AUC: 0.4901\n",
      "timestep:105, pyg_AUC: 0.4915\n",
      "timestep:106, pyg_AUC: 0.4901\n",
      "timestep:107, pyg_AUC: 0.4873\n",
      "timestep:108, pyg_AUC: 0.4859\n",
      "timestep:109, pyg_AUC: 0.4901\n",
      "timestep:110, pyg_AUC: 0.4901\n",
      "timestep:111, pyg_AUC: 0.4915\n",
      "timestep:112, pyg_AUC: 0.4887\n",
      "timestep:113, pyg_AUC: 0.4859\n",
      "timestep:114, pyg_AUC: 0.4859\n",
      "timestep:115, pyg_AUC: 0.4901\n",
      "timestep:116, pyg_AUC: 0.4915\n",
      "timestep:117, pyg_AUC: 0.4873\n",
      "timestep:118, pyg_AUC: 0.4901\n",
      "timestep:119, pyg_AUC: 0.4816\n",
      "timestep:120, pyg_AUC: 0.4873\n",
      "timestep:121, pyg_AUC: 0.4873\n",
      "timestep:122, pyg_AUC: 0.4887\n",
      "timestep:123, pyg_AUC: 0.4887\n",
      "timestep:124, pyg_AUC: 0.4873\n",
      "timestep:125, pyg_AUC: 0.4887\n",
      "timestep:126, pyg_AUC: 0.4873\n",
      "timestep:127, pyg_AUC: 0.4887\n",
      "timestep:128, pyg_AUC: 0.4845\n",
      "timestep:129, pyg_AUC: 0.4873\n",
      "timestep:130, pyg_AUC: 0.4915\n",
      "timestep:131, pyg_AUC: 0.4859\n",
      "timestep:132, pyg_AUC: 0.4873\n",
      "timestep:133, pyg_AUC: 0.4901\n",
      "timestep:134, pyg_AUC: 0.4929\n",
      "timestep:135, pyg_AUC: 0.4887\n",
      "timestep:136, pyg_AUC: 0.4901\n",
      "timestep:137, pyg_AUC: 0.4901\n",
      "timestep:138, pyg_AUC: 0.4901\n",
      "timestep:139, pyg_AUC: 0.4901\n",
      "timestep:140, pyg_AUC: 0.4887\n",
      "timestep:141, pyg_AUC: 0.4915\n",
      "timestep:142, pyg_AUC: 0.4873\n",
      "timestep:143, pyg_AUC: 0.4915\n",
      "timestep:144, pyg_AUC: 0.4887\n",
      "timestep:145, pyg_AUC: 0.4901\n",
      "timestep:146, pyg_AUC: 0.4887\n",
      "timestep:147, pyg_AUC: 0.4845\n",
      "timestep:148, pyg_AUC: 0.4859\n",
      "timestep:149, pyg_AUC: 0.4901\n",
      "timestep:150, pyg_AUC: 0.4901\n",
      "timestep:151, pyg_AUC: 0.4901\n",
      "timestep:152, pyg_AUC: 0.4859\n",
      "timestep:153, pyg_AUC: 0.4887\n",
      "timestep:154, pyg_AUC: 0.4887\n",
      "timestep:155, pyg_AUC: 0.4831\n",
      "timestep:156, pyg_AUC: 0.4845\n",
      "timestep:157, pyg_AUC: 0.4901\n",
      "timestep:158, pyg_AUC: 0.4901\n",
      "timestep:159, pyg_AUC: 0.4859\n",
      "timestep:160, pyg_AUC: 0.4901\n",
      "timestep:161, pyg_AUC: 0.4845\n",
      "timestep:162, pyg_AUC: 0.4901\n",
      "timestep:163, pyg_AUC: 0.4873\n",
      "timestep:164, pyg_AUC: 0.4887\n",
      "timestep:165, pyg_AUC: 0.4901\n",
      "timestep:166, pyg_AUC: 0.4859\n",
      "timestep:167, pyg_AUC: 0.4901\n",
      "timestep:168, pyg_AUC: 0.4887\n",
      "timestep:169, pyg_AUC: 0.4873\n",
      "timestep:170, pyg_AUC: 0.4901\n",
      "timestep:171, pyg_AUC: 0.4915\n",
      "timestep:172, pyg_AUC: 0.4887\n",
      "timestep:173, pyg_AUC: 0.4901\n",
      "timestep:174, pyg_AUC: 0.4901\n",
      "timestep:175, pyg_AUC: 0.4901\n",
      "timestep:176, pyg_AUC: 0.4929\n",
      "timestep:177, pyg_AUC: 0.4887\n",
      "timestep:178, pyg_AUC: 0.4901\n",
      "timestep:179, pyg_AUC: 0.4887\n",
      "timestep:180, pyg_AUC: 0.4873\n",
      "timestep:181, pyg_AUC: 0.4915\n",
      "timestep:182, pyg_AUC: 0.4887\n",
      "timestep:183, pyg_AUC: 0.4859\n",
      "timestep:184, pyg_AUC: 0.4873\n",
      "timestep:185, pyg_AUC: 0.4887\n",
      "timestep:186, pyg_AUC: 0.4887\n",
      "timestep:187, pyg_AUC: 0.4901\n",
      "timestep:188, pyg_AUC: 0.4873\n",
      "timestep:189, pyg_AUC: 0.4845\n",
      "timestep:190, pyg_AUC: 0.4901\n",
      "timestep:191, pyg_AUC: 0.4929\n",
      "timestep:192, pyg_AUC: 0.4887\n",
      "timestep:193, pyg_AUC: 0.4887\n",
      "timestep:194, pyg_AUC: 0.4887\n",
      "timestep:195, pyg_AUC: 0.4873\n",
      "timestep:196, pyg_AUC: 0.4901\n",
      "timestep:197, pyg_AUC: 0.4887\n",
      "timestep:198, pyg_AUC: 0.4887\n",
      "timestep:199, pyg_AUC: 0.4873\n",
      "timestep:200, pyg_AUC: 0.4873\n",
      "timestep:201, pyg_AUC: 0.4901\n",
      "timestep:202, pyg_AUC: 0.4887\n",
      "timestep:203, pyg_AUC: 0.4873\n",
      "timestep:204, pyg_AUC: 0.4873\n",
      "timestep:205, pyg_AUC: 0.4873\n",
      "timestep:206, pyg_AUC: 0.4887\n",
      "timestep:207, pyg_AUC: 0.4901\n",
      "timestep:208, pyg_AUC: 0.4873\n",
      "timestep:209, pyg_AUC: 0.4887\n",
      "timestep:210, pyg_AUC: 0.4901\n",
      "timestep:211, pyg_AUC: 0.4859\n",
      "timestep:212, pyg_AUC: 0.4915\n",
      "timestep:213, pyg_AUC: 0.4915\n",
      "timestep:214, pyg_AUC: 0.4887\n",
      "timestep:215, pyg_AUC: 0.4901\n",
      "timestep:216, pyg_AUC: 0.4873\n",
      "timestep:217, pyg_AUC: 0.4887\n",
      "timestep:218, pyg_AUC: 0.4859\n",
      "timestep:219, pyg_AUC: 0.4859\n",
      "timestep:220, pyg_AUC: 0.4873\n",
      "timestep:221, pyg_AUC: 0.4915\n",
      "timestep:222, pyg_AUC: 0.4873\n",
      "timestep:223, pyg_AUC: 0.4873\n",
      "timestep:224, pyg_AUC: 0.4816\n",
      "timestep:225, pyg_AUC: 0.4887\n",
      "timestep:226, pyg_AUC: 0.4873\n",
      "timestep:227, pyg_AUC: 0.4859\n",
      "timestep:228, pyg_AUC: 0.4873\n",
      "timestep:229, pyg_AUC: 0.4845\n",
      "timestep:230, pyg_AUC: 0.4901\n",
      "timestep:231, pyg_AUC: 0.4845\n",
      "timestep:232, pyg_AUC: 0.4845\n",
      "timestep:233, pyg_AUC: 0.4845\n",
      "timestep:234, pyg_AUC: 0.4887\n",
      "timestep:235, pyg_AUC: 0.4915\n",
      "timestep:236, pyg_AUC: 0.4873\n",
      "timestep:237, pyg_AUC: 0.4901\n",
      "timestep:238, pyg_AUC: 0.4845\n",
      "timestep:239, pyg_AUC: 0.4873\n",
      "timestep:240, pyg_AUC: 0.4831\n",
      "timestep:241, pyg_AUC: 0.4873\n",
      "timestep:242, pyg_AUC: 0.4873\n",
      "timestep:243, pyg_AUC: 0.4887\n",
      "timestep:244, pyg_AUC: 0.4859\n",
      "timestep:245, pyg_AUC: 0.4859\n",
      "timestep:246, pyg_AUC: 0.4887\n",
      "timestep:247, pyg_AUC: 0.4859\n",
      "timestep:248, pyg_AUC: 0.4887\n",
      "timestep:249, pyg_AUC: 0.4901\n",
      "timestep:250, pyg_AUC: 0.4887\n",
      "timestep:251, pyg_AUC: 0.4901\n",
      "timestep:252, pyg_AUC: 0.4887\n",
      "timestep:253, pyg_AUC: 0.4873\n",
      "timestep:254, pyg_AUC: 0.4887\n",
      "timestep:255, pyg_AUC: 0.4873\n",
      "timestep:256, pyg_AUC: 0.4859\n",
      "timestep:257, pyg_AUC: 0.4831\n",
      "timestep:258, pyg_AUC: 0.4873\n",
      "timestep:259, pyg_AUC: 0.4887\n",
      "timestep:260, pyg_AUC: 0.4887\n",
      "timestep:261, pyg_AUC: 0.4887\n",
      "timestep:262, pyg_AUC: 0.4887\n",
      "timestep:263, pyg_AUC: 0.4887\n",
      "timestep:264, pyg_AUC: 0.4887\n",
      "timestep:265, pyg_AUC: 0.4887\n",
      "timestep:266, pyg_AUC: 0.4873\n",
      "timestep:267, pyg_AUC: 0.4873\n",
      "timestep:268, pyg_AUC: 0.4859\n",
      "timestep:269, pyg_AUC: 0.4887\n",
      "timestep:270, pyg_AUC: 0.4873\n",
      "timestep:271, pyg_AUC: 0.4887\n",
      "timestep:272, pyg_AUC: 0.4845\n",
      "timestep:273, pyg_AUC: 0.4859\n",
      "timestep:274, pyg_AUC: 0.4859\n",
      "timestep:275, pyg_AUC: 0.4873\n",
      "timestep:276, pyg_AUC: 0.4831\n",
      "timestep:277, pyg_AUC: 0.4859\n",
      "timestep:278, pyg_AUC: 0.4887\n",
      "timestep:279, pyg_AUC: 0.4873\n",
      "timestep:280, pyg_AUC: 0.4873\n",
      "timestep:281, pyg_AUC: 0.4873\n",
      "timestep:282, pyg_AUC: 0.4887\n",
      "timestep:283, pyg_AUC: 0.4873\n",
      "timestep:284, pyg_AUC: 0.4859\n",
      "timestep:285, pyg_AUC: 0.4901\n",
      "timestep:286, pyg_AUC: 0.4859\n",
      "timestep:287, pyg_AUC: 0.4859\n",
      "timestep:288, pyg_AUC: 0.4873\n",
      "timestep:289, pyg_AUC: 0.4831\n",
      "timestep:290, pyg_AUC: 0.4873\n",
      "timestep:291, pyg_AUC: 0.4887\n",
      "timestep:292, pyg_AUC: 0.4873\n",
      "timestep:293, pyg_AUC: 0.4901\n",
      "timestep:294, pyg_AUC: 0.4859\n",
      "timestep:295, pyg_AUC: 0.4873\n",
      "timestep:296, pyg_AUC: 0.4873\n",
      "timestep:297, pyg_AUC: 0.4873\n",
      "timestep:298, pyg_AUC: 0.4887\n",
      "timestep:299, pyg_AUC: 0.4887\n",
      "timestep:300, pyg_AUC: 0.4873\n",
      "timestep:301, pyg_AUC: 0.4901\n",
      "timestep:302, pyg_AUC: 0.4873\n",
      "timestep:303, pyg_AUC: 0.4873\n",
      "timestep:304, pyg_AUC: 0.4873\n",
      "timestep:305, pyg_AUC: 0.4901\n",
      "timestep:306, pyg_AUC: 0.4873\n",
      "timestep:307, pyg_AUC: 0.4901\n",
      "timestep:308, pyg_AUC: 0.4873\n",
      "timestep:309, pyg_AUC: 0.4873\n",
      "timestep:310, pyg_AUC: 0.4873\n",
      "timestep:311, pyg_AUC: 0.4873\n",
      "timestep:312, pyg_AUC: 0.4859\n",
      "timestep:313, pyg_AUC: 0.4873\n",
      "timestep:314, pyg_AUC: 0.4901\n",
      "timestep:315, pyg_AUC: 0.4887\n",
      "timestep:316, pyg_AUC: 0.4873\n",
      "timestep:317, pyg_AUC: 0.4887\n",
      "timestep:318, pyg_AUC: 0.4887\n",
      "timestep:319, pyg_AUC: 0.4845\n",
      "timestep:320, pyg_AUC: 0.4887\n",
      "timestep:321, pyg_AUC: 0.4901\n",
      "timestep:322, pyg_AUC: 0.4887\n",
      "timestep:323, pyg_AUC: 0.4873\n",
      "timestep:324, pyg_AUC: 0.4873\n",
      "timestep:325, pyg_AUC: 0.4915\n",
      "timestep:326, pyg_AUC: 0.4915\n",
      "timestep:327, pyg_AUC: 0.4859\n",
      "timestep:328, pyg_AUC: 0.4873\n",
      "timestep:329, pyg_AUC: 0.4887\n",
      "timestep:330, pyg_AUC: 0.4915\n",
      "timestep:331, pyg_AUC: 0.4887\n",
      "timestep:332, pyg_AUC: 0.4873\n",
      "timestep:333, pyg_AUC: 0.4887\n",
      "timestep:334, pyg_AUC: 0.4873\n",
      "timestep:335, pyg_AUC: 0.4873\n",
      "timestep:336, pyg_AUC: 0.4873\n",
      "timestep:337, pyg_AUC: 0.4887\n",
      "timestep:338, pyg_AUC: 0.4901\n",
      "timestep:339, pyg_AUC: 0.4887\n",
      "timestep:340, pyg_AUC: 0.4887\n",
      "timestep:341, pyg_AUC: 0.4887\n",
      "timestep:342, pyg_AUC: 0.4887\n",
      "timestep:343, pyg_AUC: 0.4887\n",
      "timestep:344, pyg_AUC: 0.4901\n",
      "timestep:345, pyg_AUC: 0.4873\n",
      "timestep:346, pyg_AUC: 0.4915\n",
      "timestep:347, pyg_AUC: 0.4873\n",
      "timestep:348, pyg_AUC: 0.4873\n",
      "timestep:349, pyg_AUC: 0.4887\n",
      "timestep:350, pyg_AUC: 0.4915\n",
      "timestep:351, pyg_AUC: 0.4873\n",
      "timestep:352, pyg_AUC: 0.4887\n",
      "timestep:353, pyg_AUC: 0.4901\n",
      "timestep:354, pyg_AUC: 0.4873\n",
      "timestep:355, pyg_AUC: 0.4887\n",
      "timestep:356, pyg_AUC: 0.4887\n",
      "timestep:357, pyg_AUC: 0.4901\n",
      "timestep:358, pyg_AUC: 0.4887\n",
      "timestep:359, pyg_AUC: 0.4929\n",
      "timestep:360, pyg_AUC: 0.4901\n",
      "timestep:361, pyg_AUC: 0.4887\n",
      "timestep:362, pyg_AUC: 0.4901\n",
      "timestep:363, pyg_AUC: 0.4915\n",
      "timestep:364, pyg_AUC: 0.4901\n",
      "timestep:365, pyg_AUC: 0.4901\n",
      "timestep:366, pyg_AUC: 0.4901\n",
      "timestep:367, pyg_AUC: 0.4887\n",
      "timestep:368, pyg_AUC: 0.4873\n",
      "timestep:369, pyg_AUC: 0.4901\n",
      "timestep:370, pyg_AUC: 0.4915\n",
      "timestep:371, pyg_AUC: 0.4915\n",
      "timestep:372, pyg_AUC: 0.4915\n",
      "timestep:373, pyg_AUC: 0.4887\n",
      "timestep:374, pyg_AUC: 0.4887\n",
      "timestep:375, pyg_AUC: 0.4901\n",
      "timestep:376, pyg_AUC: 0.4859\n",
      "timestep:377, pyg_AUC: 0.4901\n",
      "timestep:378, pyg_AUC: 0.4859\n",
      "timestep:379, pyg_AUC: 0.4887\n",
      "timestep:380, pyg_AUC: 0.4887\n",
      "timestep:381, pyg_AUC: 0.4859\n",
      "timestep:382, pyg_AUC: 0.4901\n",
      "timestep:383, pyg_AUC: 0.4901\n",
      "timestep:384, pyg_AUC: 0.4901\n",
      "timestep:385, pyg_AUC: 0.4887\n",
      "timestep:386, pyg_AUC: 0.4915\n",
      "timestep:387, pyg_AUC: 0.4873\n",
      "timestep:388, pyg_AUC: 0.4887\n",
      "timestep:389, pyg_AUC: 0.4887\n",
      "timestep:390, pyg_AUC: 0.4901\n",
      "timestep:391, pyg_AUC: 0.4901\n",
      "timestep:392, pyg_AUC: 0.4873\n",
      "timestep:393, pyg_AUC: 0.4887\n",
      "timestep:394, pyg_AUC: 0.4887\n",
      "timestep:395, pyg_AUC: 0.4901\n",
      "timestep:396, pyg_AUC: 0.4901\n",
      "timestep:397, pyg_AUC: 0.4887\n",
      "timestep:398, pyg_AUC: 0.4901\n",
      "timestep:399, pyg_AUC: 0.4901\n",
      "timestep:400, pyg_AUC: 0.4873\n",
      "timestep:401, pyg_AUC: 0.4873\n",
      "timestep:402, pyg_AUC: 0.4887\n",
      "timestep:403, pyg_AUC: 0.4901\n",
      "timestep:404, pyg_AUC: 0.4887\n",
      "timestep:405, pyg_AUC: 0.4915\n",
      "timestep:406, pyg_AUC: 0.4873\n",
      "timestep:407, pyg_AUC: 0.4887\n",
      "timestep:408, pyg_AUC: 0.4887\n",
      "timestep:409, pyg_AUC: 0.4873\n",
      "timestep:410, pyg_AUC: 0.4901\n",
      "timestep:411, pyg_AUC: 0.4887\n",
      "timestep:412, pyg_AUC: 0.4873\n",
      "timestep:413, pyg_AUC: 0.4887\n",
      "timestep:414, pyg_AUC: 0.4887\n",
      "timestep:415, pyg_AUC: 0.4887\n",
      "timestep:416, pyg_AUC: 0.4887\n",
      "timestep:417, pyg_AUC: 0.4887\n",
      "timestep:418, pyg_AUC: 0.4901\n",
      "timestep:419, pyg_AUC: 0.4887\n",
      "timestep:420, pyg_AUC: 0.4901\n",
      "timestep:421, pyg_AUC: 0.4901\n",
      "timestep:422, pyg_AUC: 0.4873\n",
      "timestep:423, pyg_AUC: 0.4859\n",
      "timestep:424, pyg_AUC: 0.4873\n",
      "timestep:425, pyg_AUC: 0.4901\n",
      "timestep:426, pyg_AUC: 0.4915\n",
      "timestep:427, pyg_AUC: 0.4873\n",
      "timestep:428, pyg_AUC: 0.4915\n",
      "timestep:429, pyg_AUC: 0.4915\n",
      "timestep:430, pyg_AUC: 0.4901\n",
      "timestep:431, pyg_AUC: 0.4887\n",
      "timestep:432, pyg_AUC: 0.4887\n",
      "timestep:433, pyg_AUC: 0.4901\n",
      "timestep:434, pyg_AUC: 0.4901\n",
      "timestep:435, pyg_AUC: 0.4887\n",
      "timestep:436, pyg_AUC: 0.4901\n",
      "timestep:437, pyg_AUC: 0.4887\n",
      "timestep:438, pyg_AUC: 0.4873\n",
      "timestep:439, pyg_AUC: 0.4887\n",
      "timestep:440, pyg_AUC: 0.4915\n",
      "timestep:441, pyg_AUC: 0.4887\n",
      "timestep:442, pyg_AUC: 0.4887\n",
      "timestep:443, pyg_AUC: 0.4887\n",
      "timestep:444, pyg_AUC: 0.4901\n",
      "timestep:445, pyg_AUC: 0.4901\n",
      "timestep:446, pyg_AUC: 0.4915\n",
      "timestep:447, pyg_AUC: 0.4887\n",
      "timestep:448, pyg_AUC: 0.4901\n",
      "timestep:449, pyg_AUC: 0.4887\n",
      "timestep:450, pyg_AUC: 0.4901\n",
      "timestep:451, pyg_AUC: 0.4901\n",
      "timestep:452, pyg_AUC: 0.4887\n",
      "timestep:453, pyg_AUC: 0.4901\n",
      "timestep:454, pyg_AUC: 0.4901\n",
      "timestep:455, pyg_AUC: 0.4901\n",
      "timestep:456, pyg_AUC: 0.4873\n",
      "timestep:457, pyg_AUC: 0.4915\n",
      "timestep:458, pyg_AUC: 0.4887\n",
      "timestep:459, pyg_AUC: 0.4859\n",
      "timestep:460, pyg_AUC: 0.4873\n",
      "timestep:461, pyg_AUC: 0.4887\n",
      "timestep:462, pyg_AUC: 0.4915\n",
      "timestep:463, pyg_AUC: 0.4887\n",
      "timestep:464, pyg_AUC: 0.4901\n",
      "timestep:465, pyg_AUC: 0.4845\n",
      "timestep:466, pyg_AUC: 0.4873\n",
      "timestep:467, pyg_AUC: 0.4887\n",
      "timestep:468, pyg_AUC: 0.4915\n",
      "timestep:469, pyg_AUC: 0.4901\n",
      "timestep:470, pyg_AUC: 0.4901\n",
      "timestep:471, pyg_AUC: 0.4859\n",
      "timestep:472, pyg_AUC: 0.4901\n",
      "timestep:473, pyg_AUC: 0.4873\n",
      "timestep:474, pyg_AUC: 0.4887\n",
      "timestep:475, pyg_AUC: 0.4887\n",
      "timestep:476, pyg_AUC: 0.4915\n",
      "timestep:477, pyg_AUC: 0.4887\n",
      "timestep:478, pyg_AUC: 0.4901\n",
      "timestep:479, pyg_AUC: 0.4915\n",
      "timestep:480, pyg_AUC: 0.4901\n",
      "timestep:481, pyg_AUC: 0.4929\n",
      "timestep:482, pyg_AUC: 0.4887\n",
      "timestep:483, pyg_AUC: 0.4915\n",
      "timestep:484, pyg_AUC: 0.4901\n",
      "timestep:485, pyg_AUC: 0.4859\n",
      "timestep:486, pyg_AUC: 0.4901\n",
      "timestep:487, pyg_AUC: 0.4887\n",
      "timestep:488, pyg_AUC: 0.4887\n",
      "timestep:489, pyg_AUC: 0.4915\n",
      "timestep:490, pyg_AUC: 0.4915\n",
      "timestep:491, pyg_AUC: 0.4887\n",
      "timestep:492, pyg_AUC: 0.4901\n",
      "timestep:493, pyg_AUC: 0.4859\n",
      "timestep:494, pyg_AUC: 0.4901\n",
      "timestep:495, pyg_AUC: 0.4901\n",
      "timestep:496, pyg_AUC: 0.4915\n",
      "timestep:497, pyg_AUC: 0.4859\n",
      "timestep:498, pyg_AUC: 0.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [20:14<13:41, 102.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:499, pyg_AUC: 0.4901\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 36.23397\n",
      "Epoch: 0010 loss= 26.90117\n",
      "Epoch: 0020 loss= 19.87727\n",
      "Epoch: 0030 loss= 15.22818\n",
      "Epoch: 0040 loss= 11.90355\n",
      "Epoch: 0050 loss= 6.82045\n",
      "Epoch: 0060 loss= 2.14924\n",
      "Epoch: 0070 loss= 1.06985\n",
      "Epoch: 0080 loss= 0.75501\n",
      "Epoch: 0090 loss= 0.88825\n",
      "Epoch: 0100 loss= 0.67324\n",
      "Epoch: 0110 loss= 0.63991\n",
      "Epoch: 0120 loss= 0.60692\n",
      "Epoch: 0130 loss= 0.69814\n",
      "Epoch: 0140 loss= 0.59540\n",
      "Epoch: 0150 loss= 0.54465\n",
      "Epoch: 0160 loss= 0.66323\n",
      "Epoch: 0170 loss= 0.71100\n",
      "Epoch: 0180 loss= 0.52948\n",
      "Epoch: 0190 loss= 0.53572\n",
      "Epoch: 0200 loss= 0.60856\n",
      "Epoch: 0210 loss= 0.57210\n",
      "Epoch: 0220 loss= 0.61477\n",
      "Epoch: 0230 loss= 0.67811\n",
      "Epoch: 0240 loss= 0.56939\n",
      "Epoch: 0250 loss= 0.54642\n",
      "Epoch: 0260 loss= 0.62267\n",
      "Epoch: 0270 loss= 0.65880\n",
      "Epoch: 0280 loss= 0.59095\n",
      "Epoch: 0290 loss= 0.53844\n",
      "Epoch: 0300 loss= 0.58203\n",
      "Epoch: 0310 loss= 0.54537\n",
      "Epoch: 0320 loss= 0.56969\n",
      "Epoch: 0330 loss= 0.56328\n",
      "Epoch: 0340 loss= 0.56745\n",
      "Epoch: 0350 loss= 0.61225\n",
      "Epoch: 0360 loss= 0.62016\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7633,  4.7381, -5.3095, -4.6209, -4.3673,  5.6474,  5.4476, -5.2219]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 36.42982\n",
      "Epoch: 0010 loss= 19.02233\n",
      "Epoch: 0020 loss= 11.94447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0030 loss= 4.25458\n",
      "Epoch: 0040 loss= 1.27438\n",
      "Epoch: 0050 loss= 0.87749\n",
      "Epoch: 0060 loss= 0.80064\n",
      "Epoch: 0070 loss= 0.68512\n",
      "Epoch: 0080 loss= 1.12678\n",
      "Epoch: 0090 loss= 0.67713\n",
      "Epoch: 0100 loss= 0.65629\n",
      "Epoch: 0110 loss= 0.66010\n",
      "Epoch: 0120 loss= 0.59068\n",
      "Epoch: 0130 loss= 0.67171\n",
      "Epoch: 0140 loss= 0.59263\n",
      "Epoch: 0150 loss= 0.66711\n",
      "Epoch: 0160 loss= 0.67503\n",
      "Epoch: 0170 loss= 0.55934\n",
      "Epoch: 0180 loss= 0.54927\n",
      "Epoch: 0190 loss= 0.59628\n",
      "Epoch: 0200 loss= 0.65954\n",
      "Epoch: 0210 loss= 0.64192\n",
      "Epoch: 0220 loss= 0.56661\n",
      "Epoch: 0230 loss= 0.62024\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4859\n",
      "timestep:1, pyg_AUC: 0.4901\n",
      "timestep:2, pyg_AUC: 0.4873\n",
      "timestep:3, pyg_AUC: 0.4887\n",
      "timestep:4, pyg_AUC: 0.4873\n",
      "timestep:5, pyg_AUC: 0.4887\n",
      "timestep:6, pyg_AUC: 0.4901\n",
      "timestep:7, pyg_AUC: 0.4859\n",
      "timestep:8, pyg_AUC: 0.4859\n",
      "timestep:9, pyg_AUC: 0.4887\n",
      "timestep:10, pyg_AUC: 0.4901\n",
      "timestep:11, pyg_AUC: 0.4887\n",
      "timestep:12, pyg_AUC: 0.4887\n",
      "timestep:13, pyg_AUC: 0.4859\n",
      "timestep:14, pyg_AUC: 0.4887\n",
      "timestep:15, pyg_AUC: 0.4873\n",
      "timestep:16, pyg_AUC: 0.4901\n",
      "timestep:17, pyg_AUC: 0.4901\n",
      "timestep:18, pyg_AUC: 0.4887\n",
      "timestep:19, pyg_AUC: 0.4915\n",
      "timestep:20, pyg_AUC: 0.4901\n",
      "timestep:21, pyg_AUC: 0.4944\n",
      "timestep:22, pyg_AUC: 0.4915\n",
      "timestep:23, pyg_AUC: 0.4901\n",
      "timestep:24, pyg_AUC: 0.4901\n",
      "timestep:25, pyg_AUC: 0.4873\n",
      "timestep:26, pyg_AUC: 0.4887\n",
      "timestep:27, pyg_AUC: 0.4887\n",
      "timestep:28, pyg_AUC: 0.4887\n",
      "timestep:29, pyg_AUC: 0.4915\n",
      "timestep:30, pyg_AUC: 0.4915\n",
      "timestep:31, pyg_AUC: 0.4873\n",
      "timestep:32, pyg_AUC: 0.4901\n",
      "timestep:33, pyg_AUC: 0.4915\n",
      "timestep:34, pyg_AUC: 0.4873\n",
      "timestep:35, pyg_AUC: 0.4915\n",
      "timestep:36, pyg_AUC: 0.4859\n",
      "timestep:37, pyg_AUC: 0.4887\n",
      "timestep:38, pyg_AUC: 0.4901\n",
      "timestep:39, pyg_AUC: 0.4901\n",
      "timestep:40, pyg_AUC: 0.4859\n",
      "timestep:41, pyg_AUC: 0.4887\n",
      "timestep:42, pyg_AUC: 0.4901\n",
      "timestep:43, pyg_AUC: 0.4901\n",
      "timestep:44, pyg_AUC: 0.4873\n",
      "timestep:45, pyg_AUC: 0.4887\n",
      "timestep:46, pyg_AUC: 0.4887\n",
      "timestep:47, pyg_AUC: 0.4887\n",
      "timestep:48, pyg_AUC: 0.4915\n",
      "timestep:49, pyg_AUC: 0.4859\n",
      "timestep:50, pyg_AUC: 0.4901\n",
      "timestep:51, pyg_AUC: 0.4901\n",
      "timestep:52, pyg_AUC: 0.4887\n",
      "timestep:53, pyg_AUC: 0.4873\n",
      "timestep:54, pyg_AUC: 0.4831\n",
      "timestep:55, pyg_AUC: 0.4901\n",
      "timestep:56, pyg_AUC: 0.4873\n",
      "timestep:57, pyg_AUC: 0.4887\n",
      "timestep:58, pyg_AUC: 0.4873\n",
      "timestep:59, pyg_AUC: 0.4915\n",
      "timestep:60, pyg_AUC: 0.4873\n",
      "timestep:61, pyg_AUC: 0.4901\n",
      "timestep:62, pyg_AUC: 0.4859\n",
      "timestep:63, pyg_AUC: 0.4901\n",
      "timestep:64, pyg_AUC: 0.4901\n",
      "timestep:65, pyg_AUC: 0.4929\n",
      "timestep:66, pyg_AUC: 0.4859\n",
      "timestep:67, pyg_AUC: 0.4887\n",
      "timestep:68, pyg_AUC: 0.4901\n",
      "timestep:69, pyg_AUC: 0.4873\n",
      "timestep:70, pyg_AUC: 0.4845\n",
      "timestep:71, pyg_AUC: 0.4901\n",
      "timestep:72, pyg_AUC: 0.4929\n",
      "timestep:73, pyg_AUC: 0.4887\n",
      "timestep:74, pyg_AUC: 0.4915\n",
      "timestep:75, pyg_AUC: 0.4887\n",
      "timestep:76, pyg_AUC: 0.4859\n",
      "timestep:77, pyg_AUC: 0.4901\n",
      "timestep:78, pyg_AUC: 0.4831\n",
      "timestep:79, pyg_AUC: 0.4873\n",
      "timestep:80, pyg_AUC: 0.4887\n",
      "timestep:81, pyg_AUC: 0.4901\n",
      "timestep:82, pyg_AUC: 0.4901\n",
      "timestep:83, pyg_AUC: 0.4929\n",
      "timestep:84, pyg_AUC: 0.4901\n",
      "timestep:85, pyg_AUC: 0.4901\n",
      "timestep:86, pyg_AUC: 0.4887\n",
      "timestep:87, pyg_AUC: 0.4915\n",
      "timestep:88, pyg_AUC: 0.4901\n",
      "timestep:89, pyg_AUC: 0.4873\n",
      "timestep:90, pyg_AUC: 0.4873\n",
      "timestep:91, pyg_AUC: 0.4901\n",
      "timestep:92, pyg_AUC: 0.4901\n",
      "timestep:93, pyg_AUC: 0.4887\n",
      "timestep:94, pyg_AUC: 0.4887\n",
      "timestep:95, pyg_AUC: 0.4845\n",
      "timestep:96, pyg_AUC: 0.4915\n",
      "timestep:97, pyg_AUC: 0.4887\n",
      "timestep:98, pyg_AUC: 0.4915\n",
      "timestep:99, pyg_AUC: 0.4901\n",
      "timestep:100, pyg_AUC: 0.4901\n",
      "timestep:101, pyg_AUC: 0.4887\n",
      "timestep:102, pyg_AUC: 0.4901\n",
      "timestep:103, pyg_AUC: 0.4915\n",
      "timestep:104, pyg_AUC: 0.4944\n",
      "timestep:105, pyg_AUC: 0.4915\n",
      "timestep:106, pyg_AUC: 0.4901\n",
      "timestep:107, pyg_AUC: 0.4859\n",
      "timestep:108, pyg_AUC: 0.4915\n",
      "timestep:109, pyg_AUC: 0.4859\n",
      "timestep:110, pyg_AUC: 0.4887\n",
      "timestep:111, pyg_AUC: 0.4929\n",
      "timestep:112, pyg_AUC: 0.4929\n",
      "timestep:113, pyg_AUC: 0.4873\n",
      "timestep:114, pyg_AUC: 0.4887\n",
      "timestep:115, pyg_AUC: 0.4901\n",
      "timestep:116, pyg_AUC: 0.4901\n",
      "timestep:117, pyg_AUC: 0.4873\n",
      "timestep:118, pyg_AUC: 0.4915\n",
      "timestep:119, pyg_AUC: 0.4859\n",
      "timestep:120, pyg_AUC: 0.4859\n",
      "timestep:121, pyg_AUC: 0.4887\n",
      "timestep:122, pyg_AUC: 0.4929\n",
      "timestep:123, pyg_AUC: 0.4859\n",
      "timestep:124, pyg_AUC: 0.4929\n",
      "timestep:125, pyg_AUC: 0.4873\n",
      "timestep:126, pyg_AUC: 0.4887\n",
      "timestep:127, pyg_AUC: 0.4944\n",
      "timestep:128, pyg_AUC: 0.4873\n",
      "timestep:129, pyg_AUC: 0.4901\n",
      "timestep:130, pyg_AUC: 0.4859\n",
      "timestep:131, pyg_AUC: 0.4901\n",
      "timestep:132, pyg_AUC: 0.4901\n",
      "timestep:133, pyg_AUC: 0.4873\n",
      "timestep:134, pyg_AUC: 0.4915\n",
      "timestep:135, pyg_AUC: 0.4887\n",
      "timestep:136, pyg_AUC: 0.4887\n",
      "timestep:137, pyg_AUC: 0.4901\n",
      "timestep:138, pyg_AUC: 0.4901\n",
      "timestep:139, pyg_AUC: 0.4873\n",
      "timestep:140, pyg_AUC: 0.4845\n",
      "timestep:141, pyg_AUC: 0.4873\n",
      "timestep:142, pyg_AUC: 0.4859\n",
      "timestep:143, pyg_AUC: 0.4901\n",
      "timestep:144, pyg_AUC: 0.4859\n",
      "timestep:145, pyg_AUC: 0.4859\n",
      "timestep:146, pyg_AUC: 0.4873\n",
      "timestep:147, pyg_AUC: 0.4901\n",
      "timestep:148, pyg_AUC: 0.4859\n",
      "timestep:149, pyg_AUC: 0.4901\n",
      "timestep:150, pyg_AUC: 0.4929\n",
      "timestep:151, pyg_AUC: 0.4845\n",
      "timestep:152, pyg_AUC: 0.4873\n",
      "timestep:153, pyg_AUC: 0.4915\n",
      "timestep:154, pyg_AUC: 0.4859\n",
      "timestep:155, pyg_AUC: 0.4901\n",
      "timestep:156, pyg_AUC: 0.4887\n",
      "timestep:157, pyg_AUC: 0.4873\n",
      "timestep:158, pyg_AUC: 0.4887\n",
      "timestep:159, pyg_AUC: 0.4901\n",
      "timestep:160, pyg_AUC: 0.4873\n",
      "timestep:161, pyg_AUC: 0.4873\n",
      "timestep:162, pyg_AUC: 0.4873\n",
      "timestep:163, pyg_AUC: 0.4901\n",
      "timestep:164, pyg_AUC: 0.4873\n",
      "timestep:165, pyg_AUC: 0.4915\n",
      "timestep:166, pyg_AUC: 0.4859\n",
      "timestep:167, pyg_AUC: 0.4831\n",
      "timestep:168, pyg_AUC: 0.4859\n",
      "timestep:169, pyg_AUC: 0.4873\n",
      "timestep:170, pyg_AUC: 0.4887\n",
      "timestep:171, pyg_AUC: 0.4873\n",
      "timestep:172, pyg_AUC: 0.4944\n",
      "timestep:173, pyg_AUC: 0.4816\n",
      "timestep:174, pyg_AUC: 0.4887\n",
      "timestep:175, pyg_AUC: 0.4958\n",
      "timestep:176, pyg_AUC: 0.4887\n",
      "timestep:177, pyg_AUC: 0.4873\n",
      "timestep:178, pyg_AUC: 0.4887\n",
      "timestep:179, pyg_AUC: 0.4944\n",
      "timestep:180, pyg_AUC: 0.4901\n",
      "timestep:181, pyg_AUC: 0.4887\n",
      "timestep:182, pyg_AUC: 0.4887\n",
      "timestep:183, pyg_AUC: 0.4873\n",
      "timestep:184, pyg_AUC: 0.4901\n",
      "timestep:185, pyg_AUC: 0.4887\n",
      "timestep:186, pyg_AUC: 0.4887\n",
      "timestep:187, pyg_AUC: 0.4887\n",
      "timestep:188, pyg_AUC: 0.4929\n",
      "timestep:189, pyg_AUC: 0.4887\n",
      "timestep:190, pyg_AUC: 0.4915\n",
      "timestep:191, pyg_AUC: 0.4901\n",
      "timestep:192, pyg_AUC: 0.4859\n",
      "timestep:193, pyg_AUC: 0.4873\n",
      "timestep:194, pyg_AUC: 0.4915\n",
      "timestep:195, pyg_AUC: 0.4915\n",
      "timestep:196, pyg_AUC: 0.4901\n",
      "timestep:197, pyg_AUC: 0.4901\n",
      "timestep:198, pyg_AUC: 0.4901\n",
      "timestep:199, pyg_AUC: 0.4915\n",
      "timestep:200, pyg_AUC: 0.4929\n",
      "timestep:201, pyg_AUC: 0.4915\n",
      "timestep:202, pyg_AUC: 0.4873\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4915\n",
      "timestep:205, pyg_AUC: 0.4901\n",
      "timestep:206, pyg_AUC: 0.4915\n",
      "timestep:207, pyg_AUC: 0.4859\n",
      "timestep:208, pyg_AUC: 0.4901\n",
      "timestep:209, pyg_AUC: 0.4859\n",
      "timestep:210, pyg_AUC: 0.4915\n",
      "timestep:211, pyg_AUC: 0.4929\n",
      "timestep:212, pyg_AUC: 0.4887\n",
      "timestep:213, pyg_AUC: 0.4901\n",
      "timestep:214, pyg_AUC: 0.4929\n",
      "timestep:215, pyg_AUC: 0.4845\n",
      "timestep:216, pyg_AUC: 0.4901\n",
      "timestep:217, pyg_AUC: 0.4929\n",
      "timestep:218, pyg_AUC: 0.4929\n",
      "timestep:219, pyg_AUC: 0.4944\n",
      "timestep:220, pyg_AUC: 0.4901\n",
      "timestep:221, pyg_AUC: 0.4929\n",
      "timestep:222, pyg_AUC: 0.4873\n",
      "timestep:223, pyg_AUC: 0.4929\n",
      "timestep:224, pyg_AUC: 0.4915\n",
      "timestep:225, pyg_AUC: 0.4915\n",
      "timestep:226, pyg_AUC: 0.4901\n",
      "timestep:227, pyg_AUC: 0.4944\n",
      "timestep:228, pyg_AUC: 0.4901\n",
      "timestep:229, pyg_AUC: 0.4901\n",
      "timestep:230, pyg_AUC: 0.4915\n",
      "timestep:231, pyg_AUC: 0.4873\n",
      "timestep:232, pyg_AUC: 0.4915\n",
      "timestep:233, pyg_AUC: 0.4929\n",
      "timestep:234, pyg_AUC: 0.4915\n",
      "timestep:235, pyg_AUC: 0.4887\n",
      "timestep:236, pyg_AUC: 0.4929\n",
      "timestep:237, pyg_AUC: 0.4915\n",
      "timestep:238, pyg_AUC: 0.4929\n",
      "timestep:239, pyg_AUC: 0.4929\n",
      "timestep:240, pyg_AUC: 0.4901\n",
      "timestep:241, pyg_AUC: 0.4873\n",
      "timestep:242, pyg_AUC: 0.4915\n",
      "timestep:243, pyg_AUC: 0.4873\n",
      "timestep:244, pyg_AUC: 0.4929\n",
      "timestep:245, pyg_AUC: 0.4901\n",
      "timestep:246, pyg_AUC: 0.4929\n",
      "timestep:247, pyg_AUC: 0.4915\n",
      "timestep:248, pyg_AUC: 0.4915\n",
      "timestep:249, pyg_AUC: 0.4887\n",
      "timestep:250, pyg_AUC: 0.4901\n",
      "timestep:251, pyg_AUC: 0.4887\n",
      "timestep:252, pyg_AUC: 0.4901\n",
      "timestep:253, pyg_AUC: 0.4859\n",
      "timestep:254, pyg_AUC: 0.4887\n",
      "timestep:255, pyg_AUC: 0.4915\n",
      "timestep:256, pyg_AUC: 0.4845\n",
      "timestep:257, pyg_AUC: 0.4887\n",
      "timestep:258, pyg_AUC: 0.4915\n",
      "timestep:259, pyg_AUC: 0.4887\n",
      "timestep:260, pyg_AUC: 0.4915\n",
      "timestep:261, pyg_AUC: 0.4901\n",
      "timestep:262, pyg_AUC: 0.4915\n",
      "timestep:263, pyg_AUC: 0.4901\n",
      "timestep:264, pyg_AUC: 0.4901\n",
      "timestep:265, pyg_AUC: 0.4887\n",
      "timestep:266, pyg_AUC: 0.4901\n",
      "timestep:267, pyg_AUC: 0.4845\n",
      "timestep:268, pyg_AUC: 0.4915\n",
      "timestep:269, pyg_AUC: 0.4915\n",
      "timestep:270, pyg_AUC: 0.4887\n",
      "timestep:271, pyg_AUC: 0.4915\n",
      "timestep:272, pyg_AUC: 0.4859\n",
      "timestep:273, pyg_AUC: 0.4887\n",
      "timestep:274, pyg_AUC: 0.4873\n",
      "timestep:275, pyg_AUC: 0.4887\n",
      "timestep:276, pyg_AUC: 0.4901\n",
      "timestep:277, pyg_AUC: 0.4901\n",
      "timestep:278, pyg_AUC: 0.4887\n",
      "timestep:279, pyg_AUC: 0.4831\n",
      "timestep:280, pyg_AUC: 0.4887\n",
      "timestep:281, pyg_AUC: 0.4887\n",
      "timestep:282, pyg_AUC: 0.4859\n",
      "timestep:283, pyg_AUC: 0.4887\n",
      "timestep:284, pyg_AUC: 0.4887\n",
      "timestep:285, pyg_AUC: 0.4873\n",
      "timestep:286, pyg_AUC: 0.4901\n",
      "timestep:287, pyg_AUC: 0.4901\n",
      "timestep:288, pyg_AUC: 0.4859\n",
      "timestep:289, pyg_AUC: 0.4873\n",
      "timestep:290, pyg_AUC: 0.4873\n",
      "timestep:291, pyg_AUC: 0.4887\n",
      "timestep:292, pyg_AUC: 0.4901\n",
      "timestep:293, pyg_AUC: 0.4873\n",
      "timestep:294, pyg_AUC: 0.4887\n",
      "timestep:295, pyg_AUC: 0.4873\n",
      "timestep:296, pyg_AUC: 0.4915\n",
      "timestep:297, pyg_AUC: 0.4901\n",
      "timestep:298, pyg_AUC: 0.4873\n",
      "timestep:299, pyg_AUC: 0.4873\n",
      "timestep:300, pyg_AUC: 0.4845\n",
      "timestep:301, pyg_AUC: 0.4845\n",
      "timestep:302, pyg_AUC: 0.4873\n",
      "timestep:303, pyg_AUC: 0.4859\n",
      "timestep:304, pyg_AUC: 0.4859\n",
      "timestep:305, pyg_AUC: 0.4873\n",
      "timestep:306, pyg_AUC: 0.4845\n",
      "timestep:307, pyg_AUC: 0.4887\n",
      "timestep:308, pyg_AUC: 0.4887\n",
      "timestep:309, pyg_AUC: 0.4873\n",
      "timestep:310, pyg_AUC: 0.4873\n",
      "timestep:311, pyg_AUC: 0.4887\n",
      "timestep:312, pyg_AUC: 0.4816\n",
      "timestep:313, pyg_AUC: 0.4901\n",
      "timestep:314, pyg_AUC: 0.4915\n",
      "timestep:315, pyg_AUC: 0.4915\n",
      "timestep:316, pyg_AUC: 0.4887\n",
      "timestep:317, pyg_AUC: 0.4873\n",
      "timestep:318, pyg_AUC: 0.4887\n",
      "timestep:319, pyg_AUC: 0.4873\n",
      "timestep:320, pyg_AUC: 0.4873\n",
      "timestep:321, pyg_AUC: 0.4887\n",
      "timestep:322, pyg_AUC: 0.4887\n",
      "timestep:323, pyg_AUC: 0.4887\n",
      "timestep:324, pyg_AUC: 0.4873\n",
      "timestep:325, pyg_AUC: 0.4873\n",
      "timestep:326, pyg_AUC: 0.4901\n",
      "timestep:327, pyg_AUC: 0.4887\n",
      "timestep:328, pyg_AUC: 0.4887\n",
      "timestep:329, pyg_AUC: 0.4901\n",
      "timestep:330, pyg_AUC: 0.4915\n",
      "timestep:331, pyg_AUC: 0.4887\n",
      "timestep:332, pyg_AUC: 0.4887\n",
      "timestep:333, pyg_AUC: 0.4915\n",
      "timestep:334, pyg_AUC: 0.4901\n",
      "timestep:335, pyg_AUC: 0.4845\n",
      "timestep:336, pyg_AUC: 0.4887\n",
      "timestep:337, pyg_AUC: 0.4915\n",
      "timestep:338, pyg_AUC: 0.4887\n",
      "timestep:339, pyg_AUC: 0.4915\n",
      "timestep:340, pyg_AUC: 0.4901\n",
      "timestep:341, pyg_AUC: 0.4887\n",
      "timestep:342, pyg_AUC: 0.4887\n",
      "timestep:343, pyg_AUC: 0.4887\n",
      "timestep:344, pyg_AUC: 0.4859\n",
      "timestep:345, pyg_AUC: 0.4901\n",
      "timestep:346, pyg_AUC: 0.4859\n",
      "timestep:347, pyg_AUC: 0.4859\n",
      "timestep:348, pyg_AUC: 0.4873\n",
      "timestep:349, pyg_AUC: 0.4859\n",
      "timestep:350, pyg_AUC: 0.4859\n",
      "timestep:351, pyg_AUC: 0.4873\n",
      "timestep:352, pyg_AUC: 0.4915\n",
      "timestep:353, pyg_AUC: 0.4901\n",
      "timestep:354, pyg_AUC: 0.4873\n",
      "timestep:355, pyg_AUC: 0.4859\n",
      "timestep:356, pyg_AUC: 0.4831\n",
      "timestep:357, pyg_AUC: 0.4901\n",
      "timestep:358, pyg_AUC: 0.4873\n",
      "timestep:359, pyg_AUC: 0.4859\n",
      "timestep:360, pyg_AUC: 0.4915\n",
      "timestep:361, pyg_AUC: 0.4887\n",
      "timestep:362, pyg_AUC: 0.4915\n",
      "timestep:363, pyg_AUC: 0.4873\n",
      "timestep:364, pyg_AUC: 0.4887\n",
      "timestep:365, pyg_AUC: 0.4901\n",
      "timestep:366, pyg_AUC: 0.4859\n",
      "timestep:367, pyg_AUC: 0.4873\n",
      "timestep:368, pyg_AUC: 0.4901\n",
      "timestep:369, pyg_AUC: 0.4915\n",
      "timestep:370, pyg_AUC: 0.4873\n",
      "timestep:371, pyg_AUC: 0.4873\n",
      "timestep:372, pyg_AUC: 0.4873\n",
      "timestep:373, pyg_AUC: 0.4901\n",
      "timestep:374, pyg_AUC: 0.4873\n",
      "timestep:375, pyg_AUC: 0.4859\n",
      "timestep:376, pyg_AUC: 0.4915\n",
      "timestep:377, pyg_AUC: 0.4887\n",
      "timestep:378, pyg_AUC: 0.4873\n",
      "timestep:379, pyg_AUC: 0.4901\n",
      "timestep:380, pyg_AUC: 0.4887\n",
      "timestep:381, pyg_AUC: 0.4901\n",
      "timestep:382, pyg_AUC: 0.4873\n",
      "timestep:383, pyg_AUC: 0.4873\n",
      "timestep:384, pyg_AUC: 0.4887\n",
      "timestep:385, pyg_AUC: 0.4845\n",
      "timestep:386, pyg_AUC: 0.4831\n",
      "timestep:387, pyg_AUC: 0.4845\n",
      "timestep:388, pyg_AUC: 0.4873\n",
      "timestep:389, pyg_AUC: 0.4845\n",
      "timestep:390, pyg_AUC: 0.4859\n",
      "timestep:391, pyg_AUC: 0.4901\n",
      "timestep:392, pyg_AUC: 0.4887\n",
      "timestep:393, pyg_AUC: 0.4859\n",
      "timestep:394, pyg_AUC: 0.4873\n",
      "timestep:395, pyg_AUC: 0.4887\n",
      "timestep:396, pyg_AUC: 0.4887\n",
      "timestep:397, pyg_AUC: 0.4873\n",
      "timestep:398, pyg_AUC: 0.4887\n",
      "timestep:399, pyg_AUC: 0.4887\n",
      "timestep:400, pyg_AUC: 0.4859\n",
      "timestep:401, pyg_AUC: 0.4831\n",
      "timestep:402, pyg_AUC: 0.4831\n",
      "timestep:403, pyg_AUC: 0.4873\n",
      "timestep:404, pyg_AUC: 0.4887\n",
      "timestep:405, pyg_AUC: 0.4845\n",
      "timestep:406, pyg_AUC: 0.4873\n",
      "timestep:407, pyg_AUC: 0.4859\n",
      "timestep:408, pyg_AUC: 0.4873\n",
      "timestep:409, pyg_AUC: 0.4887\n",
      "timestep:410, pyg_AUC: 0.4873\n",
      "timestep:411, pyg_AUC: 0.4901\n",
      "timestep:412, pyg_AUC: 0.4845\n",
      "timestep:413, pyg_AUC: 0.4859\n",
      "timestep:414, pyg_AUC: 0.4831\n",
      "timestep:415, pyg_AUC: 0.4816\n",
      "timestep:416, pyg_AUC: 0.4873\n",
      "timestep:417, pyg_AUC: 0.4901\n",
      "timestep:418, pyg_AUC: 0.4859\n",
      "timestep:419, pyg_AUC: 0.4859\n",
      "timestep:420, pyg_AUC: 0.4887\n",
      "timestep:421, pyg_AUC: 0.4873\n",
      "timestep:422, pyg_AUC: 0.4816\n",
      "timestep:423, pyg_AUC: 0.4887\n",
      "timestep:424, pyg_AUC: 0.4873\n",
      "timestep:425, pyg_AUC: 0.4887\n",
      "timestep:426, pyg_AUC: 0.4859\n",
      "timestep:427, pyg_AUC: 0.4887\n",
      "timestep:428, pyg_AUC: 0.4845\n",
      "timestep:429, pyg_AUC: 0.4873\n",
      "timestep:430, pyg_AUC: 0.4859\n",
      "timestep:431, pyg_AUC: 0.4859\n",
      "timestep:432, pyg_AUC: 0.4859\n",
      "timestep:433, pyg_AUC: 0.4887\n",
      "timestep:434, pyg_AUC: 0.4887\n",
      "timestep:435, pyg_AUC: 0.4873\n",
      "timestep:436, pyg_AUC: 0.4887\n",
      "timestep:437, pyg_AUC: 0.4887\n",
      "timestep:438, pyg_AUC: 0.4873\n",
      "timestep:439, pyg_AUC: 0.4873\n",
      "timestep:440, pyg_AUC: 0.4873\n",
      "timestep:441, pyg_AUC: 0.4887\n",
      "timestep:442, pyg_AUC: 0.4873\n",
      "timestep:443, pyg_AUC: 0.4873\n",
      "timestep:444, pyg_AUC: 0.4887\n",
      "timestep:445, pyg_AUC: 0.4873\n",
      "timestep:446, pyg_AUC: 0.4845\n",
      "timestep:447, pyg_AUC: 0.4901\n",
      "timestep:448, pyg_AUC: 0.4887\n",
      "timestep:449, pyg_AUC: 0.4873\n",
      "timestep:450, pyg_AUC: 0.4845\n",
      "timestep:451, pyg_AUC: 0.4887\n",
      "timestep:452, pyg_AUC: 0.4887\n",
      "timestep:453, pyg_AUC: 0.4873\n",
      "timestep:454, pyg_AUC: 0.4845\n",
      "timestep:455, pyg_AUC: 0.4831\n",
      "timestep:456, pyg_AUC: 0.4873\n",
      "timestep:457, pyg_AUC: 0.4859\n",
      "timestep:458, pyg_AUC: 0.4859\n",
      "timestep:459, pyg_AUC: 0.4873\n",
      "timestep:460, pyg_AUC: 0.4859\n",
      "timestep:461, pyg_AUC: 0.4873\n",
      "timestep:462, pyg_AUC: 0.4901\n",
      "timestep:463, pyg_AUC: 0.4873\n",
      "timestep:464, pyg_AUC: 0.4873\n",
      "timestep:465, pyg_AUC: 0.4831\n",
      "timestep:466, pyg_AUC: 0.4859\n",
      "timestep:467, pyg_AUC: 0.4887\n",
      "timestep:468, pyg_AUC: 0.4887\n",
      "timestep:469, pyg_AUC: 0.4859\n",
      "timestep:470, pyg_AUC: 0.4901\n",
      "timestep:471, pyg_AUC: 0.4873\n",
      "timestep:472, pyg_AUC: 0.4887\n",
      "timestep:473, pyg_AUC: 0.4859\n",
      "timestep:474, pyg_AUC: 0.4873\n",
      "timestep:475, pyg_AUC: 0.4845\n",
      "timestep:476, pyg_AUC: 0.4873\n",
      "timestep:477, pyg_AUC: 0.4887\n",
      "timestep:478, pyg_AUC: 0.4901\n",
      "timestep:479, pyg_AUC: 0.4859\n",
      "timestep:480, pyg_AUC: 0.4887\n",
      "timestep:481, pyg_AUC: 0.4887\n",
      "timestep:482, pyg_AUC: 0.4887\n",
      "timestep:483, pyg_AUC: 0.4859\n",
      "timestep:484, pyg_AUC: 0.4845\n",
      "timestep:485, pyg_AUC: 0.4901\n",
      "timestep:486, pyg_AUC: 0.4873\n",
      "timestep:487, pyg_AUC: 0.4873\n",
      "timestep:488, pyg_AUC: 0.4901\n",
      "timestep:489, pyg_AUC: 0.4915\n",
      "timestep:490, pyg_AUC: 0.4901\n",
      "timestep:491, pyg_AUC: 0.4901\n",
      "timestep:492, pyg_AUC: 0.4887\n",
      "timestep:493, pyg_AUC: 0.4901\n",
      "timestep:494, pyg_AUC: 0.4887\n",
      "timestep:495, pyg_AUC: 0.4859\n",
      "timestep:496, pyg_AUC: 0.4859\n",
      "timestep:497, pyg_AUC: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [21:57<11:59, 102.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:498, pyg_AUC: 0.4859\n",
      "timestep:499, pyg_AUC: 0.4887\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 40.11938\n",
      "Epoch: 0010 loss= 24.70359\n",
      "Epoch: 0020 loss= 19.34404\n",
      "Epoch: 0030 loss= 15.07860\n",
      "Epoch: 0040 loss= 14.36049\n",
      "Epoch: 0050 loss= 5.95601\n",
      "Epoch: 0060 loss= 2.74010\n",
      "Epoch: 0070 loss= 1.84789\n",
      "Epoch: 0080 loss= 0.80541\n",
      "Epoch: 0090 loss= 0.71407\n",
      "Epoch: 0100 loss= 0.71716\n",
      "Epoch: 0110 loss= 0.60542\n",
      "Epoch: 0120 loss= 0.61667\n",
      "Epoch: 0130 loss= 0.61614\n",
      "Epoch: 0140 loss= 0.67708\n",
      "Epoch: 0150 loss= 0.64543\n",
      "Epoch: 0160 loss= 0.60725\n",
      "Epoch: 0170 loss= 0.59986\n",
      "Epoch: 0180 loss= 0.60719\n",
      "Epoch: 0190 loss= 0.61792\n",
      "Epoch: 0200 loss= 0.60540\n",
      "Epoch: 0210 loss= 0.59125\n",
      "Epoch: 0220 loss= 0.60447\n",
      "Epoch: 0230 loss= 0.61284\n",
      "Epoch: 0240 loss= 0.51267\n",
      "Epoch: 0250 loss= 0.55766\n",
      "Epoch: 0260 loss= 0.55647\n",
      "Epoch: 0270 loss= 0.53824\n",
      "Epoch: 0280 loss= 0.53275\n",
      "Epoch: 0290 loss= 0.59926\n",
      "Epoch: 0300 loss= 0.55750\n",
      "Epoch: 0310 loss= 0.55885\n",
      "Epoch: 0320 loss= 0.53951\n",
      "Epoch: 0330 loss= 0.52188\n",
      "Epoch: 0340 loss= 0.51224\n",
      "Epoch: 0350 loss= 0.64158\n",
      "Epoch: 0360 loss= 0.57333\n",
      "Epoch: 0370 loss= 0.59912\n",
      "Epoch: 0380 loss= 0.54356\n",
      "Epoch: 0390 loss= 0.50744\n",
      "Epoch: 0400 loss= 0.58177\n",
      "Epoch: 0410 loss= 0.54769\n",
      "Epoch: 0420 loss= 0.51758\n",
      "Epoch: 0430 loss= 0.54030\n",
      "Epoch: 0440 loss= 0.48448\n",
      "Epoch: 0450 loss= 0.57850\n",
      "Epoch: 0460 loss= 0.53484\n",
      "Epoch: 0470 loss= 0.60781\n",
      "Epoch: 0480 loss= 0.59702\n",
      "Epoch: 0490 loss= 0.53762\n",
      "Epoch: 0500 loss= 0.55733\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7629,  4.7067, -5.3664, -4.6428, -4.4185,  5.6322,  5.4661, -5.2624]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 35.86526\n",
      "Epoch: 0010 loss= 18.55354\n",
      "Epoch: 0020 loss= 12.27443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0030 loss= 6.72635\n",
      "Epoch: 0040 loss= 3.04723\n",
      "Epoch: 0050 loss= 0.92867\n",
      "Epoch: 0060 loss= 1.79798\n",
      "Epoch: 0070 loss= 0.73109\n",
      "Epoch: 0080 loss= 0.97270\n",
      "Epoch: 0090 loss= 0.72456\n",
      "Epoch: 0100 loss= 0.59531\n",
      "Epoch: 0110 loss= 0.62335\n",
      "Epoch: 0120 loss= 0.65345\n",
      "Epoch: 0130 loss= 0.55997\n",
      "Epoch: 0140 loss= 0.58046\n",
      "Epoch: 0150 loss= 0.49292\n",
      "Epoch: 0160 loss= 0.63118\n",
      "Epoch: 0170 loss= 0.58131\n",
      "Epoch: 0180 loss= 0.55873\n",
      "Epoch: 0190 loss= 0.63147\n",
      "Epoch: 0200 loss= 0.56926\n",
      "Epoch: 0210 loss= 0.53257\n",
      "Epoch: 0220 loss= 0.61724\n",
      "Epoch: 0230 loss= 0.57747\n",
      "Epoch: 0240 loss= 0.55292\n",
      "Epoch: 0250 loss= 0.56004\n",
      "Epoch: 0260 loss= 0.55808\n",
      "Epoch: 0270 loss= 0.50165\n",
      "Epoch: 0280 loss= 0.60063\n",
      "Epoch: 0290 loss= 0.47494\n",
      "Epoch: 0300 loss= 0.52866\n",
      "Epoch: 0310 loss= 0.49927\n",
      "Epoch: 0320 loss= 0.56499\n",
      "Epoch: 0330 loss= 0.47230\n",
      "Epoch: 0340 loss= 0.50826\n",
      "Epoch: 0350 loss= 0.61077\n",
      "Epoch: 0360 loss= 0.55989\n",
      "Epoch: 0370 loss= 0.51098\n",
      "Epoch: 0380 loss= 0.48933\n",
      "Epoch: 0390 loss= 0.47726\n",
      "Epoch: 0400 loss= 0.46712\n",
      "Epoch: 0410 loss= 0.51527\n",
      "Epoch: 0420 loss= 0.51445\n",
      "Epoch: 0430 loss= 0.54226\n",
      "Epoch: 0440 loss= 0.46181\n",
      "Epoch: 0450 loss= 0.55194\n",
      "Epoch: 0460 loss= 0.48881\n",
      "Epoch: 0470 loss= 0.49275\n",
      "Epoch: 0480 loss= 0.47899\n",
      "Epoch: 0490 loss= 0.57519\n",
      "Epoch: 0500 loss= 0.47105\n",
      "Epoch: 0510 loss= 0.46323\n",
      "Epoch: 0520 loss= 0.52642\n",
      "Epoch: 0530 loss= 0.51080\n",
      "Epoch: 0540 loss= 0.50230\n",
      "Epoch: 0550 loss= 0.45938\n",
      "Epoch: 0560 loss= 0.48909\n",
      "Epoch: 0570 loss= 0.45952\n",
      "Epoch: 0580 loss= 0.49448\n",
      "Epoch: 0590 loss= 0.49243\n",
      "Epoch: 0600 loss= 0.47644\n",
      "Epoch: 0610 loss= 0.46992\n",
      "Epoch: 0620 loss= 0.45013\n",
      "Epoch: 0630 loss= 0.47626\n",
      "Epoch: 0640 loss= 0.54125\n",
      "Epoch: 0650 loss= 0.46372\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4816\n",
      "timestep:1, pyg_AUC: 0.4831\n",
      "timestep:2, pyg_AUC: 0.4901\n",
      "timestep:3, pyg_AUC: 0.4873\n",
      "timestep:4, pyg_AUC: 0.4845\n",
      "timestep:5, pyg_AUC: 0.4915\n",
      "timestep:6, pyg_AUC: 0.4887\n",
      "timestep:7, pyg_AUC: 0.4901\n",
      "timestep:8, pyg_AUC: 0.4915\n",
      "timestep:9, pyg_AUC: 0.4929\n",
      "timestep:10, pyg_AUC: 0.4915\n",
      "timestep:11, pyg_AUC: 0.4958\n",
      "timestep:12, pyg_AUC: 0.4873\n",
      "timestep:13, pyg_AUC: 0.4901\n",
      "timestep:14, pyg_AUC: 0.4901\n",
      "timestep:15, pyg_AUC: 0.4915\n",
      "timestep:16, pyg_AUC: 0.4901\n",
      "timestep:17, pyg_AUC: 0.4915\n",
      "timestep:18, pyg_AUC: 0.4859\n",
      "timestep:19, pyg_AUC: 0.4901\n",
      "timestep:20, pyg_AUC: 0.4887\n",
      "timestep:21, pyg_AUC: 0.4873\n",
      "timestep:22, pyg_AUC: 0.4887\n",
      "timestep:23, pyg_AUC: 0.4915\n",
      "timestep:24, pyg_AUC: 0.4901\n",
      "timestep:25, pyg_AUC: 0.4887\n",
      "timestep:26, pyg_AUC: 0.4915\n",
      "timestep:27, pyg_AUC: 0.4873\n",
      "timestep:28, pyg_AUC: 0.4901\n",
      "timestep:29, pyg_AUC: 0.4944\n",
      "timestep:30, pyg_AUC: 0.4915\n",
      "timestep:31, pyg_AUC: 0.4873\n",
      "timestep:32, pyg_AUC: 0.4915\n",
      "timestep:33, pyg_AUC: 0.4901\n",
      "timestep:34, pyg_AUC: 0.4915\n",
      "timestep:35, pyg_AUC: 0.4859\n",
      "timestep:36, pyg_AUC: 0.4901\n",
      "timestep:37, pyg_AUC: 0.4944\n",
      "timestep:38, pyg_AUC: 0.4901\n",
      "timestep:39, pyg_AUC: 0.4887\n",
      "timestep:40, pyg_AUC: 0.4901\n",
      "timestep:41, pyg_AUC: 0.4901\n",
      "timestep:42, pyg_AUC: 0.4887\n",
      "timestep:43, pyg_AUC: 0.4859\n",
      "timestep:44, pyg_AUC: 0.4901\n",
      "timestep:45, pyg_AUC: 0.4901\n",
      "timestep:46, pyg_AUC: 0.4887\n",
      "timestep:47, pyg_AUC: 0.4915\n",
      "timestep:48, pyg_AUC: 0.4887\n",
      "timestep:49, pyg_AUC: 0.4887\n",
      "timestep:50, pyg_AUC: 0.4901\n",
      "timestep:51, pyg_AUC: 0.4859\n",
      "timestep:52, pyg_AUC: 0.4887\n",
      "timestep:53, pyg_AUC: 0.4873\n",
      "timestep:54, pyg_AUC: 0.4873\n",
      "timestep:55, pyg_AUC: 0.4887\n",
      "timestep:56, pyg_AUC: 0.4944\n",
      "timestep:57, pyg_AUC: 0.4915\n",
      "timestep:58, pyg_AUC: 0.4915\n",
      "timestep:59, pyg_AUC: 0.4915\n",
      "timestep:60, pyg_AUC: 0.4901\n",
      "timestep:61, pyg_AUC: 0.4901\n",
      "timestep:62, pyg_AUC: 0.4901\n",
      "timestep:63, pyg_AUC: 0.4845\n",
      "timestep:64, pyg_AUC: 0.4901\n",
      "timestep:65, pyg_AUC: 0.4958\n",
      "timestep:66, pyg_AUC: 0.4859\n",
      "timestep:67, pyg_AUC: 0.4929\n",
      "timestep:68, pyg_AUC: 0.4887\n",
      "timestep:69, pyg_AUC: 0.4873\n",
      "timestep:70, pyg_AUC: 0.4845\n",
      "timestep:71, pyg_AUC: 0.4887\n",
      "timestep:72, pyg_AUC: 0.4915\n",
      "timestep:73, pyg_AUC: 0.4887\n",
      "timestep:74, pyg_AUC: 0.4901\n",
      "timestep:75, pyg_AUC: 0.4915\n",
      "timestep:76, pyg_AUC: 0.4901\n",
      "timestep:77, pyg_AUC: 0.4929\n",
      "timestep:78, pyg_AUC: 0.4901\n",
      "timestep:79, pyg_AUC: 0.4944\n",
      "timestep:80, pyg_AUC: 0.4901\n",
      "timestep:81, pyg_AUC: 0.4958\n",
      "timestep:82, pyg_AUC: 0.4915\n",
      "timestep:83, pyg_AUC: 0.4915\n",
      "timestep:84, pyg_AUC: 0.4873\n",
      "timestep:85, pyg_AUC: 0.4901\n",
      "timestep:86, pyg_AUC: 0.4915\n",
      "timestep:87, pyg_AUC: 0.4901\n",
      "timestep:88, pyg_AUC: 0.4915\n",
      "timestep:89, pyg_AUC: 0.4915\n",
      "timestep:90, pyg_AUC: 0.4929\n",
      "timestep:91, pyg_AUC: 0.4873\n",
      "timestep:92, pyg_AUC: 0.4873\n",
      "timestep:93, pyg_AUC: 0.4915\n",
      "timestep:94, pyg_AUC: 0.4901\n",
      "timestep:95, pyg_AUC: 0.4873\n",
      "timestep:96, pyg_AUC: 0.4887\n",
      "timestep:97, pyg_AUC: 0.4859\n",
      "timestep:98, pyg_AUC: 0.4887\n",
      "timestep:99, pyg_AUC: 0.4915\n",
      "timestep:100, pyg_AUC: 0.4929\n",
      "timestep:101, pyg_AUC: 0.4901\n",
      "timestep:102, pyg_AUC: 0.4831\n",
      "timestep:103, pyg_AUC: 0.4929\n",
      "timestep:104, pyg_AUC: 0.4887\n",
      "timestep:105, pyg_AUC: 0.4873\n",
      "timestep:106, pyg_AUC: 0.4887\n",
      "timestep:107, pyg_AUC: 0.4845\n",
      "timestep:108, pyg_AUC: 0.4845\n",
      "timestep:109, pyg_AUC: 0.4859\n",
      "timestep:110, pyg_AUC: 0.4958\n",
      "timestep:111, pyg_AUC: 0.4915\n",
      "timestep:112, pyg_AUC: 0.4901\n",
      "timestep:113, pyg_AUC: 0.4887\n",
      "timestep:114, pyg_AUC: 0.4887\n",
      "timestep:115, pyg_AUC: 0.4873\n",
      "timestep:116, pyg_AUC: 0.4845\n",
      "timestep:117, pyg_AUC: 0.4845\n",
      "timestep:118, pyg_AUC: 0.4929\n",
      "timestep:119, pyg_AUC: 0.4873\n",
      "timestep:120, pyg_AUC: 0.4915\n",
      "timestep:121, pyg_AUC: 0.4859\n",
      "timestep:122, pyg_AUC: 0.4901\n",
      "timestep:123, pyg_AUC: 0.4901\n",
      "timestep:124, pyg_AUC: 0.4901\n",
      "timestep:125, pyg_AUC: 0.4873\n",
      "timestep:126, pyg_AUC: 0.4915\n",
      "timestep:127, pyg_AUC: 0.4915\n",
      "timestep:128, pyg_AUC: 0.4929\n",
      "timestep:129, pyg_AUC: 0.4972\n",
      "timestep:130, pyg_AUC: 0.4873\n",
      "timestep:131, pyg_AUC: 0.4929\n",
      "timestep:132, pyg_AUC: 0.4929\n",
      "timestep:133, pyg_AUC: 0.4915\n",
      "timestep:134, pyg_AUC: 0.4915\n",
      "timestep:135, pyg_AUC: 0.4873\n",
      "timestep:136, pyg_AUC: 0.4944\n",
      "timestep:137, pyg_AUC: 0.4887\n",
      "timestep:138, pyg_AUC: 0.4915\n",
      "timestep:139, pyg_AUC: 0.4915\n",
      "timestep:140, pyg_AUC: 0.4958\n",
      "timestep:141, pyg_AUC: 0.4901\n",
      "timestep:142, pyg_AUC: 0.4901\n",
      "timestep:143, pyg_AUC: 0.4887\n",
      "timestep:144, pyg_AUC: 0.4873\n",
      "timestep:145, pyg_AUC: 0.4944\n",
      "timestep:146, pyg_AUC: 0.4944\n",
      "timestep:147, pyg_AUC: 0.4873\n",
      "timestep:148, pyg_AUC: 0.4929\n",
      "timestep:149, pyg_AUC: 0.4887\n",
      "timestep:150, pyg_AUC: 0.4887\n",
      "timestep:151, pyg_AUC: 0.4887\n",
      "timestep:152, pyg_AUC: 0.4845\n",
      "timestep:153, pyg_AUC: 0.4845\n",
      "timestep:154, pyg_AUC: 0.4915\n",
      "timestep:155, pyg_AUC: 0.4845\n",
      "timestep:156, pyg_AUC: 0.4845\n",
      "timestep:157, pyg_AUC: 0.4901\n",
      "timestep:158, pyg_AUC: 0.4929\n",
      "timestep:159, pyg_AUC: 0.4915\n",
      "timestep:160, pyg_AUC: 0.4929\n",
      "timestep:161, pyg_AUC: 0.4873\n",
      "timestep:162, pyg_AUC: 0.4831\n",
      "timestep:163, pyg_AUC: 0.4887\n",
      "timestep:164, pyg_AUC: 0.4845\n",
      "timestep:165, pyg_AUC: 0.4929\n",
      "timestep:166, pyg_AUC: 0.4901\n",
      "timestep:167, pyg_AUC: 0.4873\n",
      "timestep:168, pyg_AUC: 0.4929\n",
      "timestep:169, pyg_AUC: 0.4816\n",
      "timestep:170, pyg_AUC: 0.4929\n",
      "timestep:171, pyg_AUC: 0.4873\n",
      "timestep:172, pyg_AUC: 0.4873\n",
      "timestep:173, pyg_AUC: 0.4859\n",
      "timestep:174, pyg_AUC: 0.4845\n",
      "timestep:175, pyg_AUC: 0.4873\n",
      "timestep:176, pyg_AUC: 0.4915\n",
      "timestep:177, pyg_AUC: 0.4944\n",
      "timestep:178, pyg_AUC: 0.4915\n",
      "timestep:179, pyg_AUC: 0.4901\n",
      "timestep:180, pyg_AUC: 0.4915\n",
      "timestep:181, pyg_AUC: 0.4873\n",
      "timestep:182, pyg_AUC: 0.4887\n",
      "timestep:183, pyg_AUC: 0.4929\n",
      "timestep:184, pyg_AUC: 0.4929\n",
      "timestep:185, pyg_AUC: 0.4915\n",
      "timestep:186, pyg_AUC: 0.4915\n",
      "timestep:187, pyg_AUC: 0.4887\n",
      "timestep:188, pyg_AUC: 0.4887\n",
      "timestep:189, pyg_AUC: 0.4873\n",
      "timestep:190, pyg_AUC: 0.4901\n",
      "timestep:191, pyg_AUC: 0.4929\n",
      "timestep:192, pyg_AUC: 0.4915\n",
      "timestep:193, pyg_AUC: 0.4901\n",
      "timestep:194, pyg_AUC: 0.4915\n",
      "timestep:195, pyg_AUC: 0.4929\n",
      "timestep:196, pyg_AUC: 0.4901\n",
      "timestep:197, pyg_AUC: 0.4944\n",
      "timestep:198, pyg_AUC: 0.4915\n",
      "timestep:199, pyg_AUC: 0.4944\n",
      "timestep:200, pyg_AUC: 0.4915\n",
      "timestep:201, pyg_AUC: 0.4887\n",
      "timestep:202, pyg_AUC: 0.4929\n",
      "timestep:203, pyg_AUC: 0.4831\n",
      "timestep:204, pyg_AUC: 0.4901\n",
      "timestep:205, pyg_AUC: 0.4929\n",
      "timestep:206, pyg_AUC: 0.4915\n",
      "timestep:207, pyg_AUC: 0.4915\n",
      "timestep:208, pyg_AUC: 0.4929\n",
      "timestep:209, pyg_AUC: 0.4887\n",
      "timestep:210, pyg_AUC: 0.4915\n",
      "timestep:211, pyg_AUC: 0.4915\n",
      "timestep:212, pyg_AUC: 0.4915\n",
      "timestep:213, pyg_AUC: 0.4929\n",
      "timestep:214, pyg_AUC: 0.4929\n",
      "timestep:215, pyg_AUC: 0.4915\n",
      "timestep:216, pyg_AUC: 0.4901\n",
      "timestep:217, pyg_AUC: 0.4873\n",
      "timestep:218, pyg_AUC: 0.4901\n",
      "timestep:219, pyg_AUC: 0.4915\n",
      "timestep:220, pyg_AUC: 0.4901\n",
      "timestep:221, pyg_AUC: 0.4901\n",
      "timestep:222, pyg_AUC: 0.4859\n",
      "timestep:223, pyg_AUC: 0.4929\n",
      "timestep:224, pyg_AUC: 0.4873\n",
      "timestep:225, pyg_AUC: 0.4873\n",
      "timestep:226, pyg_AUC: 0.4915\n",
      "timestep:227, pyg_AUC: 0.4915\n",
      "timestep:228, pyg_AUC: 0.4873\n",
      "timestep:229, pyg_AUC: 0.4929\n",
      "timestep:230, pyg_AUC: 0.4929\n",
      "timestep:231, pyg_AUC: 0.4901\n",
      "timestep:232, pyg_AUC: 0.4915\n",
      "timestep:233, pyg_AUC: 0.4901\n",
      "timestep:234, pyg_AUC: 0.4915\n",
      "timestep:235, pyg_AUC: 0.4873\n",
      "timestep:236, pyg_AUC: 0.4915\n",
      "timestep:237, pyg_AUC: 0.4901\n",
      "timestep:238, pyg_AUC: 0.4859\n",
      "timestep:239, pyg_AUC: 0.4915\n",
      "timestep:240, pyg_AUC: 0.4887\n",
      "timestep:241, pyg_AUC: 0.4873\n",
      "timestep:242, pyg_AUC: 0.4887\n",
      "timestep:243, pyg_AUC: 0.4845\n",
      "timestep:244, pyg_AUC: 0.4873\n",
      "timestep:245, pyg_AUC: 0.4901\n",
      "timestep:246, pyg_AUC: 0.4887\n",
      "timestep:247, pyg_AUC: 0.4915\n",
      "timestep:248, pyg_AUC: 0.4915\n",
      "timestep:249, pyg_AUC: 0.4887\n",
      "timestep:250, pyg_AUC: 0.4915\n",
      "timestep:251, pyg_AUC: 0.4915\n",
      "timestep:252, pyg_AUC: 0.4887\n",
      "timestep:253, pyg_AUC: 0.4901\n",
      "timestep:254, pyg_AUC: 0.4845\n",
      "timestep:255, pyg_AUC: 0.4873\n",
      "timestep:256, pyg_AUC: 0.4887\n",
      "timestep:257, pyg_AUC: 0.4845\n",
      "timestep:258, pyg_AUC: 0.4887\n",
      "timestep:259, pyg_AUC: 0.4859\n",
      "timestep:260, pyg_AUC: 0.4915\n",
      "timestep:261, pyg_AUC: 0.4859\n",
      "timestep:262, pyg_AUC: 0.4859\n",
      "timestep:263, pyg_AUC: 0.4859\n",
      "timestep:264, pyg_AUC: 0.4901\n",
      "timestep:265, pyg_AUC: 0.4845\n",
      "timestep:266, pyg_AUC: 0.4887\n",
      "timestep:267, pyg_AUC: 0.4901\n",
      "timestep:268, pyg_AUC: 0.4901\n",
      "timestep:269, pyg_AUC: 0.4873\n",
      "timestep:270, pyg_AUC: 0.4873\n",
      "timestep:271, pyg_AUC: 0.4901\n",
      "timestep:272, pyg_AUC: 0.4929\n",
      "timestep:273, pyg_AUC: 0.4873\n",
      "timestep:274, pyg_AUC: 0.4887\n",
      "timestep:275, pyg_AUC: 0.4901\n",
      "timestep:276, pyg_AUC: 0.4887\n",
      "timestep:277, pyg_AUC: 0.4887\n",
      "timestep:278, pyg_AUC: 0.4887\n",
      "timestep:279, pyg_AUC: 0.4901\n",
      "timestep:280, pyg_AUC: 0.4873\n",
      "timestep:281, pyg_AUC: 0.4887\n",
      "timestep:282, pyg_AUC: 0.4915\n",
      "timestep:283, pyg_AUC: 0.4887\n",
      "timestep:284, pyg_AUC: 0.4873\n",
      "timestep:285, pyg_AUC: 0.4901\n",
      "timestep:286, pyg_AUC: 0.4859\n",
      "timestep:287, pyg_AUC: 0.4901\n",
      "timestep:288, pyg_AUC: 0.4887\n",
      "timestep:289, pyg_AUC: 0.4831\n",
      "timestep:290, pyg_AUC: 0.4901\n",
      "timestep:291, pyg_AUC: 0.4887\n",
      "timestep:292, pyg_AUC: 0.4901\n",
      "timestep:293, pyg_AUC: 0.4873\n",
      "timestep:294, pyg_AUC: 0.4887\n",
      "timestep:295, pyg_AUC: 0.4915\n",
      "timestep:296, pyg_AUC: 0.4873\n",
      "timestep:297, pyg_AUC: 0.4901\n",
      "timestep:298, pyg_AUC: 0.4887\n",
      "timestep:299, pyg_AUC: 0.4873\n",
      "timestep:300, pyg_AUC: 0.4887\n",
      "timestep:301, pyg_AUC: 0.4901\n",
      "timestep:302, pyg_AUC: 0.4887\n",
      "timestep:303, pyg_AUC: 0.4873\n",
      "timestep:304, pyg_AUC: 0.4873\n",
      "timestep:305, pyg_AUC: 0.4845\n",
      "timestep:306, pyg_AUC: 0.4873\n",
      "timestep:307, pyg_AUC: 0.4901\n",
      "timestep:308, pyg_AUC: 0.4901\n",
      "timestep:309, pyg_AUC: 0.4845\n",
      "timestep:310, pyg_AUC: 0.4901\n",
      "timestep:311, pyg_AUC: 0.4845\n",
      "timestep:312, pyg_AUC: 0.4859\n",
      "timestep:313, pyg_AUC: 0.4873\n",
      "timestep:314, pyg_AUC: 0.4873\n",
      "timestep:315, pyg_AUC: 0.4859\n",
      "timestep:316, pyg_AUC: 0.4859\n",
      "timestep:317, pyg_AUC: 0.4873\n",
      "timestep:318, pyg_AUC: 0.4901\n",
      "timestep:319, pyg_AUC: 0.4859\n",
      "timestep:320, pyg_AUC: 0.4873\n",
      "timestep:321, pyg_AUC: 0.4873\n",
      "timestep:322, pyg_AUC: 0.4901\n",
      "timestep:323, pyg_AUC: 0.4873\n",
      "timestep:324, pyg_AUC: 0.4859\n",
      "timestep:325, pyg_AUC: 0.4887\n",
      "timestep:326, pyg_AUC: 0.4887\n",
      "timestep:327, pyg_AUC: 0.4887\n",
      "timestep:328, pyg_AUC: 0.4901\n",
      "timestep:329, pyg_AUC: 0.4845\n",
      "timestep:330, pyg_AUC: 0.4887\n",
      "timestep:331, pyg_AUC: 0.4873\n",
      "timestep:332, pyg_AUC: 0.4873\n",
      "timestep:333, pyg_AUC: 0.4831\n",
      "timestep:334, pyg_AUC: 0.4859\n",
      "timestep:335, pyg_AUC: 0.4873\n",
      "timestep:336, pyg_AUC: 0.4873\n",
      "timestep:337, pyg_AUC: 0.4859\n",
      "timestep:338, pyg_AUC: 0.4873\n",
      "timestep:339, pyg_AUC: 0.4859\n",
      "timestep:340, pyg_AUC: 0.4887\n",
      "timestep:341, pyg_AUC: 0.4901\n",
      "timestep:342, pyg_AUC: 0.4845\n",
      "timestep:343, pyg_AUC: 0.4873\n",
      "timestep:344, pyg_AUC: 0.4859\n",
      "timestep:345, pyg_AUC: 0.4859\n",
      "timestep:346, pyg_AUC: 0.4873\n",
      "timestep:347, pyg_AUC: 0.4887\n",
      "timestep:348, pyg_AUC: 0.4887\n",
      "timestep:349, pyg_AUC: 0.4887\n",
      "timestep:350, pyg_AUC: 0.4873\n",
      "timestep:351, pyg_AUC: 0.4901\n",
      "timestep:352, pyg_AUC: 0.4873\n",
      "timestep:353, pyg_AUC: 0.4873\n",
      "timestep:354, pyg_AUC: 0.4887\n",
      "timestep:355, pyg_AUC: 0.4887\n",
      "timestep:356, pyg_AUC: 0.4859\n",
      "timestep:357, pyg_AUC: 0.4873\n",
      "timestep:358, pyg_AUC: 0.4859\n",
      "timestep:359, pyg_AUC: 0.4901\n",
      "timestep:360, pyg_AUC: 0.4859\n",
      "timestep:361, pyg_AUC: 0.4873\n",
      "timestep:362, pyg_AUC: 0.4859\n",
      "timestep:363, pyg_AUC: 0.4859\n",
      "timestep:364, pyg_AUC: 0.4901\n",
      "timestep:365, pyg_AUC: 0.4887\n",
      "timestep:366, pyg_AUC: 0.4845\n",
      "timestep:367, pyg_AUC: 0.4831\n",
      "timestep:368, pyg_AUC: 0.4873\n",
      "timestep:369, pyg_AUC: 0.4887\n",
      "timestep:370, pyg_AUC: 0.4915\n",
      "timestep:371, pyg_AUC: 0.4873\n",
      "timestep:372, pyg_AUC: 0.4873\n",
      "timestep:373, pyg_AUC: 0.4873\n",
      "timestep:374, pyg_AUC: 0.4887\n",
      "timestep:375, pyg_AUC: 0.4859\n",
      "timestep:376, pyg_AUC: 0.4859\n",
      "timestep:377, pyg_AUC: 0.4901\n",
      "timestep:378, pyg_AUC: 0.4901\n",
      "timestep:379, pyg_AUC: 0.4915\n",
      "timestep:380, pyg_AUC: 0.4887\n",
      "timestep:381, pyg_AUC: 0.4887\n",
      "timestep:382, pyg_AUC: 0.4873\n",
      "timestep:383, pyg_AUC: 0.4915\n",
      "timestep:384, pyg_AUC: 0.4887\n",
      "timestep:385, pyg_AUC: 0.4831\n",
      "timestep:386, pyg_AUC: 0.4859\n",
      "timestep:387, pyg_AUC: 0.4887\n",
      "timestep:388, pyg_AUC: 0.4901\n",
      "timestep:389, pyg_AUC: 0.4873\n",
      "timestep:390, pyg_AUC: 0.4915\n",
      "timestep:391, pyg_AUC: 0.4901\n",
      "timestep:392, pyg_AUC: 0.4901\n",
      "timestep:393, pyg_AUC: 0.4901\n",
      "timestep:394, pyg_AUC: 0.4887\n",
      "timestep:395, pyg_AUC: 0.4887\n",
      "timestep:396, pyg_AUC: 0.4873\n",
      "timestep:397, pyg_AUC: 0.4901\n",
      "timestep:398, pyg_AUC: 0.4887\n",
      "timestep:399, pyg_AUC: 0.4901\n",
      "timestep:400, pyg_AUC: 0.4887\n",
      "timestep:401, pyg_AUC: 0.4901\n",
      "timestep:402, pyg_AUC: 0.4901\n",
      "timestep:403, pyg_AUC: 0.4859\n",
      "timestep:404, pyg_AUC: 0.4887\n",
      "timestep:405, pyg_AUC: 0.4915\n",
      "timestep:406, pyg_AUC: 0.4887\n",
      "timestep:407, pyg_AUC: 0.4859\n",
      "timestep:408, pyg_AUC: 0.4887\n",
      "timestep:409, pyg_AUC: 0.4887\n",
      "timestep:410, pyg_AUC: 0.4859\n",
      "timestep:411, pyg_AUC: 0.4929\n",
      "timestep:412, pyg_AUC: 0.4901\n",
      "timestep:413, pyg_AUC: 0.4887\n",
      "timestep:414, pyg_AUC: 0.4887\n",
      "timestep:415, pyg_AUC: 0.4873\n",
      "timestep:416, pyg_AUC: 0.4901\n",
      "timestep:417, pyg_AUC: 0.4901\n",
      "timestep:418, pyg_AUC: 0.4873\n",
      "timestep:419, pyg_AUC: 0.4887\n",
      "timestep:420, pyg_AUC: 0.4901\n",
      "timestep:421, pyg_AUC: 0.4887\n",
      "timestep:422, pyg_AUC: 0.4887\n",
      "timestep:423, pyg_AUC: 0.4859\n",
      "timestep:424, pyg_AUC: 0.4873\n",
      "timestep:425, pyg_AUC: 0.4887\n",
      "timestep:426, pyg_AUC: 0.4915\n",
      "timestep:427, pyg_AUC: 0.4873\n",
      "timestep:428, pyg_AUC: 0.4887\n",
      "timestep:429, pyg_AUC: 0.4887\n",
      "timestep:430, pyg_AUC: 0.4873\n",
      "timestep:431, pyg_AUC: 0.4831\n",
      "timestep:432, pyg_AUC: 0.4873\n",
      "timestep:433, pyg_AUC: 0.4887\n",
      "timestep:434, pyg_AUC: 0.4873\n",
      "timestep:435, pyg_AUC: 0.4915\n",
      "timestep:436, pyg_AUC: 0.4873\n",
      "timestep:437, pyg_AUC: 0.4887\n",
      "timestep:438, pyg_AUC: 0.4859\n",
      "timestep:439, pyg_AUC: 0.4915\n",
      "timestep:440, pyg_AUC: 0.4887\n",
      "timestep:441, pyg_AUC: 0.4901\n",
      "timestep:442, pyg_AUC: 0.4845\n",
      "timestep:443, pyg_AUC: 0.4887\n",
      "timestep:444, pyg_AUC: 0.4901\n",
      "timestep:445, pyg_AUC: 0.4887\n",
      "timestep:446, pyg_AUC: 0.4901\n",
      "timestep:447, pyg_AUC: 0.4887\n",
      "timestep:448, pyg_AUC: 0.4887\n",
      "timestep:449, pyg_AUC: 0.4915\n",
      "timestep:450, pyg_AUC: 0.4901\n",
      "timestep:451, pyg_AUC: 0.4887\n",
      "timestep:452, pyg_AUC: 0.4901\n",
      "timestep:453, pyg_AUC: 0.4859\n",
      "timestep:454, pyg_AUC: 0.4859\n",
      "timestep:455, pyg_AUC: 0.4859\n",
      "timestep:456, pyg_AUC: 0.4873\n",
      "timestep:457, pyg_AUC: 0.4901\n",
      "timestep:458, pyg_AUC: 0.4929\n",
      "timestep:459, pyg_AUC: 0.4845\n",
      "timestep:460, pyg_AUC: 0.4887\n",
      "timestep:461, pyg_AUC: 0.4901\n",
      "timestep:462, pyg_AUC: 0.4901\n",
      "timestep:463, pyg_AUC: 0.4901\n",
      "timestep:464, pyg_AUC: 0.4901\n",
      "timestep:465, pyg_AUC: 0.4901\n",
      "timestep:466, pyg_AUC: 0.4901\n",
      "timestep:467, pyg_AUC: 0.4915\n",
      "timestep:468, pyg_AUC: 0.4887\n",
      "timestep:469, pyg_AUC: 0.4887\n",
      "timestep:470, pyg_AUC: 0.4915\n",
      "timestep:471, pyg_AUC: 0.4915\n",
      "timestep:472, pyg_AUC: 0.4887\n",
      "timestep:473, pyg_AUC: 0.4901\n",
      "timestep:474, pyg_AUC: 0.4845\n",
      "timestep:475, pyg_AUC: 0.4901\n",
      "timestep:476, pyg_AUC: 0.4887\n",
      "timestep:477, pyg_AUC: 0.4887\n",
      "timestep:478, pyg_AUC: 0.4901\n",
      "timestep:479, pyg_AUC: 0.4901\n",
      "timestep:480, pyg_AUC: 0.4831\n",
      "timestep:481, pyg_AUC: 0.4901\n",
      "timestep:482, pyg_AUC: 0.4859\n",
      "timestep:483, pyg_AUC: 0.4915\n",
      "timestep:484, pyg_AUC: 0.4901\n",
      "timestep:485, pyg_AUC: 0.4901\n",
      "timestep:486, pyg_AUC: 0.4915\n",
      "timestep:487, pyg_AUC: 0.4887\n",
      "timestep:488, pyg_AUC: 0.4845\n",
      "timestep:489, pyg_AUC: 0.4901\n",
      "timestep:490, pyg_AUC: 0.4901\n",
      "timestep:491, pyg_AUC: 0.4901\n",
      "timestep:492, pyg_AUC: 0.4887\n",
      "timestep:493, pyg_AUC: 0.4901\n",
      "timestep:494, pyg_AUC: 0.4887\n",
      "timestep:495, pyg_AUC: 0.4901\n",
      "timestep:496, pyg_AUC: 0.4901\n",
      "timestep:497, pyg_AUC: 0.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [23:43<10:21, 103.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:498, pyg_AUC: 0.4915\n",
      "timestep:499, pyg_AUC: 0.4915\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 38.32571\n",
      "Epoch: 0010 loss= 29.86969\n",
      "Epoch: 0020 loss= 25.57851\n",
      "Epoch: 0030 loss= 20.39316\n",
      "Epoch: 0040 loss= 15.33101\n",
      "Epoch: 0050 loss= 11.71732\n",
      "Epoch: 0060 loss= 3.65759\n",
      "Epoch: 0070 loss= 1.83816\n",
      "Epoch: 0080 loss= 1.10079\n",
      "Epoch: 0090 loss= 0.77652\n",
      "Epoch: 0100 loss= 0.75840\n",
      "Epoch: 0110 loss= 0.61433\n",
      "Epoch: 0120 loss= 0.62306\n",
      "Epoch: 0130 loss= 0.66919\n",
      "Epoch: 0140 loss= 0.65748\n",
      "Epoch: 0150 loss= 0.59054\n",
      "Epoch: 0160 loss= 0.63096\n",
      "Epoch: 0170 loss= 0.61209\n",
      "Epoch: 0180 loss= 0.64359\n",
      "Epoch: 0190 loss= 0.60465\n",
      "Epoch: 0200 loss= 0.62956\n",
      "Epoch: 0210 loss= 0.60971\n",
      "Epoch: 0220 loss= 0.57378\n",
      "Epoch: 0230 loss= 0.59157\n",
      "Epoch: 0240 loss= 0.58462\n",
      "Epoch: 0250 loss= 0.62646\n",
      "Epoch: 0260 loss= 0.56112\n",
      "Epoch: 0270 loss= 0.59263\n",
      "Epoch: 0280 loss= 0.52960\n",
      "Epoch: 0290 loss= 0.64433\n",
      "Epoch: 0300 loss= 0.61016\n",
      "Epoch: 0310 loss= 0.55779\n",
      "Epoch: 0320 loss= 0.53568\n",
      "Epoch: 0330 loss= 0.55813\n",
      "Epoch: 0340 loss= 0.57322\n",
      "Epoch: 0350 loss= 0.60198\n",
      "Epoch: 0360 loss= 0.61723\n",
      "Epoch: 0370 loss= 0.55585\n",
      "Epoch: 0380 loss= 0.54848\n",
      "Epoch: 0390 loss= 0.52776\n",
      "Epoch: 0400 loss= 0.55357\n",
      "Epoch: 0410 loss= 0.57496\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7974,  4.7452, -5.3207, -4.6632, -4.3980,  5.6494,  5.4837, -5.2653]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 30.85716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss= 26.55460\n",
      "Epoch: 0020 loss= 17.73634\n",
      "Epoch: 0030 loss= 13.58685\n",
      "Epoch: 0040 loss= 7.75981\n",
      "Epoch: 0050 loss= 1.78495\n",
      "Epoch: 0060 loss= 0.85992\n",
      "Epoch: 0070 loss= 0.67967\n",
      "Epoch: 0080 loss= 0.68469\n",
      "Epoch: 0090 loss= 0.65749\n",
      "Epoch: 0100 loss= 0.58378\n",
      "Epoch: 0110 loss= 0.52620\n",
      "Epoch: 0120 loss= 0.65414\n",
      "Epoch: 0130 loss= 0.58833\n",
      "Epoch: 0140 loss= 0.57408\n",
      "Epoch: 0150 loss= 0.57252\n",
      "Epoch: 0160 loss= 0.55539\n",
      "Epoch: 0170 loss= 0.56246\n",
      "Epoch: 0180 loss= 0.63358\n",
      "Epoch: 0190 loss= 0.60719\n",
      "Epoch: 0200 loss= 0.62587\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4887\n",
      "timestep:1, pyg_AUC: 0.4831\n",
      "timestep:2, pyg_AUC: 0.4915\n",
      "timestep:3, pyg_AUC: 0.4929\n",
      "timestep:4, pyg_AUC: 0.4887\n",
      "timestep:5, pyg_AUC: 0.5000\n",
      "timestep:6, pyg_AUC: 0.4915\n",
      "timestep:7, pyg_AUC: 0.4859\n",
      "timestep:8, pyg_AUC: 0.4915\n",
      "timestep:9, pyg_AUC: 0.4816\n",
      "timestep:10, pyg_AUC: 0.4929\n",
      "timestep:11, pyg_AUC: 0.4972\n",
      "timestep:12, pyg_AUC: 0.4929\n",
      "timestep:13, pyg_AUC: 0.4915\n",
      "timestep:14, pyg_AUC: 0.4901\n",
      "timestep:15, pyg_AUC: 0.4859\n",
      "timestep:16, pyg_AUC: 0.4873\n",
      "timestep:17, pyg_AUC: 0.4887\n",
      "timestep:18, pyg_AUC: 0.4929\n",
      "timestep:19, pyg_AUC: 0.4929\n",
      "timestep:20, pyg_AUC: 0.4887\n",
      "timestep:21, pyg_AUC: 0.4859\n",
      "timestep:22, pyg_AUC: 0.4944\n",
      "timestep:23, pyg_AUC: 0.4915\n",
      "timestep:24, pyg_AUC: 0.4845\n",
      "timestep:25, pyg_AUC: 0.4873\n",
      "timestep:26, pyg_AUC: 0.4859\n",
      "timestep:27, pyg_AUC: 0.4845\n",
      "timestep:28, pyg_AUC: 0.4859\n",
      "timestep:29, pyg_AUC: 0.4887\n",
      "timestep:30, pyg_AUC: 0.4859\n",
      "timestep:31, pyg_AUC: 0.4873\n",
      "timestep:32, pyg_AUC: 0.4859\n",
      "timestep:33, pyg_AUC: 0.4831\n",
      "timestep:34, pyg_AUC: 0.4845\n",
      "timestep:35, pyg_AUC: 0.4915\n",
      "timestep:36, pyg_AUC: 0.4845\n",
      "timestep:37, pyg_AUC: 0.4915\n",
      "timestep:38, pyg_AUC: 0.4859\n",
      "timestep:39, pyg_AUC: 0.4845\n",
      "timestep:40, pyg_AUC: 0.4929\n",
      "timestep:41, pyg_AUC: 0.4901\n",
      "timestep:42, pyg_AUC: 0.4873\n",
      "timestep:43, pyg_AUC: 0.4845\n",
      "timestep:44, pyg_AUC: 0.4972\n",
      "timestep:45, pyg_AUC: 0.4901\n",
      "timestep:46, pyg_AUC: 0.4901\n",
      "timestep:47, pyg_AUC: 0.4929\n",
      "timestep:48, pyg_AUC: 0.4944\n",
      "timestep:49, pyg_AUC: 0.4887\n",
      "timestep:50, pyg_AUC: 0.4845\n",
      "timestep:51, pyg_AUC: 0.4873\n",
      "timestep:52, pyg_AUC: 0.4831\n",
      "timestep:53, pyg_AUC: 0.4929\n",
      "timestep:54, pyg_AUC: 0.4859\n",
      "timestep:55, pyg_AUC: 0.4915\n",
      "timestep:56, pyg_AUC: 0.4929\n",
      "timestep:57, pyg_AUC: 0.4915\n",
      "timestep:58, pyg_AUC: 0.4873\n",
      "timestep:59, pyg_AUC: 0.4901\n",
      "timestep:60, pyg_AUC: 0.4901\n",
      "timestep:61, pyg_AUC: 0.4901\n",
      "timestep:62, pyg_AUC: 0.4901\n",
      "timestep:63, pyg_AUC: 0.4831\n",
      "timestep:64, pyg_AUC: 0.4859\n",
      "timestep:65, pyg_AUC: 0.4901\n",
      "timestep:66, pyg_AUC: 0.4887\n",
      "timestep:67, pyg_AUC: 0.4831\n",
      "timestep:68, pyg_AUC: 0.4845\n",
      "timestep:69, pyg_AUC: 0.4915\n",
      "timestep:70, pyg_AUC: 0.4887\n",
      "timestep:71, pyg_AUC: 0.4901\n",
      "timestep:72, pyg_AUC: 0.4929\n",
      "timestep:73, pyg_AUC: 0.4887\n",
      "timestep:74, pyg_AUC: 0.4873\n",
      "timestep:75, pyg_AUC: 0.4944\n",
      "timestep:76, pyg_AUC: 0.4887\n",
      "timestep:77, pyg_AUC: 0.4915\n",
      "timestep:78, pyg_AUC: 0.4887\n",
      "timestep:79, pyg_AUC: 0.4887\n",
      "timestep:80, pyg_AUC: 0.4816\n",
      "timestep:81, pyg_AUC: 0.4859\n",
      "timestep:82, pyg_AUC: 0.4901\n",
      "timestep:83, pyg_AUC: 0.4901\n",
      "timestep:84, pyg_AUC: 0.4873\n",
      "timestep:85, pyg_AUC: 0.4901\n",
      "timestep:86, pyg_AUC: 0.4859\n",
      "timestep:87, pyg_AUC: 0.4831\n",
      "timestep:88, pyg_AUC: 0.4859\n",
      "timestep:89, pyg_AUC: 0.4873\n",
      "timestep:90, pyg_AUC: 0.4915\n",
      "timestep:91, pyg_AUC: 0.4901\n",
      "timestep:92, pyg_AUC: 0.4887\n",
      "timestep:93, pyg_AUC: 0.4915\n",
      "timestep:94, pyg_AUC: 0.4873\n",
      "timestep:95, pyg_AUC: 0.4831\n",
      "timestep:96, pyg_AUC: 0.4915\n",
      "timestep:97, pyg_AUC: 0.4901\n",
      "timestep:98, pyg_AUC: 0.4887\n",
      "timestep:99, pyg_AUC: 0.4887\n",
      "timestep:100, pyg_AUC: 0.4929\n",
      "timestep:101, pyg_AUC: 0.4887\n",
      "timestep:102, pyg_AUC: 0.4887\n",
      "timestep:103, pyg_AUC: 0.4873\n",
      "timestep:104, pyg_AUC: 0.4859\n",
      "timestep:105, pyg_AUC: 0.4859\n",
      "timestep:106, pyg_AUC: 0.4873\n",
      "timestep:107, pyg_AUC: 0.4929\n",
      "timestep:108, pyg_AUC: 0.4859\n",
      "timestep:109, pyg_AUC: 0.4958\n",
      "timestep:110, pyg_AUC: 0.4915\n",
      "timestep:111, pyg_AUC: 0.4859\n",
      "timestep:112, pyg_AUC: 0.4845\n",
      "timestep:113, pyg_AUC: 0.4831\n",
      "timestep:114, pyg_AUC: 0.4915\n",
      "timestep:115, pyg_AUC: 0.4901\n",
      "timestep:116, pyg_AUC: 0.4944\n",
      "timestep:117, pyg_AUC: 0.4887\n",
      "timestep:118, pyg_AUC: 0.4873\n",
      "timestep:119, pyg_AUC: 0.4887\n",
      "timestep:120, pyg_AUC: 0.4859\n",
      "timestep:121, pyg_AUC: 0.4901\n",
      "timestep:122, pyg_AUC: 0.4845\n",
      "timestep:123, pyg_AUC: 0.4929\n",
      "timestep:124, pyg_AUC: 0.4887\n",
      "timestep:125, pyg_AUC: 0.4901\n",
      "timestep:126, pyg_AUC: 0.4859\n",
      "timestep:127, pyg_AUC: 0.4958\n",
      "timestep:128, pyg_AUC: 0.4845\n",
      "timestep:129, pyg_AUC: 0.4859\n",
      "timestep:130, pyg_AUC: 0.4887\n",
      "timestep:131, pyg_AUC: 0.4873\n",
      "timestep:132, pyg_AUC: 0.4944\n",
      "timestep:133, pyg_AUC: 0.4901\n",
      "timestep:134, pyg_AUC: 0.4873\n",
      "timestep:135, pyg_AUC: 0.4845\n",
      "timestep:136, pyg_AUC: 0.4845\n",
      "timestep:137, pyg_AUC: 0.4915\n",
      "timestep:138, pyg_AUC: 0.4901\n",
      "timestep:139, pyg_AUC: 0.4831\n",
      "timestep:140, pyg_AUC: 0.4873\n",
      "timestep:141, pyg_AUC: 0.4873\n",
      "timestep:142, pyg_AUC: 0.4901\n",
      "timestep:143, pyg_AUC: 0.4859\n",
      "timestep:144, pyg_AUC: 0.4901\n",
      "timestep:145, pyg_AUC: 0.4887\n",
      "timestep:146, pyg_AUC: 0.4887\n",
      "timestep:147, pyg_AUC: 0.4915\n",
      "timestep:148, pyg_AUC: 0.4915\n",
      "timestep:149, pyg_AUC: 0.4831\n",
      "timestep:150, pyg_AUC: 0.4972\n",
      "timestep:151, pyg_AUC: 0.4859\n",
      "timestep:152, pyg_AUC: 0.4831\n",
      "timestep:153, pyg_AUC: 0.4915\n",
      "timestep:154, pyg_AUC: 0.4887\n",
      "timestep:155, pyg_AUC: 0.4887\n",
      "timestep:156, pyg_AUC: 0.4873\n",
      "timestep:157, pyg_AUC: 0.4859\n",
      "timestep:158, pyg_AUC: 0.4915\n",
      "timestep:159, pyg_AUC: 0.4958\n",
      "timestep:160, pyg_AUC: 0.4901\n",
      "timestep:161, pyg_AUC: 0.4873\n",
      "timestep:162, pyg_AUC: 0.4901\n",
      "timestep:163, pyg_AUC: 0.4929\n",
      "timestep:164, pyg_AUC: 0.4901\n",
      "timestep:165, pyg_AUC: 0.4901\n",
      "timestep:166, pyg_AUC: 0.4901\n",
      "timestep:167, pyg_AUC: 0.4901\n",
      "timestep:168, pyg_AUC: 0.4915\n",
      "timestep:169, pyg_AUC: 0.4929\n",
      "timestep:170, pyg_AUC: 0.4859\n",
      "timestep:171, pyg_AUC: 0.4915\n",
      "timestep:172, pyg_AUC: 0.4915\n",
      "timestep:173, pyg_AUC: 0.4873\n",
      "timestep:174, pyg_AUC: 0.4873\n",
      "timestep:175, pyg_AUC: 0.4859\n",
      "timestep:176, pyg_AUC: 0.4831\n",
      "timestep:177, pyg_AUC: 0.4859\n",
      "timestep:178, pyg_AUC: 0.4915\n",
      "timestep:179, pyg_AUC: 0.4887\n",
      "timestep:180, pyg_AUC: 0.4887\n",
      "timestep:181, pyg_AUC: 0.4859\n",
      "timestep:182, pyg_AUC: 0.4873\n",
      "timestep:183, pyg_AUC: 0.4887\n",
      "timestep:184, pyg_AUC: 0.4859\n",
      "timestep:185, pyg_AUC: 0.4873\n",
      "timestep:186, pyg_AUC: 0.4915\n",
      "timestep:187, pyg_AUC: 0.4901\n",
      "timestep:188, pyg_AUC: 0.4873\n",
      "timestep:189, pyg_AUC: 0.4901\n",
      "timestep:190, pyg_AUC: 0.4859\n",
      "timestep:191, pyg_AUC: 0.4873\n",
      "timestep:192, pyg_AUC: 0.4873\n",
      "timestep:193, pyg_AUC: 0.4887\n",
      "timestep:194, pyg_AUC: 0.4915\n",
      "timestep:195, pyg_AUC: 0.4887\n",
      "timestep:196, pyg_AUC: 0.4915\n",
      "timestep:197, pyg_AUC: 0.4887\n",
      "timestep:198, pyg_AUC: 0.4873\n",
      "timestep:199, pyg_AUC: 0.4901\n",
      "timestep:200, pyg_AUC: 0.4901\n",
      "timestep:201, pyg_AUC: 0.4845\n",
      "timestep:202, pyg_AUC: 0.4887\n",
      "timestep:203, pyg_AUC: 0.4901\n",
      "timestep:204, pyg_AUC: 0.4929\n",
      "timestep:205, pyg_AUC: 0.4859\n",
      "timestep:206, pyg_AUC: 0.4887\n",
      "timestep:207, pyg_AUC: 0.4859\n",
      "timestep:208, pyg_AUC: 0.4873\n",
      "timestep:209, pyg_AUC: 0.4845\n",
      "timestep:210, pyg_AUC: 0.4887\n",
      "timestep:211, pyg_AUC: 0.4859\n",
      "timestep:212, pyg_AUC: 0.4901\n",
      "timestep:213, pyg_AUC: 0.4831\n",
      "timestep:214, pyg_AUC: 0.4887\n",
      "timestep:215, pyg_AUC: 0.4873\n",
      "timestep:216, pyg_AUC: 0.4887\n",
      "timestep:217, pyg_AUC: 0.4887\n",
      "timestep:218, pyg_AUC: 0.4831\n",
      "timestep:219, pyg_AUC: 0.4887\n",
      "timestep:220, pyg_AUC: 0.4859\n",
      "timestep:221, pyg_AUC: 0.4901\n",
      "timestep:222, pyg_AUC: 0.4831\n",
      "timestep:223, pyg_AUC: 0.4859\n",
      "timestep:224, pyg_AUC: 0.4887\n",
      "timestep:225, pyg_AUC: 0.4816\n",
      "timestep:226, pyg_AUC: 0.4915\n",
      "timestep:227, pyg_AUC: 0.4873\n",
      "timestep:228, pyg_AUC: 0.4915\n",
      "timestep:229, pyg_AUC: 0.4887\n",
      "timestep:230, pyg_AUC: 0.4873\n",
      "timestep:231, pyg_AUC: 0.4845\n",
      "timestep:232, pyg_AUC: 0.4859\n",
      "timestep:233, pyg_AUC: 0.4929\n",
      "timestep:234, pyg_AUC: 0.4845\n",
      "timestep:235, pyg_AUC: 0.4859\n",
      "timestep:236, pyg_AUC: 0.4901\n",
      "timestep:237, pyg_AUC: 0.4887\n",
      "timestep:238, pyg_AUC: 0.4901\n",
      "timestep:239, pyg_AUC: 0.4901\n",
      "timestep:240, pyg_AUC: 0.4816\n",
      "timestep:241, pyg_AUC: 0.4929\n",
      "timestep:242, pyg_AUC: 0.4873\n",
      "timestep:243, pyg_AUC: 0.4873\n",
      "timestep:244, pyg_AUC: 0.4915\n",
      "timestep:245, pyg_AUC: 0.4873\n",
      "timestep:246, pyg_AUC: 0.4859\n",
      "timestep:247, pyg_AUC: 0.4873\n",
      "timestep:248, pyg_AUC: 0.4901\n",
      "timestep:249, pyg_AUC: 0.4831\n",
      "timestep:250, pyg_AUC: 0.4873\n",
      "timestep:251, pyg_AUC: 0.4816\n",
      "timestep:252, pyg_AUC: 0.4887\n",
      "timestep:253, pyg_AUC: 0.4859\n",
      "timestep:254, pyg_AUC: 0.4845\n",
      "timestep:255, pyg_AUC: 0.4845\n",
      "timestep:256, pyg_AUC: 0.4915\n",
      "timestep:257, pyg_AUC: 0.4873\n",
      "timestep:258, pyg_AUC: 0.4845\n",
      "timestep:259, pyg_AUC: 0.4873\n",
      "timestep:260, pyg_AUC: 0.4859\n",
      "timestep:261, pyg_AUC: 0.4887\n",
      "timestep:262, pyg_AUC: 0.4887\n",
      "timestep:263, pyg_AUC: 0.4873\n",
      "timestep:264, pyg_AUC: 0.4887\n",
      "timestep:265, pyg_AUC: 0.4873\n",
      "timestep:266, pyg_AUC: 0.4901\n",
      "timestep:267, pyg_AUC: 0.4831\n",
      "timestep:268, pyg_AUC: 0.4845\n",
      "timestep:269, pyg_AUC: 0.4887\n",
      "timestep:270, pyg_AUC: 0.4873\n",
      "timestep:271, pyg_AUC: 0.4873\n",
      "timestep:272, pyg_AUC: 0.4873\n",
      "timestep:273, pyg_AUC: 0.4873\n",
      "timestep:274, pyg_AUC: 0.4873\n",
      "timestep:275, pyg_AUC: 0.4873\n",
      "timestep:276, pyg_AUC: 0.4887\n",
      "timestep:277, pyg_AUC: 0.4901\n",
      "timestep:278, pyg_AUC: 0.4901\n",
      "timestep:279, pyg_AUC: 0.4901\n",
      "timestep:280, pyg_AUC: 0.4859\n",
      "timestep:281, pyg_AUC: 0.4873\n",
      "timestep:282, pyg_AUC: 0.4901\n",
      "timestep:283, pyg_AUC: 0.4831\n",
      "timestep:284, pyg_AUC: 0.4859\n",
      "timestep:285, pyg_AUC: 0.4873\n",
      "timestep:286, pyg_AUC: 0.4873\n",
      "timestep:287, pyg_AUC: 0.4845\n",
      "timestep:288, pyg_AUC: 0.4859\n",
      "timestep:289, pyg_AUC: 0.4873\n",
      "timestep:290, pyg_AUC: 0.4831\n",
      "timestep:291, pyg_AUC: 0.4831\n",
      "timestep:292, pyg_AUC: 0.4873\n",
      "timestep:293, pyg_AUC: 0.4831\n",
      "timestep:294, pyg_AUC: 0.4901\n",
      "timestep:295, pyg_AUC: 0.4887\n",
      "timestep:296, pyg_AUC: 0.4873\n",
      "timestep:297, pyg_AUC: 0.4845\n",
      "timestep:298, pyg_AUC: 0.4873\n",
      "timestep:299, pyg_AUC: 0.4901\n",
      "timestep:300, pyg_AUC: 0.4859\n",
      "timestep:301, pyg_AUC: 0.4901\n",
      "timestep:302, pyg_AUC: 0.4887\n",
      "timestep:303, pyg_AUC: 0.4873\n",
      "timestep:304, pyg_AUC: 0.4873\n",
      "timestep:305, pyg_AUC: 0.4887\n",
      "timestep:306, pyg_AUC: 0.4901\n",
      "timestep:307, pyg_AUC: 0.4831\n",
      "timestep:308, pyg_AUC: 0.4901\n",
      "timestep:309, pyg_AUC: 0.4859\n",
      "timestep:310, pyg_AUC: 0.4859\n",
      "timestep:311, pyg_AUC: 0.4831\n",
      "timestep:312, pyg_AUC: 0.4873\n",
      "timestep:313, pyg_AUC: 0.4816\n",
      "timestep:314, pyg_AUC: 0.4845\n",
      "timestep:315, pyg_AUC: 0.4887\n",
      "timestep:316, pyg_AUC: 0.4845\n",
      "timestep:317, pyg_AUC: 0.4873\n",
      "timestep:318, pyg_AUC: 0.4859\n",
      "timestep:319, pyg_AUC: 0.4901\n",
      "timestep:320, pyg_AUC: 0.4845\n",
      "timestep:321, pyg_AUC: 0.4859\n",
      "timestep:322, pyg_AUC: 0.4873\n",
      "timestep:323, pyg_AUC: 0.4859\n",
      "timestep:324, pyg_AUC: 0.4859\n",
      "timestep:325, pyg_AUC: 0.4901\n",
      "timestep:326, pyg_AUC: 0.4873\n",
      "timestep:327, pyg_AUC: 0.4859\n",
      "timestep:328, pyg_AUC: 0.4873\n",
      "timestep:329, pyg_AUC: 0.4859\n",
      "timestep:330, pyg_AUC: 0.4887\n",
      "timestep:331, pyg_AUC: 0.4887\n",
      "timestep:332, pyg_AUC: 0.4887\n",
      "timestep:333, pyg_AUC: 0.4845\n",
      "timestep:334, pyg_AUC: 0.4845\n",
      "timestep:335, pyg_AUC: 0.4929\n",
      "timestep:336, pyg_AUC: 0.4845\n",
      "timestep:337, pyg_AUC: 0.4915\n",
      "timestep:338, pyg_AUC: 0.4831\n",
      "timestep:339, pyg_AUC: 0.4887\n",
      "timestep:340, pyg_AUC: 0.4845\n",
      "timestep:341, pyg_AUC: 0.4901\n",
      "timestep:342, pyg_AUC: 0.4873\n",
      "timestep:343, pyg_AUC: 0.4887\n",
      "timestep:344, pyg_AUC: 0.4901\n",
      "timestep:345, pyg_AUC: 0.4873\n",
      "timestep:346, pyg_AUC: 0.4887\n",
      "timestep:347, pyg_AUC: 0.4859\n",
      "timestep:348, pyg_AUC: 0.4873\n",
      "timestep:349, pyg_AUC: 0.4831\n",
      "timestep:350, pyg_AUC: 0.4887\n",
      "timestep:351, pyg_AUC: 0.4859\n",
      "timestep:352, pyg_AUC: 0.4859\n",
      "timestep:353, pyg_AUC: 0.4873\n",
      "timestep:354, pyg_AUC: 0.4887\n",
      "timestep:355, pyg_AUC: 0.4887\n",
      "timestep:356, pyg_AUC: 0.4887\n",
      "timestep:357, pyg_AUC: 0.4845\n",
      "timestep:358, pyg_AUC: 0.4873\n",
      "timestep:359, pyg_AUC: 0.4873\n",
      "timestep:360, pyg_AUC: 0.4887\n",
      "timestep:361, pyg_AUC: 0.4873\n",
      "timestep:362, pyg_AUC: 0.4859\n",
      "timestep:363, pyg_AUC: 0.4859\n",
      "timestep:364, pyg_AUC: 0.4887\n",
      "timestep:365, pyg_AUC: 0.4873\n",
      "timestep:366, pyg_AUC: 0.4859\n",
      "timestep:367, pyg_AUC: 0.4816\n",
      "timestep:368, pyg_AUC: 0.4859\n",
      "timestep:369, pyg_AUC: 0.4915\n",
      "timestep:370, pyg_AUC: 0.4845\n",
      "timestep:371, pyg_AUC: 0.4873\n",
      "timestep:372, pyg_AUC: 0.4901\n",
      "timestep:373, pyg_AUC: 0.4831\n",
      "timestep:374, pyg_AUC: 0.4873\n",
      "timestep:375, pyg_AUC: 0.4845\n",
      "timestep:376, pyg_AUC: 0.4859\n",
      "timestep:377, pyg_AUC: 0.4873\n",
      "timestep:378, pyg_AUC: 0.4859\n",
      "timestep:379, pyg_AUC: 0.4887\n",
      "timestep:380, pyg_AUC: 0.4859\n",
      "timestep:381, pyg_AUC: 0.4887\n",
      "timestep:382, pyg_AUC: 0.4859\n",
      "timestep:383, pyg_AUC: 0.4887\n",
      "timestep:384, pyg_AUC: 0.4901\n",
      "timestep:385, pyg_AUC: 0.4901\n",
      "timestep:386, pyg_AUC: 0.4873\n",
      "timestep:387, pyg_AUC: 0.4873\n",
      "timestep:388, pyg_AUC: 0.4859\n",
      "timestep:389, pyg_AUC: 0.4873\n",
      "timestep:390, pyg_AUC: 0.4859\n",
      "timestep:391, pyg_AUC: 0.4873\n",
      "timestep:392, pyg_AUC: 0.4887\n",
      "timestep:393, pyg_AUC: 0.4859\n",
      "timestep:394, pyg_AUC: 0.4873\n",
      "timestep:395, pyg_AUC: 0.4901\n",
      "timestep:396, pyg_AUC: 0.4873\n",
      "timestep:397, pyg_AUC: 0.4873\n",
      "timestep:398, pyg_AUC: 0.4859\n",
      "timestep:399, pyg_AUC: 0.4887\n",
      "timestep:400, pyg_AUC: 0.4873\n",
      "timestep:401, pyg_AUC: 0.4873\n",
      "timestep:402, pyg_AUC: 0.4887\n",
      "timestep:403, pyg_AUC: 0.4873\n",
      "timestep:404, pyg_AUC: 0.4873\n",
      "timestep:405, pyg_AUC: 0.4873\n",
      "timestep:406, pyg_AUC: 0.4901\n",
      "timestep:407, pyg_AUC: 0.4873\n",
      "timestep:408, pyg_AUC: 0.4887\n",
      "timestep:409, pyg_AUC: 0.4859\n",
      "timestep:410, pyg_AUC: 0.4887\n",
      "timestep:411, pyg_AUC: 0.4887\n",
      "timestep:412, pyg_AUC: 0.4873\n",
      "timestep:413, pyg_AUC: 0.4873\n",
      "timestep:414, pyg_AUC: 0.4873\n",
      "timestep:415, pyg_AUC: 0.4887\n",
      "timestep:416, pyg_AUC: 0.4873\n",
      "timestep:417, pyg_AUC: 0.4859\n",
      "timestep:418, pyg_AUC: 0.4859\n",
      "timestep:419, pyg_AUC: 0.4944\n",
      "timestep:420, pyg_AUC: 0.4831\n",
      "timestep:421, pyg_AUC: 0.4901\n",
      "timestep:422, pyg_AUC: 0.4845\n",
      "timestep:423, pyg_AUC: 0.4901\n",
      "timestep:424, pyg_AUC: 0.4873\n",
      "timestep:425, pyg_AUC: 0.4887\n",
      "timestep:426, pyg_AUC: 0.4887\n",
      "timestep:427, pyg_AUC: 0.4873\n",
      "timestep:428, pyg_AUC: 0.4873\n",
      "timestep:429, pyg_AUC: 0.4901\n",
      "timestep:430, pyg_AUC: 0.4901\n",
      "timestep:431, pyg_AUC: 0.4901\n",
      "timestep:432, pyg_AUC: 0.4873\n",
      "timestep:433, pyg_AUC: 0.4859\n",
      "timestep:434, pyg_AUC: 0.4901\n",
      "timestep:435, pyg_AUC: 0.4887\n",
      "timestep:436, pyg_AUC: 0.4901\n",
      "timestep:437, pyg_AUC: 0.4887\n",
      "timestep:438, pyg_AUC: 0.4901\n",
      "timestep:439, pyg_AUC: 0.4873\n",
      "timestep:440, pyg_AUC: 0.4859\n",
      "timestep:441, pyg_AUC: 0.4887\n",
      "timestep:442, pyg_AUC: 0.4859\n",
      "timestep:443, pyg_AUC: 0.4887\n",
      "timestep:444, pyg_AUC: 0.4929\n",
      "timestep:445, pyg_AUC: 0.4915\n",
      "timestep:446, pyg_AUC: 0.4915\n",
      "timestep:447, pyg_AUC: 0.4901\n",
      "timestep:448, pyg_AUC: 0.4873\n",
      "timestep:449, pyg_AUC: 0.4859\n",
      "timestep:450, pyg_AUC: 0.4929\n",
      "timestep:451, pyg_AUC: 0.4887\n",
      "timestep:452, pyg_AUC: 0.4887\n",
      "timestep:453, pyg_AUC: 0.4887\n",
      "timestep:454, pyg_AUC: 0.4859\n",
      "timestep:455, pyg_AUC: 0.4901\n",
      "timestep:456, pyg_AUC: 0.4859\n",
      "timestep:457, pyg_AUC: 0.4901\n",
      "timestep:458, pyg_AUC: 0.4873\n",
      "timestep:459, pyg_AUC: 0.4915\n",
      "timestep:460, pyg_AUC: 0.4859\n",
      "timestep:461, pyg_AUC: 0.4859\n",
      "timestep:462, pyg_AUC: 0.4873\n",
      "timestep:463, pyg_AUC: 0.4901\n",
      "timestep:464, pyg_AUC: 0.4887\n",
      "timestep:465, pyg_AUC: 0.4887\n",
      "timestep:466, pyg_AUC: 0.4873\n",
      "timestep:467, pyg_AUC: 0.4873\n",
      "timestep:468, pyg_AUC: 0.4901\n",
      "timestep:469, pyg_AUC: 0.4831\n",
      "timestep:470, pyg_AUC: 0.4887\n",
      "timestep:471, pyg_AUC: 0.4873\n",
      "timestep:472, pyg_AUC: 0.4873\n",
      "timestep:473, pyg_AUC: 0.4873\n",
      "timestep:474, pyg_AUC: 0.4915\n",
      "timestep:475, pyg_AUC: 0.4915\n",
      "timestep:476, pyg_AUC: 0.4901\n",
      "timestep:477, pyg_AUC: 0.4873\n",
      "timestep:478, pyg_AUC: 0.4915\n",
      "timestep:479, pyg_AUC: 0.4887\n",
      "timestep:480, pyg_AUC: 0.4901\n",
      "timestep:481, pyg_AUC: 0.4901\n",
      "timestep:482, pyg_AUC: 0.4901\n",
      "timestep:483, pyg_AUC: 0.4901\n",
      "timestep:484, pyg_AUC: 0.4915\n",
      "timestep:485, pyg_AUC: 0.4873\n",
      "timestep:486, pyg_AUC: 0.4859\n",
      "timestep:487, pyg_AUC: 0.4887\n",
      "timestep:488, pyg_AUC: 0.4887\n",
      "timestep:489, pyg_AUC: 0.4901\n",
      "timestep:490, pyg_AUC: 0.4831\n",
      "timestep:491, pyg_AUC: 0.4901\n",
      "timestep:492, pyg_AUC: 0.4873\n",
      "timestep:493, pyg_AUC: 0.4859\n",
      "timestep:494, pyg_AUC: 0.4859\n",
      "timestep:495, pyg_AUC: 0.4873\n",
      "timestep:496, pyg_AUC: 0.4901\n",
      "timestep:497, pyg_AUC: 0.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [25:25<08:35, 103.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:498, pyg_AUC: 0.4845\n",
      "timestep:499, pyg_AUC: 0.4859\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 46.96055\n",
      "Epoch: 0010 loss= 27.72296\n",
      "Epoch: 0020 loss= 18.71268\n",
      "Epoch: 0030 loss= 16.67502\n",
      "Epoch: 0040 loss= 12.09468\n",
      "Epoch: 0050 loss= 7.14389\n",
      "Epoch: 0060 loss= 2.99753\n",
      "Epoch: 0070 loss= 0.69929\n",
      "Epoch: 0080 loss= 0.64417\n",
      "Epoch: 0090 loss= 0.76861\n",
      "Epoch: 0100 loss= 0.65354\n",
      "Epoch: 0110 loss= 0.62975\n",
      "Epoch: 0120 loss= 0.66175\n",
      "Epoch: 0130 loss= 0.62493\n",
      "Epoch: 0140 loss= 0.59529\n",
      "Epoch: 0150 loss= 0.57452\n",
      "Epoch: 0160 loss= 0.60846\n",
      "Epoch: 0170 loss= 0.61542\n",
      "Epoch: 0180 loss= 0.67766\n",
      "Epoch: 0190 loss= 0.63992\n",
      "Epoch: 0200 loss= 0.61400\n",
      "Epoch: 0210 loss= 0.59975\n",
      "Epoch: 0220 loss= 0.59754\n",
      "Epoch: 0230 loss= 0.57064\n",
      "Epoch: 0240 loss= 0.53588\n",
      "Epoch: 0250 loss= 0.58023\n",
      "Epoch: 0260 loss= 0.61076\n",
      "Epoch: 0270 loss= 0.57571\n",
      "Epoch: 0280 loss= 0.53753\n",
      "Epoch: 0290 loss= 0.54681\n",
      "Epoch: 0300 loss= 0.55722\n",
      "Epoch: 0310 loss= 0.58031\n",
      "Epoch: 0320 loss= 0.53478\n",
      "Epoch: 0330 loss= 0.50970\n",
      "Epoch: 0340 loss= 0.52838\n",
      "Epoch: 0350 loss= 0.56420\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7944,  4.7200, -5.3409, -4.5857, -4.3547,  5.6746,  5.4747, -5.2401]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 38.63867\n",
      "Epoch: 0010 loss= 20.57784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0020 loss= 13.65278\n",
      "Epoch: 0030 loss= 9.15199\n",
      "Epoch: 0040 loss= 3.73097\n",
      "Epoch: 0050 loss= 1.14531\n",
      "Epoch: 0060 loss= 0.90801\n",
      "Epoch: 0070 loss= 0.67545\n",
      "Epoch: 0080 loss= 0.68483\n",
      "Epoch: 0090 loss= 0.62712\n",
      "Epoch: 0100 loss= 0.61185\n",
      "Epoch: 0110 loss= 0.56833\n",
      "Epoch: 0120 loss= 0.61043\n",
      "Epoch: 0130 loss= 0.63586\n",
      "Epoch: 0140 loss= 0.52281\n",
      "Epoch: 0150 loss= 0.64929\n",
      "Epoch: 0160 loss= 0.61932\n",
      "Epoch: 0170 loss= 0.57780\n",
      "Epoch: 0180 loss= 0.53643\n",
      "Epoch: 0190 loss= 0.56999\n",
      "Epoch: 0200 loss= 0.48895\n",
      "Epoch: 0210 loss= 0.58098\n",
      "Epoch: 0220 loss= 0.51550\n",
      "Epoch: 0230 loss= 0.54480\n",
      "Epoch: 0240 loss= 0.62267\n",
      "Epoch: 0250 loss= 0.59923\n",
      "Epoch: 0260 loss= 0.57330\n",
      "Epoch: 0270 loss= 0.51541\n",
      "Epoch: 0280 loss= 0.54510\n",
      "Epoch: 0290 loss= 0.61283\n",
      "Epoch: 0300 loss= 0.58172\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4901\n",
      "timestep:1, pyg_AUC: 0.4873\n",
      "timestep:2, pyg_AUC: 0.4901\n",
      "timestep:3, pyg_AUC: 0.4859\n",
      "timestep:4, pyg_AUC: 0.4831\n",
      "timestep:5, pyg_AUC: 0.4901\n",
      "timestep:6, pyg_AUC: 0.4915\n",
      "timestep:7, pyg_AUC: 0.4915\n",
      "timestep:8, pyg_AUC: 0.4901\n",
      "timestep:9, pyg_AUC: 0.4915\n",
      "timestep:10, pyg_AUC: 0.4915\n",
      "timestep:11, pyg_AUC: 0.4887\n",
      "timestep:12, pyg_AUC: 0.4929\n",
      "timestep:13, pyg_AUC: 0.4901\n",
      "timestep:14, pyg_AUC: 0.4915\n",
      "timestep:15, pyg_AUC: 0.4901\n",
      "timestep:16, pyg_AUC: 0.4929\n",
      "timestep:17, pyg_AUC: 0.4929\n",
      "timestep:18, pyg_AUC: 0.4915\n",
      "timestep:19, pyg_AUC: 0.4887\n",
      "timestep:20, pyg_AUC: 0.4859\n",
      "timestep:21, pyg_AUC: 0.4859\n",
      "timestep:22, pyg_AUC: 0.4887\n",
      "timestep:23, pyg_AUC: 0.4901\n",
      "timestep:24, pyg_AUC: 0.4901\n",
      "timestep:25, pyg_AUC: 0.4915\n",
      "timestep:26, pyg_AUC: 0.4816\n",
      "timestep:27, pyg_AUC: 0.4887\n",
      "timestep:28, pyg_AUC: 0.4845\n",
      "timestep:29, pyg_AUC: 0.4901\n",
      "timestep:30, pyg_AUC: 0.4901\n",
      "timestep:31, pyg_AUC: 0.4929\n",
      "timestep:32, pyg_AUC: 0.4901\n",
      "timestep:33, pyg_AUC: 0.4901\n",
      "timestep:34, pyg_AUC: 0.4915\n",
      "timestep:35, pyg_AUC: 0.4887\n",
      "timestep:36, pyg_AUC: 0.4929\n",
      "timestep:37, pyg_AUC: 0.4887\n",
      "timestep:38, pyg_AUC: 0.4873\n",
      "timestep:39, pyg_AUC: 0.4887\n",
      "timestep:40, pyg_AUC: 0.4901\n",
      "timestep:41, pyg_AUC: 0.4929\n",
      "timestep:42, pyg_AUC: 0.4901\n",
      "timestep:43, pyg_AUC: 0.4887\n",
      "timestep:44, pyg_AUC: 0.4901\n",
      "timestep:45, pyg_AUC: 0.4929\n",
      "timestep:46, pyg_AUC: 0.4887\n",
      "timestep:47, pyg_AUC: 0.4915\n",
      "timestep:48, pyg_AUC: 0.4887\n",
      "timestep:49, pyg_AUC: 0.4915\n",
      "timestep:50, pyg_AUC: 0.4887\n",
      "timestep:51, pyg_AUC: 0.4873\n",
      "timestep:52, pyg_AUC: 0.4901\n",
      "timestep:53, pyg_AUC: 0.4915\n",
      "timestep:54, pyg_AUC: 0.4873\n",
      "timestep:55, pyg_AUC: 0.4873\n",
      "timestep:56, pyg_AUC: 0.4873\n",
      "timestep:57, pyg_AUC: 0.4915\n",
      "timestep:58, pyg_AUC: 0.4929\n",
      "timestep:59, pyg_AUC: 0.4929\n",
      "timestep:60, pyg_AUC: 0.4887\n",
      "timestep:61, pyg_AUC: 0.4901\n",
      "timestep:62, pyg_AUC: 0.4958\n",
      "timestep:63, pyg_AUC: 0.4845\n",
      "timestep:64, pyg_AUC: 0.4915\n",
      "timestep:65, pyg_AUC: 0.4915\n",
      "timestep:66, pyg_AUC: 0.4915\n",
      "timestep:67, pyg_AUC: 0.4901\n",
      "timestep:68, pyg_AUC: 0.4915\n",
      "timestep:69, pyg_AUC: 0.4901\n",
      "timestep:70, pyg_AUC: 0.4915\n",
      "timestep:71, pyg_AUC: 0.4901\n",
      "timestep:72, pyg_AUC: 0.4873\n",
      "timestep:73, pyg_AUC: 0.4958\n",
      "timestep:74, pyg_AUC: 0.4901\n",
      "timestep:75, pyg_AUC: 0.4929\n",
      "timestep:76, pyg_AUC: 0.4887\n",
      "timestep:77, pyg_AUC: 0.4915\n",
      "timestep:78, pyg_AUC: 0.4915\n",
      "timestep:79, pyg_AUC: 0.4887\n",
      "timestep:80, pyg_AUC: 0.4901\n",
      "timestep:81, pyg_AUC: 0.4873\n",
      "timestep:82, pyg_AUC: 0.4887\n",
      "timestep:83, pyg_AUC: 0.4887\n",
      "timestep:84, pyg_AUC: 0.4901\n",
      "timestep:85, pyg_AUC: 0.4845\n",
      "timestep:86, pyg_AUC: 0.4901\n",
      "timestep:87, pyg_AUC: 0.4873\n",
      "timestep:88, pyg_AUC: 0.4901\n",
      "timestep:89, pyg_AUC: 0.4887\n",
      "timestep:90, pyg_AUC: 0.4873\n",
      "timestep:91, pyg_AUC: 0.4831\n",
      "timestep:92, pyg_AUC: 0.4901\n",
      "timestep:93, pyg_AUC: 0.4873\n",
      "timestep:94, pyg_AUC: 0.4887\n",
      "timestep:95, pyg_AUC: 0.4873\n",
      "timestep:96, pyg_AUC: 0.4873\n",
      "timestep:97, pyg_AUC: 0.4873\n",
      "timestep:98, pyg_AUC: 0.4915\n",
      "timestep:99, pyg_AUC: 0.4859\n",
      "timestep:100, pyg_AUC: 0.4887\n",
      "timestep:101, pyg_AUC: 0.4873\n",
      "timestep:102, pyg_AUC: 0.4887\n",
      "timestep:103, pyg_AUC: 0.4944\n",
      "timestep:104, pyg_AUC: 0.4901\n",
      "timestep:105, pyg_AUC: 0.4859\n",
      "timestep:106, pyg_AUC: 0.4845\n",
      "timestep:107, pyg_AUC: 0.4915\n",
      "timestep:108, pyg_AUC: 0.4901\n",
      "timestep:109, pyg_AUC: 0.4887\n",
      "timestep:110, pyg_AUC: 0.4831\n",
      "timestep:111, pyg_AUC: 0.4915\n",
      "timestep:112, pyg_AUC: 0.4845\n",
      "timestep:113, pyg_AUC: 0.4887\n",
      "timestep:114, pyg_AUC: 0.4901\n",
      "timestep:115, pyg_AUC: 0.4816\n",
      "timestep:116, pyg_AUC: 0.4915\n",
      "timestep:117, pyg_AUC: 0.4887\n",
      "timestep:118, pyg_AUC: 0.4929\n",
      "timestep:119, pyg_AUC: 0.4929\n",
      "timestep:120, pyg_AUC: 0.4873\n",
      "timestep:121, pyg_AUC: 0.4873\n",
      "timestep:122, pyg_AUC: 0.4915\n",
      "timestep:123, pyg_AUC: 0.4929\n",
      "timestep:124, pyg_AUC: 0.4873\n",
      "timestep:125, pyg_AUC: 0.4873\n",
      "timestep:126, pyg_AUC: 0.4859\n",
      "timestep:127, pyg_AUC: 0.4958\n",
      "timestep:128, pyg_AUC: 0.4958\n",
      "timestep:129, pyg_AUC: 0.4873\n",
      "timestep:130, pyg_AUC: 0.4929\n",
      "timestep:131, pyg_AUC: 0.4859\n",
      "timestep:132, pyg_AUC: 0.4845\n",
      "timestep:133, pyg_AUC: 0.4901\n",
      "timestep:134, pyg_AUC: 0.4887\n",
      "timestep:135, pyg_AUC: 0.4887\n",
      "timestep:136, pyg_AUC: 0.4929\n",
      "timestep:137, pyg_AUC: 0.4859\n",
      "timestep:138, pyg_AUC: 0.4915\n",
      "timestep:139, pyg_AUC: 0.4873\n",
      "timestep:140, pyg_AUC: 0.4972\n",
      "timestep:141, pyg_AUC: 0.4859\n",
      "timestep:142, pyg_AUC: 0.4915\n",
      "timestep:143, pyg_AUC: 0.4944\n",
      "timestep:144, pyg_AUC: 0.4873\n",
      "timestep:145, pyg_AUC: 0.4915\n",
      "timestep:146, pyg_AUC: 0.4944\n",
      "timestep:147, pyg_AUC: 0.4873\n",
      "timestep:148, pyg_AUC: 0.4859\n",
      "timestep:149, pyg_AUC: 0.4915\n",
      "timestep:150, pyg_AUC: 0.4831\n",
      "timestep:151, pyg_AUC: 0.4901\n",
      "timestep:152, pyg_AUC: 0.4887\n",
      "timestep:153, pyg_AUC: 0.4929\n",
      "timestep:154, pyg_AUC: 0.4915\n",
      "timestep:155, pyg_AUC: 0.4873\n",
      "timestep:156, pyg_AUC: 0.4901\n",
      "timestep:157, pyg_AUC: 0.4915\n",
      "timestep:158, pyg_AUC: 0.4859\n",
      "timestep:159, pyg_AUC: 0.4915\n",
      "timestep:160, pyg_AUC: 0.4944\n",
      "timestep:161, pyg_AUC: 0.4887\n",
      "timestep:162, pyg_AUC: 0.4859\n",
      "timestep:163, pyg_AUC: 0.4901\n",
      "timestep:164, pyg_AUC: 0.4887\n",
      "timestep:165, pyg_AUC: 0.4915\n",
      "timestep:166, pyg_AUC: 0.4901\n",
      "timestep:167, pyg_AUC: 0.4887\n",
      "timestep:168, pyg_AUC: 0.4831\n",
      "timestep:169, pyg_AUC: 0.4929\n",
      "timestep:170, pyg_AUC: 0.4944\n",
      "timestep:171, pyg_AUC: 0.4929\n",
      "timestep:172, pyg_AUC: 0.4901\n",
      "timestep:173, pyg_AUC: 0.4887\n",
      "timestep:174, pyg_AUC: 0.4887\n",
      "timestep:175, pyg_AUC: 0.4929\n",
      "timestep:176, pyg_AUC: 0.4929\n",
      "timestep:177, pyg_AUC: 0.4901\n",
      "timestep:178, pyg_AUC: 0.4901\n",
      "timestep:179, pyg_AUC: 0.4816\n",
      "timestep:180, pyg_AUC: 0.4887\n",
      "timestep:181, pyg_AUC: 0.4915\n",
      "timestep:182, pyg_AUC: 0.4845\n",
      "timestep:183, pyg_AUC: 0.4929\n",
      "timestep:184, pyg_AUC: 0.4915\n",
      "timestep:185, pyg_AUC: 0.4901\n",
      "timestep:186, pyg_AUC: 0.4915\n",
      "timestep:187, pyg_AUC: 0.4901\n",
      "timestep:188, pyg_AUC: 0.4901\n",
      "timestep:189, pyg_AUC: 0.4859\n",
      "timestep:190, pyg_AUC: 0.4915\n",
      "timestep:191, pyg_AUC: 0.4944\n",
      "timestep:192, pyg_AUC: 0.4915\n",
      "timestep:193, pyg_AUC: 0.4901\n",
      "timestep:194, pyg_AUC: 0.4929\n",
      "timestep:195, pyg_AUC: 0.4901\n",
      "timestep:196, pyg_AUC: 0.4915\n",
      "timestep:197, pyg_AUC: 0.4887\n",
      "timestep:198, pyg_AUC: 0.4901\n",
      "timestep:199, pyg_AUC: 0.4887\n",
      "timestep:200, pyg_AUC: 0.4915\n",
      "timestep:201, pyg_AUC: 0.4915\n",
      "timestep:202, pyg_AUC: 0.4915\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4901\n",
      "timestep:205, pyg_AUC: 0.4887\n",
      "timestep:206, pyg_AUC: 0.4929\n",
      "timestep:207, pyg_AUC: 0.4929\n",
      "timestep:208, pyg_AUC: 0.4887\n",
      "timestep:209, pyg_AUC: 0.4901\n",
      "timestep:210, pyg_AUC: 0.4873\n",
      "timestep:211, pyg_AUC: 0.4873\n",
      "timestep:212, pyg_AUC: 0.4915\n",
      "timestep:213, pyg_AUC: 0.4873\n",
      "timestep:214, pyg_AUC: 0.4901\n",
      "timestep:215, pyg_AUC: 0.4859\n",
      "timestep:216, pyg_AUC: 0.4887\n",
      "timestep:217, pyg_AUC: 0.4887\n",
      "timestep:218, pyg_AUC: 0.4901\n",
      "timestep:219, pyg_AUC: 0.4887\n",
      "timestep:220, pyg_AUC: 0.4901\n",
      "timestep:221, pyg_AUC: 0.4901\n",
      "timestep:222, pyg_AUC: 0.4887\n",
      "timestep:223, pyg_AUC: 0.4887\n",
      "timestep:224, pyg_AUC: 0.4887\n",
      "timestep:225, pyg_AUC: 0.4901\n",
      "timestep:226, pyg_AUC: 0.4859\n",
      "timestep:227, pyg_AUC: 0.4887\n",
      "timestep:228, pyg_AUC: 0.4887\n",
      "timestep:229, pyg_AUC: 0.4901\n",
      "timestep:230, pyg_AUC: 0.4873\n",
      "timestep:231, pyg_AUC: 0.4915\n",
      "timestep:232, pyg_AUC: 0.4915\n",
      "timestep:233, pyg_AUC: 0.4887\n",
      "timestep:234, pyg_AUC: 0.4887\n",
      "timestep:235, pyg_AUC: 0.4915\n",
      "timestep:236, pyg_AUC: 0.4901\n",
      "timestep:237, pyg_AUC: 0.4901\n",
      "timestep:238, pyg_AUC: 0.4901\n",
      "timestep:239, pyg_AUC: 0.4901\n",
      "timestep:240, pyg_AUC: 0.4901\n",
      "timestep:241, pyg_AUC: 0.4887\n",
      "timestep:242, pyg_AUC: 0.4873\n",
      "timestep:243, pyg_AUC: 0.4901\n",
      "timestep:244, pyg_AUC: 0.4901\n",
      "timestep:245, pyg_AUC: 0.4887\n",
      "timestep:246, pyg_AUC: 0.4887\n",
      "timestep:247, pyg_AUC: 0.4887\n",
      "timestep:248, pyg_AUC: 0.4859\n",
      "timestep:249, pyg_AUC: 0.4845\n",
      "timestep:250, pyg_AUC: 0.4845\n",
      "timestep:251, pyg_AUC: 0.4887\n",
      "timestep:252, pyg_AUC: 0.4887\n",
      "timestep:253, pyg_AUC: 0.4859\n",
      "timestep:254, pyg_AUC: 0.4887\n",
      "timestep:255, pyg_AUC: 0.4873\n",
      "timestep:256, pyg_AUC: 0.4845\n",
      "timestep:257, pyg_AUC: 0.4873\n",
      "timestep:258, pyg_AUC: 0.4845\n",
      "timestep:259, pyg_AUC: 0.4831\n",
      "timestep:260, pyg_AUC: 0.4901\n",
      "timestep:261, pyg_AUC: 0.4859\n",
      "timestep:262, pyg_AUC: 0.4859\n",
      "timestep:263, pyg_AUC: 0.4887\n",
      "timestep:264, pyg_AUC: 0.4873\n",
      "timestep:265, pyg_AUC: 0.4901\n",
      "timestep:266, pyg_AUC: 0.4887\n",
      "timestep:267, pyg_AUC: 0.4873\n",
      "timestep:268, pyg_AUC: 0.4887\n",
      "timestep:269, pyg_AUC: 0.4845\n",
      "timestep:270, pyg_AUC: 0.4859\n",
      "timestep:271, pyg_AUC: 0.4873\n",
      "timestep:272, pyg_AUC: 0.4859\n",
      "timestep:273, pyg_AUC: 0.4873\n",
      "timestep:274, pyg_AUC: 0.4873\n",
      "timestep:275, pyg_AUC: 0.4859\n",
      "timestep:276, pyg_AUC: 0.4845\n",
      "timestep:277, pyg_AUC: 0.4873\n",
      "timestep:278, pyg_AUC: 0.4873\n",
      "timestep:279, pyg_AUC: 0.4845\n",
      "timestep:280, pyg_AUC: 0.4887\n",
      "timestep:281, pyg_AUC: 0.4873\n",
      "timestep:282, pyg_AUC: 0.4887\n",
      "timestep:283, pyg_AUC: 0.4845\n",
      "timestep:284, pyg_AUC: 0.4873\n",
      "timestep:285, pyg_AUC: 0.4873\n",
      "timestep:286, pyg_AUC: 0.4887\n",
      "timestep:287, pyg_AUC: 0.4845\n",
      "timestep:288, pyg_AUC: 0.4887\n",
      "timestep:289, pyg_AUC: 0.4845\n",
      "timestep:290, pyg_AUC: 0.4887\n",
      "timestep:291, pyg_AUC: 0.4887\n",
      "timestep:292, pyg_AUC: 0.4845\n",
      "timestep:293, pyg_AUC: 0.4845\n",
      "timestep:294, pyg_AUC: 0.4845\n",
      "timestep:295, pyg_AUC: 0.4901\n",
      "timestep:296, pyg_AUC: 0.4859\n",
      "timestep:297, pyg_AUC: 0.4901\n",
      "timestep:298, pyg_AUC: 0.4887\n",
      "timestep:299, pyg_AUC: 0.4859\n",
      "timestep:300, pyg_AUC: 0.4873\n",
      "timestep:301, pyg_AUC: 0.4873\n",
      "timestep:302, pyg_AUC: 0.4915\n",
      "timestep:303, pyg_AUC: 0.4873\n",
      "timestep:304, pyg_AUC: 0.4859\n",
      "timestep:305, pyg_AUC: 0.4873\n",
      "timestep:306, pyg_AUC: 0.4859\n",
      "timestep:307, pyg_AUC: 0.4873\n",
      "timestep:308, pyg_AUC: 0.4887\n",
      "timestep:309, pyg_AUC: 0.4873\n",
      "timestep:310, pyg_AUC: 0.4873\n",
      "timestep:311, pyg_AUC: 0.4859\n",
      "timestep:312, pyg_AUC: 0.4859\n",
      "timestep:313, pyg_AUC: 0.4859\n",
      "timestep:314, pyg_AUC: 0.4873\n",
      "timestep:315, pyg_AUC: 0.4845\n",
      "timestep:316, pyg_AUC: 0.4859\n",
      "timestep:317, pyg_AUC: 0.4887\n",
      "timestep:318, pyg_AUC: 0.4887\n",
      "timestep:319, pyg_AUC: 0.4845\n",
      "timestep:320, pyg_AUC: 0.4859\n",
      "timestep:321, pyg_AUC: 0.4887\n",
      "timestep:322, pyg_AUC: 0.4831\n",
      "timestep:323, pyg_AUC: 0.4873\n",
      "timestep:324, pyg_AUC: 0.4845\n",
      "timestep:325, pyg_AUC: 0.4859\n",
      "timestep:326, pyg_AUC: 0.4873\n",
      "timestep:327, pyg_AUC: 0.4845\n",
      "timestep:328, pyg_AUC: 0.4845\n",
      "timestep:329, pyg_AUC: 0.4859\n",
      "timestep:330, pyg_AUC: 0.4887\n",
      "timestep:331, pyg_AUC: 0.4873\n",
      "timestep:332, pyg_AUC: 0.4873\n",
      "timestep:333, pyg_AUC: 0.4859\n",
      "timestep:334, pyg_AUC: 0.4859\n",
      "timestep:335, pyg_AUC: 0.4873\n",
      "timestep:336, pyg_AUC: 0.4845\n",
      "timestep:337, pyg_AUC: 0.4873\n",
      "timestep:338, pyg_AUC: 0.4873\n",
      "timestep:339, pyg_AUC: 0.4873\n",
      "timestep:340, pyg_AUC: 0.4845\n",
      "timestep:341, pyg_AUC: 0.4859\n",
      "timestep:342, pyg_AUC: 0.4859\n",
      "timestep:343, pyg_AUC: 0.4859\n",
      "timestep:344, pyg_AUC: 0.4873\n",
      "timestep:345, pyg_AUC: 0.4873\n",
      "timestep:346, pyg_AUC: 0.4887\n",
      "timestep:347, pyg_AUC: 0.4845\n",
      "timestep:348, pyg_AUC: 0.4859\n",
      "timestep:349, pyg_AUC: 0.4873\n",
      "timestep:350, pyg_AUC: 0.4873\n",
      "timestep:351, pyg_AUC: 0.4859\n",
      "timestep:352, pyg_AUC: 0.4859\n",
      "timestep:353, pyg_AUC: 0.4873\n",
      "timestep:354, pyg_AUC: 0.4873\n",
      "timestep:355, pyg_AUC: 0.4887\n",
      "timestep:356, pyg_AUC: 0.4901\n",
      "timestep:357, pyg_AUC: 0.4887\n",
      "timestep:358, pyg_AUC: 0.4873\n",
      "timestep:359, pyg_AUC: 0.4831\n",
      "timestep:360, pyg_AUC: 0.4845\n",
      "timestep:361, pyg_AUC: 0.4873\n",
      "timestep:362, pyg_AUC: 0.4859\n",
      "timestep:363, pyg_AUC: 0.4887\n",
      "timestep:364, pyg_AUC: 0.4901\n",
      "timestep:365, pyg_AUC: 0.4887\n",
      "timestep:366, pyg_AUC: 0.4873\n",
      "timestep:367, pyg_AUC: 0.4845\n",
      "timestep:368, pyg_AUC: 0.4873\n",
      "timestep:369, pyg_AUC: 0.4873\n",
      "timestep:370, pyg_AUC: 0.4873\n",
      "timestep:371, pyg_AUC: 0.4831\n",
      "timestep:372, pyg_AUC: 0.4845\n",
      "timestep:373, pyg_AUC: 0.4845\n",
      "timestep:374, pyg_AUC: 0.4845\n",
      "timestep:375, pyg_AUC: 0.4901\n",
      "timestep:376, pyg_AUC: 0.4873\n",
      "timestep:377, pyg_AUC: 0.4845\n",
      "timestep:378, pyg_AUC: 0.4873\n",
      "timestep:379, pyg_AUC: 0.4887\n",
      "timestep:380, pyg_AUC: 0.4873\n",
      "timestep:381, pyg_AUC: 0.4873\n",
      "timestep:382, pyg_AUC: 0.4887\n",
      "timestep:383, pyg_AUC: 0.4873\n",
      "timestep:384, pyg_AUC: 0.4873\n",
      "timestep:385, pyg_AUC: 0.4873\n",
      "timestep:386, pyg_AUC: 0.4845\n",
      "timestep:387, pyg_AUC: 0.4873\n",
      "timestep:388, pyg_AUC: 0.4873\n",
      "timestep:389, pyg_AUC: 0.4873\n",
      "timestep:390, pyg_AUC: 0.4887\n",
      "timestep:391, pyg_AUC: 0.4859\n",
      "timestep:392, pyg_AUC: 0.4873\n",
      "timestep:393, pyg_AUC: 0.4873\n",
      "timestep:394, pyg_AUC: 0.4859\n",
      "timestep:395, pyg_AUC: 0.4873\n",
      "timestep:396, pyg_AUC: 0.4859\n",
      "timestep:397, pyg_AUC: 0.4859\n",
      "timestep:398, pyg_AUC: 0.4887\n",
      "timestep:399, pyg_AUC: 0.4873\n",
      "timestep:400, pyg_AUC: 0.4859\n",
      "timestep:401, pyg_AUC: 0.4873\n",
      "timestep:402, pyg_AUC: 0.4859\n",
      "timestep:403, pyg_AUC: 0.4873\n",
      "timestep:404, pyg_AUC: 0.4859\n",
      "timestep:405, pyg_AUC: 0.4859\n",
      "timestep:406, pyg_AUC: 0.4873\n",
      "timestep:407, pyg_AUC: 0.4887\n",
      "timestep:408, pyg_AUC: 0.4873\n",
      "timestep:409, pyg_AUC: 0.4887\n",
      "timestep:410, pyg_AUC: 0.4887\n",
      "timestep:411, pyg_AUC: 0.4887\n",
      "timestep:412, pyg_AUC: 0.4873\n",
      "timestep:413, pyg_AUC: 0.4859\n",
      "timestep:414, pyg_AUC: 0.4859\n",
      "timestep:415, pyg_AUC: 0.4873\n",
      "timestep:416, pyg_AUC: 0.4873\n",
      "timestep:417, pyg_AUC: 0.4873\n",
      "timestep:418, pyg_AUC: 0.4873\n",
      "timestep:419, pyg_AUC: 0.4901\n",
      "timestep:420, pyg_AUC: 0.4901\n",
      "timestep:421, pyg_AUC: 0.4845\n",
      "timestep:422, pyg_AUC: 0.4873\n",
      "timestep:423, pyg_AUC: 0.4845\n",
      "timestep:424, pyg_AUC: 0.4901\n",
      "timestep:425, pyg_AUC: 0.4873\n",
      "timestep:426, pyg_AUC: 0.4887\n",
      "timestep:427, pyg_AUC: 0.4915\n",
      "timestep:428, pyg_AUC: 0.4901\n",
      "timestep:429, pyg_AUC: 0.4901\n",
      "timestep:430, pyg_AUC: 0.4887\n",
      "timestep:431, pyg_AUC: 0.4859\n",
      "timestep:432, pyg_AUC: 0.4901\n",
      "timestep:433, pyg_AUC: 0.4901\n",
      "timestep:434, pyg_AUC: 0.4873\n",
      "timestep:435, pyg_AUC: 0.4887\n",
      "timestep:436, pyg_AUC: 0.4859\n",
      "timestep:437, pyg_AUC: 0.4887\n",
      "timestep:438, pyg_AUC: 0.4859\n",
      "timestep:439, pyg_AUC: 0.4845\n",
      "timestep:440, pyg_AUC: 0.4873\n",
      "timestep:441, pyg_AUC: 0.4887\n",
      "timestep:442, pyg_AUC: 0.4887\n",
      "timestep:443, pyg_AUC: 0.4887\n",
      "timestep:444, pyg_AUC: 0.4887\n",
      "timestep:445, pyg_AUC: 0.4873\n",
      "timestep:446, pyg_AUC: 0.4845\n",
      "timestep:447, pyg_AUC: 0.4915\n",
      "timestep:448, pyg_AUC: 0.4887\n",
      "timestep:449, pyg_AUC: 0.4901\n",
      "timestep:450, pyg_AUC: 0.4901\n",
      "timestep:451, pyg_AUC: 0.4859\n",
      "timestep:452, pyg_AUC: 0.4887\n",
      "timestep:453, pyg_AUC: 0.4901\n",
      "timestep:454, pyg_AUC: 0.4887\n",
      "timestep:455, pyg_AUC: 0.4859\n",
      "timestep:456, pyg_AUC: 0.4887\n",
      "timestep:457, pyg_AUC: 0.4873\n",
      "timestep:458, pyg_AUC: 0.4887\n",
      "timestep:459, pyg_AUC: 0.4887\n",
      "timestep:460, pyg_AUC: 0.4873\n",
      "timestep:461, pyg_AUC: 0.4901\n",
      "timestep:462, pyg_AUC: 0.4901\n",
      "timestep:463, pyg_AUC: 0.4901\n",
      "timestep:464, pyg_AUC: 0.4873\n",
      "timestep:465, pyg_AUC: 0.4901\n",
      "timestep:466, pyg_AUC: 0.4845\n",
      "timestep:467, pyg_AUC: 0.4887\n",
      "timestep:468, pyg_AUC: 0.4929\n",
      "timestep:469, pyg_AUC: 0.4887\n",
      "timestep:470, pyg_AUC: 0.4887\n",
      "timestep:471, pyg_AUC: 0.4887\n",
      "timestep:472, pyg_AUC: 0.4915\n",
      "timestep:473, pyg_AUC: 0.4901\n",
      "timestep:474, pyg_AUC: 0.4901\n",
      "timestep:475, pyg_AUC: 0.4873\n",
      "timestep:476, pyg_AUC: 0.4901\n",
      "timestep:477, pyg_AUC: 0.4929\n",
      "timestep:478, pyg_AUC: 0.4873\n",
      "timestep:479, pyg_AUC: 0.4901\n",
      "timestep:480, pyg_AUC: 0.4915\n",
      "timestep:481, pyg_AUC: 0.4859\n",
      "timestep:482, pyg_AUC: 0.4915\n",
      "timestep:483, pyg_AUC: 0.4887\n",
      "timestep:484, pyg_AUC: 0.4915\n",
      "timestep:485, pyg_AUC: 0.4845\n",
      "timestep:486, pyg_AUC: 0.4887\n",
      "timestep:487, pyg_AUC: 0.4873\n",
      "timestep:488, pyg_AUC: 0.4915\n",
      "timestep:489, pyg_AUC: 0.4901\n",
      "timestep:490, pyg_AUC: 0.4901\n",
      "timestep:491, pyg_AUC: 0.4901\n",
      "timestep:492, pyg_AUC: 0.4915\n",
      "timestep:493, pyg_AUC: 0.4887\n",
      "timestep:494, pyg_AUC: 0.4901\n",
      "timestep:495, pyg_AUC: 0.4873\n",
      "timestep:496, pyg_AUC: 0.4901\n",
      "timestep:497, pyg_AUC: 0.4901\n",
      "timestep:498, pyg_AUC: 0.4873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [27:06<06:50, 102.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:499, pyg_AUC: 0.4915\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 38.02254\n",
      "Epoch: 0010 loss= 27.99958\n",
      "Epoch: 0020 loss= 19.41045\n",
      "Epoch: 0030 loss= 16.66020\n",
      "Epoch: 0040 loss= 14.44121\n",
      "Epoch: 0050 loss= 4.17237\n",
      "Epoch: 0060 loss= 1.21599\n",
      "Epoch: 0070 loss= 0.84787\n",
      "Epoch: 0080 loss= 0.61324\n",
      "Epoch: 0090 loss= 0.64598\n",
      "Epoch: 0100 loss= 0.61378\n",
      "Epoch: 0110 loss= 0.63308\n",
      "Epoch: 0120 loss= 0.65770\n",
      "Epoch: 0130 loss= 0.59138\n",
      "Epoch: 0140 loss= 0.66857\n",
      "Epoch: 0150 loss= 0.62097\n",
      "Epoch: 0160 loss= 0.54267\n",
      "Epoch: 0170 loss= 0.59196\n",
      "Epoch: 0180 loss= 0.69642\n",
      "Epoch: 0190 loss= 0.57597\n",
      "Epoch: 0200 loss= 0.59372\n",
      "Epoch: 0210 loss= 0.56244\n",
      "Epoch: 0220 loss= 0.59800\n",
      "Epoch: 0230 loss= 0.66805\n",
      "Epoch: 0240 loss= 0.63774\n",
      "Epoch: 0250 loss= 0.59312\n",
      "Epoch: 0260 loss= 0.52820\n",
      "Epoch: 0270 loss= 0.58407\n",
      "Epoch: 0280 loss= 0.56977\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7623,  4.6843, -5.3228, -4.6303, -4.3717,  5.5789,  5.4012, -5.1959]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 37.42702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss= 28.25689\n",
      "Epoch: 0020 loss= 22.48223\n",
      "Epoch: 0030 loss= 11.13611\n",
      "Epoch: 0040 loss= 5.92358\n",
      "Epoch: 0050 loss= 0.92838\n",
      "Epoch: 0060 loss= 0.90961\n",
      "Epoch: 0070 loss= 0.68554\n",
      "Epoch: 0080 loss= 0.69580\n",
      "Epoch: 0090 loss= 0.74024\n",
      "Epoch: 0100 loss= 0.66947\n",
      "Epoch: 0110 loss= 0.57658\n",
      "Epoch: 0120 loss= 0.63795\n",
      "Epoch: 0130 loss= 0.54824\n",
      "Epoch: 0140 loss= 0.65746\n",
      "Epoch: 0150 loss= 0.64628\n",
      "Epoch: 0160 loss= 0.57223\n",
      "Epoch: 0170 loss= 0.62112\n",
      "Epoch: 0180 loss= 0.59720\n",
      "Epoch: 0190 loss= 0.54443\n",
      "Epoch: 0200 loss= 0.59654\n",
      "Epoch: 0210 loss= 0.54539\n",
      "Epoch: 0220 loss= 0.52675\n",
      "Epoch: 0230 loss= 0.60256\n",
      "Epoch: 0240 loss= 0.47855\n",
      "Epoch: 0250 loss= 0.63726\n",
      "Epoch: 0260 loss= 0.59217\n",
      "Epoch: 0270 loss= 0.53564\n",
      "Epoch: 0280 loss= 0.62018\n",
      "Epoch: 0290 loss= 0.58246\n",
      "Epoch: 0300 loss= 0.56762\n",
      "Epoch: 0310 loss= 0.57211\n",
      "Epoch: 0320 loss= 0.55664\n",
      "Epoch: 0330 loss= 0.58422\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4915\n",
      "timestep:1, pyg_AUC: 0.4901\n",
      "timestep:2, pyg_AUC: 0.4901\n",
      "timestep:3, pyg_AUC: 0.4873\n",
      "timestep:4, pyg_AUC: 0.4887\n",
      "timestep:5, pyg_AUC: 0.4887\n",
      "timestep:6, pyg_AUC: 0.4915\n",
      "timestep:7, pyg_AUC: 0.4958\n",
      "timestep:8, pyg_AUC: 0.4915\n",
      "timestep:9, pyg_AUC: 0.4929\n",
      "timestep:10, pyg_AUC: 0.4901\n",
      "timestep:11, pyg_AUC: 0.4901\n",
      "timestep:12, pyg_AUC: 0.4816\n",
      "timestep:13, pyg_AUC: 0.4887\n",
      "timestep:14, pyg_AUC: 0.4972\n",
      "timestep:15, pyg_AUC: 0.4845\n",
      "timestep:16, pyg_AUC: 0.4845\n",
      "timestep:17, pyg_AUC: 0.4845\n",
      "timestep:18, pyg_AUC: 0.4859\n",
      "timestep:19, pyg_AUC: 0.4873\n",
      "timestep:20, pyg_AUC: 0.4859\n",
      "timestep:21, pyg_AUC: 0.4831\n",
      "timestep:22, pyg_AUC: 0.4944\n",
      "timestep:23, pyg_AUC: 0.4859\n",
      "timestep:24, pyg_AUC: 0.4873\n",
      "timestep:25, pyg_AUC: 0.4887\n",
      "timestep:26, pyg_AUC: 0.4901\n",
      "timestep:27, pyg_AUC: 0.4873\n",
      "timestep:28, pyg_AUC: 0.4831\n",
      "timestep:29, pyg_AUC: 0.4972\n",
      "timestep:30, pyg_AUC: 0.4958\n",
      "timestep:31, pyg_AUC: 0.4831\n",
      "timestep:32, pyg_AUC: 0.4873\n",
      "timestep:33, pyg_AUC: 0.4915\n",
      "timestep:34, pyg_AUC: 0.4901\n",
      "timestep:35, pyg_AUC: 0.4859\n",
      "timestep:36, pyg_AUC: 0.4958\n",
      "timestep:37, pyg_AUC: 0.4901\n",
      "timestep:38, pyg_AUC: 0.4944\n",
      "timestep:39, pyg_AUC: 0.4859\n",
      "timestep:40, pyg_AUC: 0.4887\n",
      "timestep:41, pyg_AUC: 0.4845\n",
      "timestep:42, pyg_AUC: 0.4915\n",
      "timestep:43, pyg_AUC: 0.4901\n",
      "timestep:44, pyg_AUC: 0.4873\n",
      "timestep:45, pyg_AUC: 0.4873\n",
      "timestep:46, pyg_AUC: 0.4915\n",
      "timestep:47, pyg_AUC: 0.4944\n",
      "timestep:48, pyg_AUC: 0.4901\n",
      "timestep:49, pyg_AUC: 0.4816\n",
      "timestep:50, pyg_AUC: 0.4845\n",
      "timestep:51, pyg_AUC: 0.4887\n",
      "timestep:52, pyg_AUC: 0.4887\n",
      "timestep:53, pyg_AUC: 0.4929\n",
      "timestep:54, pyg_AUC: 0.4873\n",
      "timestep:55, pyg_AUC: 0.4873\n",
      "timestep:56, pyg_AUC: 0.4873\n",
      "timestep:57, pyg_AUC: 0.4944\n",
      "timestep:58, pyg_AUC: 0.4831\n",
      "timestep:59, pyg_AUC: 0.4802\n",
      "timestep:60, pyg_AUC: 0.4845\n",
      "timestep:61, pyg_AUC: 0.4901\n",
      "timestep:62, pyg_AUC: 0.4873\n",
      "timestep:63, pyg_AUC: 0.4901\n",
      "timestep:64, pyg_AUC: 0.4887\n",
      "timestep:65, pyg_AUC: 0.4929\n",
      "timestep:66, pyg_AUC: 0.4915\n",
      "timestep:67, pyg_AUC: 0.4873\n",
      "timestep:68, pyg_AUC: 0.4831\n",
      "timestep:69, pyg_AUC: 0.4887\n",
      "timestep:70, pyg_AUC: 0.4845\n",
      "timestep:71, pyg_AUC: 0.4831\n",
      "timestep:72, pyg_AUC: 0.4944\n",
      "timestep:73, pyg_AUC: 0.4873\n",
      "timestep:74, pyg_AUC: 0.4816\n",
      "timestep:75, pyg_AUC: 0.4859\n",
      "timestep:76, pyg_AUC: 0.4873\n",
      "timestep:77, pyg_AUC: 0.4816\n",
      "timestep:78, pyg_AUC: 0.4929\n",
      "timestep:79, pyg_AUC: 0.4887\n",
      "timestep:80, pyg_AUC: 0.4873\n",
      "timestep:81, pyg_AUC: 0.4873\n",
      "timestep:82, pyg_AUC: 0.4887\n",
      "timestep:83, pyg_AUC: 0.4873\n",
      "timestep:84, pyg_AUC: 0.4901\n",
      "timestep:85, pyg_AUC: 0.4859\n",
      "timestep:86, pyg_AUC: 0.4915\n",
      "timestep:87, pyg_AUC: 0.4915\n",
      "timestep:88, pyg_AUC: 0.4929\n",
      "timestep:89, pyg_AUC: 0.4915\n",
      "timestep:90, pyg_AUC: 0.4915\n",
      "timestep:91, pyg_AUC: 0.4873\n",
      "timestep:92, pyg_AUC: 0.4944\n",
      "timestep:93, pyg_AUC: 0.4845\n",
      "timestep:94, pyg_AUC: 0.4788\n",
      "timestep:95, pyg_AUC: 0.4859\n",
      "timestep:96, pyg_AUC: 0.4887\n",
      "timestep:97, pyg_AUC: 0.4873\n",
      "timestep:98, pyg_AUC: 0.4845\n",
      "timestep:99, pyg_AUC: 0.4915\n",
      "timestep:100, pyg_AUC: 0.4958\n",
      "timestep:101, pyg_AUC: 0.4831\n",
      "timestep:102, pyg_AUC: 0.4958\n",
      "timestep:103, pyg_AUC: 0.4873\n",
      "timestep:104, pyg_AUC: 0.4929\n",
      "timestep:105, pyg_AUC: 0.4944\n",
      "timestep:106, pyg_AUC: 0.4859\n",
      "timestep:107, pyg_AUC: 0.4901\n",
      "timestep:108, pyg_AUC: 0.4873\n",
      "timestep:109, pyg_AUC: 0.4944\n",
      "timestep:110, pyg_AUC: 0.4845\n",
      "timestep:111, pyg_AUC: 0.4887\n",
      "timestep:112, pyg_AUC: 0.4859\n",
      "timestep:113, pyg_AUC: 0.4915\n",
      "timestep:114, pyg_AUC: 0.4859\n",
      "timestep:115, pyg_AUC: 0.4929\n",
      "timestep:116, pyg_AUC: 0.4929\n",
      "timestep:117, pyg_AUC: 0.4901\n",
      "timestep:118, pyg_AUC: 0.4831\n",
      "timestep:119, pyg_AUC: 0.4873\n",
      "timestep:120, pyg_AUC: 0.4873\n",
      "timestep:121, pyg_AUC: 0.4915\n",
      "timestep:122, pyg_AUC: 0.4802\n",
      "timestep:123, pyg_AUC: 0.4845\n",
      "timestep:124, pyg_AUC: 0.4929\n",
      "timestep:125, pyg_AUC: 0.4887\n",
      "timestep:126, pyg_AUC: 0.4845\n",
      "timestep:127, pyg_AUC: 0.4929\n",
      "timestep:128, pyg_AUC: 0.4887\n",
      "timestep:129, pyg_AUC: 0.4887\n",
      "timestep:130, pyg_AUC: 0.4873\n",
      "timestep:131, pyg_AUC: 0.4887\n",
      "timestep:132, pyg_AUC: 0.4873\n",
      "timestep:133, pyg_AUC: 0.4901\n",
      "timestep:134, pyg_AUC: 0.4901\n",
      "timestep:135, pyg_AUC: 0.4859\n",
      "timestep:136, pyg_AUC: 0.4887\n",
      "timestep:137, pyg_AUC: 0.4915\n",
      "timestep:138, pyg_AUC: 0.4929\n",
      "timestep:139, pyg_AUC: 0.4831\n",
      "timestep:140, pyg_AUC: 0.4915\n",
      "timestep:141, pyg_AUC: 0.4887\n",
      "timestep:142, pyg_AUC: 0.4845\n",
      "timestep:143, pyg_AUC: 0.4944\n",
      "timestep:144, pyg_AUC: 0.4929\n",
      "timestep:145, pyg_AUC: 0.4929\n",
      "timestep:146, pyg_AUC: 0.4887\n",
      "timestep:147, pyg_AUC: 0.4915\n",
      "timestep:148, pyg_AUC: 0.4831\n",
      "timestep:149, pyg_AUC: 0.4915\n",
      "timestep:150, pyg_AUC: 0.4831\n",
      "timestep:151, pyg_AUC: 0.4845\n",
      "timestep:152, pyg_AUC: 0.4859\n",
      "timestep:153, pyg_AUC: 0.4816\n",
      "timestep:154, pyg_AUC: 0.4944\n",
      "timestep:155, pyg_AUC: 0.4831\n",
      "timestep:156, pyg_AUC: 0.4929\n",
      "timestep:157, pyg_AUC: 0.4788\n",
      "timestep:158, pyg_AUC: 0.4831\n",
      "timestep:159, pyg_AUC: 0.4887\n",
      "timestep:160, pyg_AUC: 0.4873\n",
      "timestep:161, pyg_AUC: 0.4873\n",
      "timestep:162, pyg_AUC: 0.4929\n",
      "timestep:163, pyg_AUC: 0.4901\n",
      "timestep:164, pyg_AUC: 0.4873\n",
      "timestep:165, pyg_AUC: 0.4901\n",
      "timestep:166, pyg_AUC: 0.4901\n",
      "timestep:167, pyg_AUC: 0.4915\n",
      "timestep:168, pyg_AUC: 0.4831\n",
      "timestep:169, pyg_AUC: 0.4873\n",
      "timestep:170, pyg_AUC: 0.4873\n",
      "timestep:171, pyg_AUC: 0.4915\n",
      "timestep:172, pyg_AUC: 0.4873\n",
      "timestep:173, pyg_AUC: 0.4873\n",
      "timestep:174, pyg_AUC: 0.4831\n",
      "timestep:175, pyg_AUC: 0.4901\n",
      "timestep:176, pyg_AUC: 0.4929\n",
      "timestep:177, pyg_AUC: 0.4915\n",
      "timestep:178, pyg_AUC: 0.4929\n",
      "timestep:179, pyg_AUC: 0.4929\n",
      "timestep:180, pyg_AUC: 0.4859\n",
      "timestep:181, pyg_AUC: 0.4845\n",
      "timestep:182, pyg_AUC: 0.4944\n",
      "timestep:183, pyg_AUC: 0.4915\n",
      "timestep:184, pyg_AUC: 0.4873\n",
      "timestep:185, pyg_AUC: 0.4915\n",
      "timestep:186, pyg_AUC: 0.4788\n",
      "timestep:187, pyg_AUC: 0.4929\n",
      "timestep:188, pyg_AUC: 0.4873\n",
      "timestep:189, pyg_AUC: 0.4944\n",
      "timestep:190, pyg_AUC: 0.4887\n",
      "timestep:191, pyg_AUC: 0.4859\n",
      "timestep:192, pyg_AUC: 0.4873\n",
      "timestep:193, pyg_AUC: 0.4831\n",
      "timestep:194, pyg_AUC: 0.4887\n",
      "timestep:195, pyg_AUC: 0.4873\n",
      "timestep:196, pyg_AUC: 0.4816\n",
      "timestep:197, pyg_AUC: 0.4859\n",
      "timestep:198, pyg_AUC: 0.4887\n",
      "timestep:199, pyg_AUC: 0.4915\n",
      "timestep:200, pyg_AUC: 0.4845\n",
      "timestep:201, pyg_AUC: 0.4887\n",
      "timestep:202, pyg_AUC: 0.4901\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4887\n",
      "timestep:205, pyg_AUC: 0.4845\n",
      "timestep:206, pyg_AUC: 0.4859\n",
      "timestep:207, pyg_AUC: 0.4972\n",
      "timestep:208, pyg_AUC: 0.4816\n",
      "timestep:209, pyg_AUC: 0.4944\n",
      "timestep:210, pyg_AUC: 0.4845\n",
      "timestep:211, pyg_AUC: 0.4873\n",
      "timestep:212, pyg_AUC: 0.4831\n",
      "timestep:213, pyg_AUC: 0.4915\n",
      "timestep:214, pyg_AUC: 0.4859\n",
      "timestep:215, pyg_AUC: 0.4901\n",
      "timestep:216, pyg_AUC: 0.4887\n",
      "timestep:217, pyg_AUC: 0.4845\n",
      "timestep:218, pyg_AUC: 0.4873\n",
      "timestep:219, pyg_AUC: 0.4901\n",
      "timestep:220, pyg_AUC: 0.4873\n",
      "timestep:221, pyg_AUC: 0.4831\n",
      "timestep:222, pyg_AUC: 0.4873\n",
      "timestep:223, pyg_AUC: 0.4915\n",
      "timestep:224, pyg_AUC: 0.4845\n",
      "timestep:225, pyg_AUC: 0.4915\n",
      "timestep:226, pyg_AUC: 0.4873\n",
      "timestep:227, pyg_AUC: 0.4816\n",
      "timestep:228, pyg_AUC: 0.4831\n",
      "timestep:229, pyg_AUC: 0.4915\n",
      "timestep:230, pyg_AUC: 0.4873\n",
      "timestep:231, pyg_AUC: 0.4929\n",
      "timestep:232, pyg_AUC: 0.4929\n",
      "timestep:233, pyg_AUC: 0.4929\n",
      "timestep:234, pyg_AUC: 0.4859\n",
      "timestep:235, pyg_AUC: 0.4859\n",
      "timestep:236, pyg_AUC: 0.4887\n",
      "timestep:237, pyg_AUC: 0.4901\n",
      "timestep:238, pyg_AUC: 0.4873\n",
      "timestep:239, pyg_AUC: 0.4845\n",
      "timestep:240, pyg_AUC: 0.4929\n",
      "timestep:241, pyg_AUC: 0.4915\n",
      "timestep:242, pyg_AUC: 0.4845\n",
      "timestep:243, pyg_AUC: 0.4831\n",
      "timestep:244, pyg_AUC: 0.4816\n",
      "timestep:245, pyg_AUC: 0.4915\n",
      "timestep:246, pyg_AUC: 0.4859\n",
      "timestep:247, pyg_AUC: 0.4845\n",
      "timestep:248, pyg_AUC: 0.4873\n",
      "timestep:249, pyg_AUC: 0.4887\n",
      "timestep:250, pyg_AUC: 0.4845\n",
      "timestep:251, pyg_AUC: 0.4845\n",
      "timestep:252, pyg_AUC: 0.4845\n",
      "timestep:253, pyg_AUC: 0.4788\n",
      "timestep:254, pyg_AUC: 0.4887\n",
      "timestep:255, pyg_AUC: 0.4915\n",
      "timestep:256, pyg_AUC: 0.4873\n",
      "timestep:257, pyg_AUC: 0.4831\n",
      "timestep:258, pyg_AUC: 0.4873\n",
      "timestep:259, pyg_AUC: 0.4859\n",
      "timestep:260, pyg_AUC: 0.4915\n",
      "timestep:261, pyg_AUC: 0.4944\n",
      "timestep:262, pyg_AUC: 0.4915\n",
      "timestep:263, pyg_AUC: 0.4873\n",
      "timestep:264, pyg_AUC: 0.4831\n",
      "timestep:265, pyg_AUC: 0.4915\n",
      "timestep:266, pyg_AUC: 0.4845\n",
      "timestep:267, pyg_AUC: 0.4915\n",
      "timestep:268, pyg_AUC: 0.4859\n",
      "timestep:269, pyg_AUC: 0.4929\n",
      "timestep:270, pyg_AUC: 0.4831\n",
      "timestep:271, pyg_AUC: 0.4915\n",
      "timestep:272, pyg_AUC: 0.4873\n",
      "timestep:273, pyg_AUC: 0.4831\n",
      "timestep:274, pyg_AUC: 0.4845\n",
      "timestep:275, pyg_AUC: 0.4887\n",
      "timestep:276, pyg_AUC: 0.4873\n",
      "timestep:277, pyg_AUC: 0.4816\n",
      "timestep:278, pyg_AUC: 0.4831\n",
      "timestep:279, pyg_AUC: 0.4901\n",
      "timestep:280, pyg_AUC: 0.4873\n",
      "timestep:281, pyg_AUC: 0.4859\n",
      "timestep:282, pyg_AUC: 0.4859\n",
      "timestep:283, pyg_AUC: 0.4901\n",
      "timestep:284, pyg_AUC: 0.4901\n",
      "timestep:285, pyg_AUC: 0.4887\n",
      "timestep:286, pyg_AUC: 0.4873\n",
      "timestep:287, pyg_AUC: 0.4873\n",
      "timestep:288, pyg_AUC: 0.4873\n",
      "timestep:289, pyg_AUC: 0.4887\n",
      "timestep:290, pyg_AUC: 0.4845\n",
      "timestep:291, pyg_AUC: 0.4887\n",
      "timestep:292, pyg_AUC: 0.4859\n",
      "timestep:293, pyg_AUC: 0.4845\n",
      "timestep:294, pyg_AUC: 0.4873\n",
      "timestep:295, pyg_AUC: 0.4859\n",
      "timestep:296, pyg_AUC: 0.4887\n",
      "timestep:297, pyg_AUC: 0.4901\n",
      "timestep:298, pyg_AUC: 0.4901\n",
      "timestep:299, pyg_AUC: 0.4901\n",
      "timestep:300, pyg_AUC: 0.4845\n",
      "timestep:301, pyg_AUC: 0.4915\n",
      "timestep:302, pyg_AUC: 0.4873\n",
      "timestep:303, pyg_AUC: 0.4901\n",
      "timestep:304, pyg_AUC: 0.4873\n",
      "timestep:305, pyg_AUC: 0.4859\n",
      "timestep:306, pyg_AUC: 0.4901\n",
      "timestep:307, pyg_AUC: 0.4873\n",
      "timestep:308, pyg_AUC: 0.4816\n",
      "timestep:309, pyg_AUC: 0.4901\n",
      "timestep:310, pyg_AUC: 0.4873\n",
      "timestep:311, pyg_AUC: 0.4845\n",
      "timestep:312, pyg_AUC: 0.4873\n",
      "timestep:313, pyg_AUC: 0.4873\n",
      "timestep:314, pyg_AUC: 0.4859\n",
      "timestep:315, pyg_AUC: 0.4901\n",
      "timestep:316, pyg_AUC: 0.4887\n",
      "timestep:317, pyg_AUC: 0.4887\n",
      "timestep:318, pyg_AUC: 0.4901\n",
      "timestep:319, pyg_AUC: 0.4887\n",
      "timestep:320, pyg_AUC: 0.4873\n",
      "timestep:321, pyg_AUC: 0.4901\n",
      "timestep:322, pyg_AUC: 0.4901\n",
      "timestep:323, pyg_AUC: 0.4831\n",
      "timestep:324, pyg_AUC: 0.4831\n",
      "timestep:325, pyg_AUC: 0.4901\n",
      "timestep:326, pyg_AUC: 0.4873\n",
      "timestep:327, pyg_AUC: 0.4859\n",
      "timestep:328, pyg_AUC: 0.4887\n",
      "timestep:329, pyg_AUC: 0.4859\n",
      "timestep:330, pyg_AUC: 0.4845\n",
      "timestep:331, pyg_AUC: 0.4859\n",
      "timestep:332, pyg_AUC: 0.4831\n",
      "timestep:333, pyg_AUC: 0.4915\n",
      "timestep:334, pyg_AUC: 0.4873\n",
      "timestep:335, pyg_AUC: 0.4845\n",
      "timestep:336, pyg_AUC: 0.4873\n",
      "timestep:337, pyg_AUC: 0.4845\n",
      "timestep:338, pyg_AUC: 0.4929\n",
      "timestep:339, pyg_AUC: 0.4887\n",
      "timestep:340, pyg_AUC: 0.4845\n",
      "timestep:341, pyg_AUC: 0.4873\n",
      "timestep:342, pyg_AUC: 0.4845\n",
      "timestep:343, pyg_AUC: 0.4788\n",
      "timestep:344, pyg_AUC: 0.4887\n",
      "timestep:345, pyg_AUC: 0.4873\n",
      "timestep:346, pyg_AUC: 0.4859\n",
      "timestep:347, pyg_AUC: 0.4831\n",
      "timestep:348, pyg_AUC: 0.4845\n",
      "timestep:349, pyg_AUC: 0.4831\n",
      "timestep:350, pyg_AUC: 0.4873\n",
      "timestep:351, pyg_AUC: 0.4831\n",
      "timestep:352, pyg_AUC: 0.4901\n",
      "timestep:353, pyg_AUC: 0.4873\n",
      "timestep:354, pyg_AUC: 0.4802\n",
      "timestep:355, pyg_AUC: 0.4845\n",
      "timestep:356, pyg_AUC: 0.4859\n",
      "timestep:357, pyg_AUC: 0.4901\n",
      "timestep:358, pyg_AUC: 0.4887\n",
      "timestep:359, pyg_AUC: 0.4901\n",
      "timestep:360, pyg_AUC: 0.4859\n",
      "timestep:361, pyg_AUC: 0.4887\n",
      "timestep:362, pyg_AUC: 0.4845\n",
      "timestep:363, pyg_AUC: 0.4859\n",
      "timestep:364, pyg_AUC: 0.4901\n",
      "timestep:365, pyg_AUC: 0.4887\n",
      "timestep:366, pyg_AUC: 0.4901\n",
      "timestep:367, pyg_AUC: 0.4873\n",
      "timestep:368, pyg_AUC: 0.4859\n",
      "timestep:369, pyg_AUC: 0.4845\n",
      "timestep:370, pyg_AUC: 0.4859\n",
      "timestep:371, pyg_AUC: 0.4845\n",
      "timestep:372, pyg_AUC: 0.4915\n",
      "timestep:373, pyg_AUC: 0.4873\n",
      "timestep:374, pyg_AUC: 0.4816\n",
      "timestep:375, pyg_AUC: 0.4845\n",
      "timestep:376, pyg_AUC: 0.4887\n",
      "timestep:377, pyg_AUC: 0.4816\n",
      "timestep:378, pyg_AUC: 0.4887\n",
      "timestep:379, pyg_AUC: 0.4831\n",
      "timestep:380, pyg_AUC: 0.4873\n",
      "timestep:381, pyg_AUC: 0.4873\n",
      "timestep:382, pyg_AUC: 0.4873\n",
      "timestep:383, pyg_AUC: 0.4873\n",
      "timestep:384, pyg_AUC: 0.4873\n",
      "timestep:385, pyg_AUC: 0.4873\n",
      "timestep:386, pyg_AUC: 0.4887\n",
      "timestep:387, pyg_AUC: 0.4873\n",
      "timestep:388, pyg_AUC: 0.4887\n",
      "timestep:389, pyg_AUC: 0.4845\n",
      "timestep:390, pyg_AUC: 0.4845\n",
      "timestep:391, pyg_AUC: 0.4901\n",
      "timestep:392, pyg_AUC: 0.4873\n",
      "timestep:393, pyg_AUC: 0.4873\n",
      "timestep:394, pyg_AUC: 0.4859\n",
      "timestep:395, pyg_AUC: 0.4845\n",
      "timestep:396, pyg_AUC: 0.4901\n",
      "timestep:397, pyg_AUC: 0.4873\n",
      "timestep:398, pyg_AUC: 0.4845\n",
      "timestep:399, pyg_AUC: 0.4816\n",
      "timestep:400, pyg_AUC: 0.4915\n",
      "timestep:401, pyg_AUC: 0.4859\n",
      "timestep:402, pyg_AUC: 0.4859\n",
      "timestep:403, pyg_AUC: 0.4873\n",
      "timestep:404, pyg_AUC: 0.4859\n",
      "timestep:405, pyg_AUC: 0.4873\n",
      "timestep:406, pyg_AUC: 0.4887\n",
      "timestep:407, pyg_AUC: 0.4831\n",
      "timestep:408, pyg_AUC: 0.4915\n",
      "timestep:409, pyg_AUC: 0.4859\n",
      "timestep:410, pyg_AUC: 0.4859\n",
      "timestep:411, pyg_AUC: 0.4901\n",
      "timestep:412, pyg_AUC: 0.4859\n",
      "timestep:413, pyg_AUC: 0.4859\n",
      "timestep:414, pyg_AUC: 0.4915\n",
      "timestep:415, pyg_AUC: 0.4859\n",
      "timestep:416, pyg_AUC: 0.4845\n",
      "timestep:417, pyg_AUC: 0.4859\n",
      "timestep:418, pyg_AUC: 0.4816\n",
      "timestep:419, pyg_AUC: 0.4873\n",
      "timestep:420, pyg_AUC: 0.4887\n",
      "timestep:421, pyg_AUC: 0.4845\n",
      "timestep:422, pyg_AUC: 0.4845\n",
      "timestep:423, pyg_AUC: 0.4845\n",
      "timestep:424, pyg_AUC: 0.4845\n",
      "timestep:425, pyg_AUC: 0.4901\n",
      "timestep:426, pyg_AUC: 0.4845\n",
      "timestep:427, pyg_AUC: 0.4915\n",
      "timestep:428, pyg_AUC: 0.4831\n",
      "timestep:429, pyg_AUC: 0.4859\n",
      "timestep:430, pyg_AUC: 0.4859\n",
      "timestep:431, pyg_AUC: 0.4859\n",
      "timestep:432, pyg_AUC: 0.4831\n",
      "timestep:433, pyg_AUC: 0.4831\n",
      "timestep:434, pyg_AUC: 0.4873\n",
      "timestep:435, pyg_AUC: 0.4859\n",
      "timestep:436, pyg_AUC: 0.4859\n",
      "timestep:437, pyg_AUC: 0.4845\n",
      "timestep:438, pyg_AUC: 0.4845\n",
      "timestep:439, pyg_AUC: 0.4859\n",
      "timestep:440, pyg_AUC: 0.4873\n",
      "timestep:441, pyg_AUC: 0.4816\n",
      "timestep:442, pyg_AUC: 0.4887\n",
      "timestep:443, pyg_AUC: 0.4901\n",
      "timestep:444, pyg_AUC: 0.4887\n",
      "timestep:445, pyg_AUC: 0.4859\n",
      "timestep:446, pyg_AUC: 0.4901\n",
      "timestep:447, pyg_AUC: 0.4845\n",
      "timestep:448, pyg_AUC: 0.4873\n",
      "timestep:449, pyg_AUC: 0.4873\n",
      "timestep:450, pyg_AUC: 0.4859\n",
      "timestep:451, pyg_AUC: 0.4845\n",
      "timestep:452, pyg_AUC: 0.4873\n",
      "timestep:453, pyg_AUC: 0.4873\n",
      "timestep:454, pyg_AUC: 0.4816\n",
      "timestep:455, pyg_AUC: 0.4845\n",
      "timestep:456, pyg_AUC: 0.4831\n",
      "timestep:457, pyg_AUC: 0.4873\n",
      "timestep:458, pyg_AUC: 0.4845\n",
      "timestep:459, pyg_AUC: 0.4887\n",
      "timestep:460, pyg_AUC: 0.4859\n",
      "timestep:461, pyg_AUC: 0.4901\n",
      "timestep:462, pyg_AUC: 0.4887\n",
      "timestep:463, pyg_AUC: 0.4845\n",
      "timestep:464, pyg_AUC: 0.4859\n",
      "timestep:465, pyg_AUC: 0.4859\n",
      "timestep:466, pyg_AUC: 0.4887\n",
      "timestep:467, pyg_AUC: 0.4887\n",
      "timestep:468, pyg_AUC: 0.4901\n",
      "timestep:469, pyg_AUC: 0.4873\n",
      "timestep:470, pyg_AUC: 0.4845\n",
      "timestep:471, pyg_AUC: 0.4845\n",
      "timestep:472, pyg_AUC: 0.4845\n",
      "timestep:473, pyg_AUC: 0.4845\n",
      "timestep:474, pyg_AUC: 0.4831\n",
      "timestep:475, pyg_AUC: 0.4816\n",
      "timestep:476, pyg_AUC: 0.4831\n",
      "timestep:477, pyg_AUC: 0.4831\n",
      "timestep:478, pyg_AUC: 0.4831\n",
      "timestep:479, pyg_AUC: 0.4873\n",
      "timestep:480, pyg_AUC: 0.4901\n",
      "timestep:481, pyg_AUC: 0.4887\n",
      "timestep:482, pyg_AUC: 0.4915\n",
      "timestep:483, pyg_AUC: 0.4873\n",
      "timestep:484, pyg_AUC: 0.4901\n",
      "timestep:485, pyg_AUC: 0.4845\n",
      "timestep:486, pyg_AUC: 0.4859\n",
      "timestep:487, pyg_AUC: 0.4873\n",
      "timestep:488, pyg_AUC: 0.4887\n",
      "timestep:489, pyg_AUC: 0.4887\n",
      "timestep:490, pyg_AUC: 0.4915\n",
      "timestep:491, pyg_AUC: 0.4887\n",
      "timestep:492, pyg_AUC: 0.4873\n",
      "timestep:493, pyg_AUC: 0.4873\n",
      "timestep:494, pyg_AUC: 0.4901\n",
      "timestep:495, pyg_AUC: 0.4915\n",
      "timestep:496, pyg_AUC: 0.4859\n",
      "timestep:497, pyg_AUC: 0.4859\n",
      "timestep:498, pyg_AUC: 0.4845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [28:48<05:06, 102.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:499, pyg_AUC: 0.4859\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 35.14841\n",
      "Epoch: 0010 loss= 34.99444\n",
      "Epoch: 0020 loss= 22.03244\n",
      "Epoch: 0030 loss= 16.98173\n",
      "Epoch: 0040 loss= 13.87246\n",
      "Epoch: 0050 loss= 6.01774\n",
      "Epoch: 0060 loss= 1.51885\n",
      "Epoch: 0070 loss= 0.91288\n",
      "Epoch: 0080 loss= 0.66664\n",
      "Epoch: 0090 loss= 0.97027\n",
      "Epoch: 0100 loss= 0.68271\n",
      "Epoch: 0110 loss= 0.57238\n",
      "Epoch: 0120 loss= 0.62101\n",
      "Epoch: 0130 loss= 0.65714\n",
      "Epoch: 0140 loss= 0.60411\n",
      "Epoch: 0150 loss= 0.53573\n",
      "Epoch: 0160 loss= 0.67983\n",
      "Epoch: 0170 loss= 0.61286\n",
      "Epoch: 0180 loss= 0.58674\n",
      "Epoch: 0190 loss= 0.58891\n",
      "Epoch: 0200 loss= 0.53657\n",
      "Epoch: 0210 loss= 0.53797\n",
      "Epoch: 0220 loss= 0.51880\n",
      "Epoch: 0230 loss= 0.53463\n",
      "Epoch: 0240 loss= 0.52000\n",
      "Epoch: 0250 loss= 0.56848\n",
      "Epoch: 0260 loss= 0.50461\n",
      "Epoch: 0270 loss= 0.54934\n",
      "Epoch: 0280 loss= 0.54307\n",
      "Epoch: 0290 loss= 0.56943\n",
      "Epoch: 0300 loss= 0.53307\n",
      "Epoch: 0310 loss= 0.45466\n",
      "Epoch: 0320 loss= 0.48800\n",
      "Epoch: 0330 loss= 0.53185\n",
      "Epoch: 0340 loss= 0.54679\n",
      "Epoch: 0350 loss= 0.55088\n",
      "Epoch: 0360 loss= 0.53566\n",
      "Epoch: 0370 loss= 0.47334\n",
      "Epoch: 0380 loss= 0.49300\n",
      "Epoch: 0390 loss= 0.49332\n",
      "Epoch: 0400 loss= 0.48553\n",
      "Epoch: 0410 loss= 0.47515\n",
      "Epoch: 0420 loss= 0.41608\n",
      "Epoch: 0430 loss= 0.54177\n",
      "Epoch: 0440 loss= 0.46217\n",
      "Epoch: 0450 loss= 0.42292\n",
      "Epoch: 0460 loss= 0.50519\n",
      "Epoch: 0470 loss= 0.44121\n",
      "Epoch: 0480 loss= 0.55488\n",
      "Epoch: 0490 loss= 0.45175\n",
      "Epoch: 0500 loss= 0.52423\n",
      "Epoch: 0510 loss= 0.49368\n",
      "Epoch: 0520 loss= 0.42911\n",
      "Epoch: 0530 loss= 0.48516\n",
      "Epoch: 0540 loss= 0.50747\n",
      "Epoch: 0550 loss= 0.52593\n",
      "Epoch: 0560 loss= 0.52277\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7882,  4.7529, -5.3764, -4.6478, -4.4339,  5.6672,  5.4976, -5.2633]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 43.98753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss= 25.78571\n",
      "Epoch: 0020 loss= 13.88853\n",
      "Epoch: 0030 loss= 3.97133\n",
      "Epoch: 0040 loss= 1.88572\n",
      "Epoch: 0050 loss= 1.00469\n",
      "Epoch: 0060 loss= 0.73359\n",
      "Epoch: 0070 loss= 0.68733\n",
      "Epoch: 0080 loss= 0.67551\n",
      "Epoch: 0090 loss= 0.58386\n",
      "Epoch: 0100 loss= 0.60935\n",
      "Epoch: 0110 loss= 0.64305\n",
      "Epoch: 0120 loss= 0.59034\n",
      "Epoch: 0130 loss= 0.65237\n",
      "Epoch: 0140 loss= 0.60386\n",
      "Epoch: 0150 loss= 0.55887\n",
      "Epoch: 0160 loss= 0.62363\n",
      "Epoch: 0170 loss= 0.61377\n",
      "Epoch: 0180 loss= 0.72035\n",
      "Epoch: 0190 loss= 0.61290\n",
      "Epoch: 0200 loss= 0.62514\n",
      "Epoch: 0210 loss= 0.55095\n",
      "Epoch: 0220 loss= 0.56740\n",
      "Epoch: 0230 loss= 0.57327\n",
      "Epoch: 0240 loss= 0.57332\n",
      "Epoch: 0250 loss= 0.56735\n",
      "Epoch: 0260 loss= 0.57198\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4929\n",
      "timestep:1, pyg_AUC: 0.4887\n",
      "timestep:2, pyg_AUC: 0.4873\n",
      "timestep:3, pyg_AUC: 0.4831\n",
      "timestep:4, pyg_AUC: 0.4859\n",
      "timestep:5, pyg_AUC: 0.4901\n",
      "timestep:6, pyg_AUC: 0.4887\n",
      "timestep:7, pyg_AUC: 0.4873\n",
      "timestep:8, pyg_AUC: 0.4873\n",
      "timestep:9, pyg_AUC: 0.4901\n",
      "timestep:10, pyg_AUC: 0.4887\n",
      "timestep:11, pyg_AUC: 0.4873\n",
      "timestep:12, pyg_AUC: 0.4944\n",
      "timestep:13, pyg_AUC: 0.4887\n",
      "timestep:14, pyg_AUC: 0.4915\n",
      "timestep:15, pyg_AUC: 0.4915\n",
      "timestep:16, pyg_AUC: 0.4915\n",
      "timestep:17, pyg_AUC: 0.4944\n",
      "timestep:18, pyg_AUC: 0.4944\n",
      "timestep:19, pyg_AUC: 0.4873\n",
      "timestep:20, pyg_AUC: 0.4901\n",
      "timestep:21, pyg_AUC: 0.4901\n",
      "timestep:22, pyg_AUC: 0.4901\n",
      "timestep:23, pyg_AUC: 0.4887\n",
      "timestep:24, pyg_AUC: 0.4915\n",
      "timestep:25, pyg_AUC: 0.4958\n",
      "timestep:26, pyg_AUC: 0.4873\n",
      "timestep:27, pyg_AUC: 0.4859\n",
      "timestep:28, pyg_AUC: 0.4901\n",
      "timestep:29, pyg_AUC: 0.4915\n",
      "timestep:30, pyg_AUC: 0.4859\n",
      "timestep:31, pyg_AUC: 0.4887\n",
      "timestep:32, pyg_AUC: 0.4915\n",
      "timestep:33, pyg_AUC: 0.4929\n",
      "timestep:34, pyg_AUC: 0.4873\n",
      "timestep:35, pyg_AUC: 0.4915\n",
      "timestep:36, pyg_AUC: 0.4915\n",
      "timestep:37, pyg_AUC: 0.4873\n",
      "timestep:38, pyg_AUC: 0.4873\n",
      "timestep:39, pyg_AUC: 0.4859\n",
      "timestep:40, pyg_AUC: 0.4915\n",
      "timestep:41, pyg_AUC: 0.4859\n",
      "timestep:42, pyg_AUC: 0.4915\n",
      "timestep:43, pyg_AUC: 0.4859\n",
      "timestep:44, pyg_AUC: 0.4859\n",
      "timestep:45, pyg_AUC: 0.4915\n",
      "timestep:46, pyg_AUC: 0.4887\n",
      "timestep:47, pyg_AUC: 0.4887\n",
      "timestep:48, pyg_AUC: 0.4944\n",
      "timestep:49, pyg_AUC: 0.4859\n",
      "timestep:50, pyg_AUC: 0.4929\n",
      "timestep:51, pyg_AUC: 0.4915\n",
      "timestep:52, pyg_AUC: 0.4873\n",
      "timestep:53, pyg_AUC: 0.4887\n",
      "timestep:54, pyg_AUC: 0.4915\n",
      "timestep:55, pyg_AUC: 0.4873\n",
      "timestep:56, pyg_AUC: 0.4915\n",
      "timestep:57, pyg_AUC: 0.4873\n",
      "timestep:58, pyg_AUC: 0.4915\n",
      "timestep:59, pyg_AUC: 0.4873\n",
      "timestep:60, pyg_AUC: 0.4915\n",
      "timestep:61, pyg_AUC: 0.4915\n",
      "timestep:62, pyg_AUC: 0.4873\n",
      "timestep:63, pyg_AUC: 0.4901\n",
      "timestep:64, pyg_AUC: 0.4915\n",
      "timestep:65, pyg_AUC: 0.4901\n",
      "timestep:66, pyg_AUC: 0.4915\n",
      "timestep:67, pyg_AUC: 0.4944\n",
      "timestep:68, pyg_AUC: 0.4929\n",
      "timestep:69, pyg_AUC: 0.4915\n",
      "timestep:70, pyg_AUC: 0.4915\n",
      "timestep:71, pyg_AUC: 0.4901\n",
      "timestep:72, pyg_AUC: 0.4901\n",
      "timestep:73, pyg_AUC: 0.4887\n",
      "timestep:74, pyg_AUC: 0.4915\n",
      "timestep:75, pyg_AUC: 0.4873\n",
      "timestep:76, pyg_AUC: 0.4887\n",
      "timestep:77, pyg_AUC: 0.4901\n",
      "timestep:78, pyg_AUC: 0.4915\n",
      "timestep:79, pyg_AUC: 0.4887\n",
      "timestep:80, pyg_AUC: 0.4929\n",
      "timestep:81, pyg_AUC: 0.4944\n",
      "timestep:82, pyg_AUC: 0.4887\n",
      "timestep:83, pyg_AUC: 0.4929\n",
      "timestep:84, pyg_AUC: 0.4901\n",
      "timestep:85, pyg_AUC: 0.4901\n",
      "timestep:86, pyg_AUC: 0.4929\n",
      "timestep:87, pyg_AUC: 0.4873\n",
      "timestep:88, pyg_AUC: 0.4929\n",
      "timestep:89, pyg_AUC: 0.4915\n",
      "timestep:90, pyg_AUC: 0.4915\n",
      "timestep:91, pyg_AUC: 0.4929\n",
      "timestep:92, pyg_AUC: 0.4873\n",
      "timestep:93, pyg_AUC: 0.4901\n",
      "timestep:94, pyg_AUC: 0.4901\n",
      "timestep:95, pyg_AUC: 0.4929\n",
      "timestep:96, pyg_AUC: 0.4929\n",
      "timestep:97, pyg_AUC: 0.4915\n",
      "timestep:98, pyg_AUC: 0.4929\n",
      "timestep:99, pyg_AUC: 0.4929\n",
      "timestep:100, pyg_AUC: 0.4873\n",
      "timestep:101, pyg_AUC: 0.4944\n",
      "timestep:102, pyg_AUC: 0.4859\n",
      "timestep:103, pyg_AUC: 0.4915\n",
      "timestep:104, pyg_AUC: 0.4887\n",
      "timestep:105, pyg_AUC: 0.4915\n",
      "timestep:106, pyg_AUC: 0.4929\n",
      "timestep:107, pyg_AUC: 0.4887\n",
      "timestep:108, pyg_AUC: 0.4915\n",
      "timestep:109, pyg_AUC: 0.4915\n",
      "timestep:110, pyg_AUC: 0.4915\n",
      "timestep:111, pyg_AUC: 0.4859\n",
      "timestep:112, pyg_AUC: 0.4901\n",
      "timestep:113, pyg_AUC: 0.4915\n",
      "timestep:114, pyg_AUC: 0.4915\n",
      "timestep:115, pyg_AUC: 0.4915\n",
      "timestep:116, pyg_AUC: 0.4915\n",
      "timestep:117, pyg_AUC: 0.4901\n",
      "timestep:118, pyg_AUC: 0.4915\n",
      "timestep:119, pyg_AUC: 0.4901\n",
      "timestep:120, pyg_AUC: 0.4859\n",
      "timestep:121, pyg_AUC: 0.4929\n",
      "timestep:122, pyg_AUC: 0.4929\n",
      "timestep:123, pyg_AUC: 0.4887\n",
      "timestep:124, pyg_AUC: 0.4915\n",
      "timestep:125, pyg_AUC: 0.4915\n",
      "timestep:126, pyg_AUC: 0.4915\n",
      "timestep:127, pyg_AUC: 0.4915\n",
      "timestep:128, pyg_AUC: 0.4901\n",
      "timestep:129, pyg_AUC: 0.4915\n",
      "timestep:130, pyg_AUC: 0.4887\n",
      "timestep:131, pyg_AUC: 0.4929\n",
      "timestep:132, pyg_AUC: 0.4915\n",
      "timestep:133, pyg_AUC: 0.4915\n",
      "timestep:134, pyg_AUC: 0.4901\n",
      "timestep:135, pyg_AUC: 0.4944\n",
      "timestep:136, pyg_AUC: 0.4915\n",
      "timestep:137, pyg_AUC: 0.4901\n",
      "timestep:138, pyg_AUC: 0.4915\n",
      "timestep:139, pyg_AUC: 0.4887\n",
      "timestep:140, pyg_AUC: 0.4944\n",
      "timestep:141, pyg_AUC: 0.4915\n",
      "timestep:142, pyg_AUC: 0.4915\n",
      "timestep:143, pyg_AUC: 0.4915\n",
      "timestep:144, pyg_AUC: 0.4915\n",
      "timestep:145, pyg_AUC: 0.4929\n",
      "timestep:146, pyg_AUC: 0.4915\n",
      "timestep:147, pyg_AUC: 0.4915\n",
      "timestep:148, pyg_AUC: 0.4944\n",
      "timestep:149, pyg_AUC: 0.4915\n",
      "timestep:150, pyg_AUC: 0.4901\n",
      "timestep:151, pyg_AUC: 0.4901\n",
      "timestep:152, pyg_AUC: 0.4915\n",
      "timestep:153, pyg_AUC: 0.4929\n",
      "timestep:154, pyg_AUC: 0.4915\n",
      "timestep:155, pyg_AUC: 0.4887\n",
      "timestep:156, pyg_AUC: 0.4901\n",
      "timestep:157, pyg_AUC: 0.4901\n",
      "timestep:158, pyg_AUC: 0.4887\n",
      "timestep:159, pyg_AUC: 0.4929\n",
      "timestep:160, pyg_AUC: 0.4901\n",
      "timestep:161, pyg_AUC: 0.4915\n",
      "timestep:162, pyg_AUC: 0.4915\n",
      "timestep:163, pyg_AUC: 0.4915\n",
      "timestep:164, pyg_AUC: 0.4901\n",
      "timestep:165, pyg_AUC: 0.4915\n",
      "timestep:166, pyg_AUC: 0.4915\n",
      "timestep:167, pyg_AUC: 0.4915\n",
      "timestep:168, pyg_AUC: 0.4915\n",
      "timestep:169, pyg_AUC: 0.4873\n",
      "timestep:170, pyg_AUC: 0.4887\n",
      "timestep:171, pyg_AUC: 0.4873\n",
      "timestep:172, pyg_AUC: 0.4915\n",
      "timestep:173, pyg_AUC: 0.4901\n",
      "timestep:174, pyg_AUC: 0.4901\n",
      "timestep:175, pyg_AUC: 0.4873\n",
      "timestep:176, pyg_AUC: 0.4901\n",
      "timestep:177, pyg_AUC: 0.4929\n",
      "timestep:178, pyg_AUC: 0.4901\n",
      "timestep:179, pyg_AUC: 0.4901\n",
      "timestep:180, pyg_AUC: 0.4901\n",
      "timestep:181, pyg_AUC: 0.4887\n",
      "timestep:182, pyg_AUC: 0.4901\n",
      "timestep:183, pyg_AUC: 0.4901\n",
      "timestep:184, pyg_AUC: 0.4887\n",
      "timestep:185, pyg_AUC: 0.4873\n",
      "timestep:186, pyg_AUC: 0.4887\n",
      "timestep:187, pyg_AUC: 0.4901\n",
      "timestep:188, pyg_AUC: 0.4887\n",
      "timestep:189, pyg_AUC: 0.4887\n",
      "timestep:190, pyg_AUC: 0.4887\n",
      "timestep:191, pyg_AUC: 0.4887\n",
      "timestep:192, pyg_AUC: 0.4887\n",
      "timestep:193, pyg_AUC: 0.4873\n",
      "timestep:194, pyg_AUC: 0.4915\n",
      "timestep:195, pyg_AUC: 0.4915\n",
      "timestep:196, pyg_AUC: 0.4887\n",
      "timestep:197, pyg_AUC: 0.4887\n",
      "timestep:198, pyg_AUC: 0.4887\n",
      "timestep:199, pyg_AUC: 0.4901\n",
      "timestep:200, pyg_AUC: 0.4901\n",
      "timestep:201, pyg_AUC: 0.4929\n",
      "timestep:202, pyg_AUC: 0.4887\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4901\n",
      "timestep:205, pyg_AUC: 0.4887\n",
      "timestep:206, pyg_AUC: 0.4901\n",
      "timestep:207, pyg_AUC: 0.4887\n",
      "timestep:208, pyg_AUC: 0.4901\n",
      "timestep:209, pyg_AUC: 0.4901\n",
      "timestep:210, pyg_AUC: 0.4901\n",
      "timestep:211, pyg_AUC: 0.4873\n",
      "timestep:212, pyg_AUC: 0.4873\n",
      "timestep:213, pyg_AUC: 0.4901\n",
      "timestep:214, pyg_AUC: 0.4887\n",
      "timestep:215, pyg_AUC: 0.4901\n",
      "timestep:216, pyg_AUC: 0.4873\n",
      "timestep:217, pyg_AUC: 0.4887\n",
      "timestep:218, pyg_AUC: 0.4887\n",
      "timestep:219, pyg_AUC: 0.4887\n",
      "timestep:220, pyg_AUC: 0.4887\n",
      "timestep:221, pyg_AUC: 0.4887\n",
      "timestep:222, pyg_AUC: 0.4915\n",
      "timestep:223, pyg_AUC: 0.4887\n",
      "timestep:224, pyg_AUC: 0.4901\n",
      "timestep:225, pyg_AUC: 0.4929\n",
      "timestep:226, pyg_AUC: 0.4887\n",
      "timestep:227, pyg_AUC: 0.4887\n",
      "timestep:228, pyg_AUC: 0.4901\n",
      "timestep:229, pyg_AUC: 0.4901\n",
      "timestep:230, pyg_AUC: 0.4901\n",
      "timestep:231, pyg_AUC: 0.4887\n",
      "timestep:232, pyg_AUC: 0.4887\n",
      "timestep:233, pyg_AUC: 0.4887\n",
      "timestep:234, pyg_AUC: 0.4887\n",
      "timestep:235, pyg_AUC: 0.4887\n",
      "timestep:236, pyg_AUC: 0.4887\n",
      "timestep:237, pyg_AUC: 0.4915\n",
      "timestep:238, pyg_AUC: 0.4901\n",
      "timestep:239, pyg_AUC: 0.4873\n",
      "timestep:240, pyg_AUC: 0.4901\n",
      "timestep:241, pyg_AUC: 0.4901\n",
      "timestep:242, pyg_AUC: 0.4915\n",
      "timestep:243, pyg_AUC: 0.4887\n",
      "timestep:244, pyg_AUC: 0.4887\n",
      "timestep:245, pyg_AUC: 0.4887\n",
      "timestep:246, pyg_AUC: 0.4901\n",
      "timestep:247, pyg_AUC: 0.4901\n",
      "timestep:248, pyg_AUC: 0.4915\n",
      "timestep:249, pyg_AUC: 0.4873\n",
      "timestep:250, pyg_AUC: 0.4887\n",
      "timestep:251, pyg_AUC: 0.4901\n",
      "timestep:252, pyg_AUC: 0.4901\n",
      "timestep:253, pyg_AUC: 0.4901\n",
      "timestep:254, pyg_AUC: 0.4915\n",
      "timestep:255, pyg_AUC: 0.4901\n",
      "timestep:256, pyg_AUC: 0.4887\n",
      "timestep:257, pyg_AUC: 0.4901\n",
      "timestep:258, pyg_AUC: 0.4915\n",
      "timestep:259, pyg_AUC: 0.4901\n",
      "timestep:260, pyg_AUC: 0.4873\n",
      "timestep:261, pyg_AUC: 0.4887\n",
      "timestep:262, pyg_AUC: 0.4901\n",
      "timestep:263, pyg_AUC: 0.4901\n",
      "timestep:264, pyg_AUC: 0.4915\n",
      "timestep:265, pyg_AUC: 0.4859\n",
      "timestep:266, pyg_AUC: 0.4901\n",
      "timestep:267, pyg_AUC: 0.4901\n",
      "timestep:268, pyg_AUC: 0.4887\n",
      "timestep:269, pyg_AUC: 0.4859\n",
      "timestep:270, pyg_AUC: 0.4887\n",
      "timestep:271, pyg_AUC: 0.4887\n",
      "timestep:272, pyg_AUC: 0.4901\n",
      "timestep:273, pyg_AUC: 0.4901\n",
      "timestep:274, pyg_AUC: 0.4901\n",
      "timestep:275, pyg_AUC: 0.4859\n",
      "timestep:276, pyg_AUC: 0.4915\n",
      "timestep:277, pyg_AUC: 0.4887\n",
      "timestep:278, pyg_AUC: 0.4887\n",
      "timestep:279, pyg_AUC: 0.4873\n",
      "timestep:280, pyg_AUC: 0.4887\n",
      "timestep:281, pyg_AUC: 0.4873\n",
      "timestep:282, pyg_AUC: 0.4873\n",
      "timestep:283, pyg_AUC: 0.4901\n",
      "timestep:284, pyg_AUC: 0.4887\n",
      "timestep:285, pyg_AUC: 0.4873\n",
      "timestep:286, pyg_AUC: 0.4887\n",
      "timestep:287, pyg_AUC: 0.4901\n",
      "timestep:288, pyg_AUC: 0.4887\n",
      "timestep:289, pyg_AUC: 0.4887\n",
      "timestep:290, pyg_AUC: 0.4887\n",
      "timestep:291, pyg_AUC: 0.4901\n",
      "timestep:292, pyg_AUC: 0.4873\n",
      "timestep:293, pyg_AUC: 0.4873\n",
      "timestep:294, pyg_AUC: 0.4873\n",
      "timestep:295, pyg_AUC: 0.4873\n",
      "timestep:296, pyg_AUC: 0.4887\n",
      "timestep:297, pyg_AUC: 0.4887\n",
      "timestep:298, pyg_AUC: 0.4873\n",
      "timestep:299, pyg_AUC: 0.4873\n",
      "timestep:300, pyg_AUC: 0.4873\n",
      "timestep:301, pyg_AUC: 0.4845\n",
      "timestep:302, pyg_AUC: 0.4873\n",
      "timestep:303, pyg_AUC: 0.4859\n",
      "timestep:304, pyg_AUC: 0.4859\n",
      "timestep:305, pyg_AUC: 0.4887\n",
      "timestep:306, pyg_AUC: 0.4887\n",
      "timestep:307, pyg_AUC: 0.4873\n",
      "timestep:308, pyg_AUC: 0.4887\n",
      "timestep:309, pyg_AUC: 0.4873\n",
      "timestep:310, pyg_AUC: 0.4873\n",
      "timestep:311, pyg_AUC: 0.4887\n",
      "timestep:312, pyg_AUC: 0.4873\n",
      "timestep:313, pyg_AUC: 0.4859\n",
      "timestep:314, pyg_AUC: 0.4901\n",
      "timestep:315, pyg_AUC: 0.4845\n",
      "timestep:316, pyg_AUC: 0.4873\n",
      "timestep:317, pyg_AUC: 0.4901\n",
      "timestep:318, pyg_AUC: 0.4901\n",
      "timestep:319, pyg_AUC: 0.4873\n",
      "timestep:320, pyg_AUC: 0.4901\n",
      "timestep:321, pyg_AUC: 0.4859\n",
      "timestep:322, pyg_AUC: 0.4873\n",
      "timestep:323, pyg_AUC: 0.4873\n",
      "timestep:324, pyg_AUC: 0.4859\n",
      "timestep:325, pyg_AUC: 0.4859\n",
      "timestep:326, pyg_AUC: 0.4859\n",
      "timestep:327, pyg_AUC: 0.4873\n",
      "timestep:328, pyg_AUC: 0.4873\n",
      "timestep:329, pyg_AUC: 0.4873\n",
      "timestep:330, pyg_AUC: 0.4873\n",
      "timestep:331, pyg_AUC: 0.4887\n",
      "timestep:332, pyg_AUC: 0.4873\n",
      "timestep:333, pyg_AUC: 0.4901\n",
      "timestep:334, pyg_AUC: 0.4887\n",
      "timestep:335, pyg_AUC: 0.4873\n",
      "timestep:336, pyg_AUC: 0.4901\n",
      "timestep:337, pyg_AUC: 0.4887\n",
      "timestep:338, pyg_AUC: 0.4901\n",
      "timestep:339, pyg_AUC: 0.4887\n",
      "timestep:340, pyg_AUC: 0.4873\n",
      "timestep:341, pyg_AUC: 0.4873\n",
      "timestep:342, pyg_AUC: 0.4873\n",
      "timestep:343, pyg_AUC: 0.4887\n",
      "timestep:344, pyg_AUC: 0.4873\n",
      "timestep:345, pyg_AUC: 0.4887\n",
      "timestep:346, pyg_AUC: 0.4873\n",
      "timestep:347, pyg_AUC: 0.4901\n",
      "timestep:348, pyg_AUC: 0.4873\n",
      "timestep:349, pyg_AUC: 0.4887\n",
      "timestep:350, pyg_AUC: 0.4873\n",
      "timestep:351, pyg_AUC: 0.4901\n",
      "timestep:352, pyg_AUC: 0.4873\n",
      "timestep:353, pyg_AUC: 0.4901\n",
      "timestep:354, pyg_AUC: 0.4873\n",
      "timestep:355, pyg_AUC: 0.4887\n",
      "timestep:356, pyg_AUC: 0.4887\n",
      "timestep:357, pyg_AUC: 0.4873\n",
      "timestep:358, pyg_AUC: 0.4887\n",
      "timestep:359, pyg_AUC: 0.4873\n",
      "timestep:360, pyg_AUC: 0.4859\n",
      "timestep:361, pyg_AUC: 0.4873\n",
      "timestep:362, pyg_AUC: 0.4873\n",
      "timestep:363, pyg_AUC: 0.4887\n",
      "timestep:364, pyg_AUC: 0.4887\n",
      "timestep:365, pyg_AUC: 0.4887\n",
      "timestep:366, pyg_AUC: 0.4873\n",
      "timestep:367, pyg_AUC: 0.4887\n",
      "timestep:368, pyg_AUC: 0.4873\n",
      "timestep:369, pyg_AUC: 0.4873\n",
      "timestep:370, pyg_AUC: 0.4887\n",
      "timestep:371, pyg_AUC: 0.4873\n",
      "timestep:372, pyg_AUC: 0.4901\n",
      "timestep:373, pyg_AUC: 0.4873\n",
      "timestep:374, pyg_AUC: 0.4901\n",
      "timestep:375, pyg_AUC: 0.4887\n",
      "timestep:376, pyg_AUC: 0.4887\n",
      "timestep:377, pyg_AUC: 0.4901\n",
      "timestep:378, pyg_AUC: 0.4915\n",
      "timestep:379, pyg_AUC: 0.4887\n",
      "timestep:380, pyg_AUC: 0.4887\n",
      "timestep:381, pyg_AUC: 0.4887\n",
      "timestep:382, pyg_AUC: 0.4887\n",
      "timestep:383, pyg_AUC: 0.4887\n",
      "timestep:384, pyg_AUC: 0.4887\n",
      "timestep:385, pyg_AUC: 0.4873\n",
      "timestep:386, pyg_AUC: 0.4887\n",
      "timestep:387, pyg_AUC: 0.4887\n",
      "timestep:388, pyg_AUC: 0.4859\n",
      "timestep:389, pyg_AUC: 0.4901\n",
      "timestep:390, pyg_AUC: 0.4887\n",
      "timestep:391, pyg_AUC: 0.4873\n",
      "timestep:392, pyg_AUC: 0.4901\n",
      "timestep:393, pyg_AUC: 0.4901\n",
      "timestep:394, pyg_AUC: 0.4901\n",
      "timestep:395, pyg_AUC: 0.4915\n",
      "timestep:396, pyg_AUC: 0.4915\n",
      "timestep:397, pyg_AUC: 0.4887\n",
      "timestep:398, pyg_AUC: 0.4887\n",
      "timestep:399, pyg_AUC: 0.4944\n",
      "timestep:400, pyg_AUC: 0.4915\n",
      "timestep:401, pyg_AUC: 0.4915\n",
      "timestep:402, pyg_AUC: 0.4887\n",
      "timestep:403, pyg_AUC: 0.4901\n",
      "timestep:404, pyg_AUC: 0.4887\n",
      "timestep:405, pyg_AUC: 0.4901\n",
      "timestep:406, pyg_AUC: 0.4887\n",
      "timestep:407, pyg_AUC: 0.4873\n",
      "timestep:408, pyg_AUC: 0.4873\n",
      "timestep:409, pyg_AUC: 0.4901\n",
      "timestep:410, pyg_AUC: 0.4901\n",
      "timestep:411, pyg_AUC: 0.4901\n",
      "timestep:412, pyg_AUC: 0.4901\n",
      "timestep:413, pyg_AUC: 0.4887\n",
      "timestep:414, pyg_AUC: 0.4901\n",
      "timestep:415, pyg_AUC: 0.4887\n",
      "timestep:416, pyg_AUC: 0.4887\n",
      "timestep:417, pyg_AUC: 0.4901\n",
      "timestep:418, pyg_AUC: 0.4901\n",
      "timestep:419, pyg_AUC: 0.4915\n",
      "timestep:420, pyg_AUC: 0.4887\n",
      "timestep:421, pyg_AUC: 0.4887\n",
      "timestep:422, pyg_AUC: 0.4873\n",
      "timestep:423, pyg_AUC: 0.4901\n",
      "timestep:424, pyg_AUC: 0.4901\n",
      "timestep:425, pyg_AUC: 0.4901\n",
      "timestep:426, pyg_AUC: 0.4915\n",
      "timestep:427, pyg_AUC: 0.4887\n",
      "timestep:428, pyg_AUC: 0.4901\n",
      "timestep:429, pyg_AUC: 0.4915\n",
      "timestep:430, pyg_AUC: 0.4873\n",
      "timestep:431, pyg_AUC: 0.4901\n",
      "timestep:432, pyg_AUC: 0.4901\n",
      "timestep:433, pyg_AUC: 0.4915\n",
      "timestep:434, pyg_AUC: 0.4887\n",
      "timestep:435, pyg_AUC: 0.4901\n",
      "timestep:436, pyg_AUC: 0.4873\n",
      "timestep:437, pyg_AUC: 0.4887\n",
      "timestep:438, pyg_AUC: 0.4901\n",
      "timestep:439, pyg_AUC: 0.4915\n",
      "timestep:440, pyg_AUC: 0.4901\n",
      "timestep:441, pyg_AUC: 0.4915\n",
      "timestep:442, pyg_AUC: 0.4859\n",
      "timestep:443, pyg_AUC: 0.4901\n",
      "timestep:444, pyg_AUC: 0.4887\n",
      "timestep:445, pyg_AUC: 0.4929\n",
      "timestep:446, pyg_AUC: 0.4901\n",
      "timestep:447, pyg_AUC: 0.4901\n",
      "timestep:448, pyg_AUC: 0.4915\n",
      "timestep:449, pyg_AUC: 0.4915\n",
      "timestep:450, pyg_AUC: 0.4929\n",
      "timestep:451, pyg_AUC: 0.4915\n",
      "timestep:452, pyg_AUC: 0.4901\n",
      "timestep:453, pyg_AUC: 0.4901\n",
      "timestep:454, pyg_AUC: 0.4901\n",
      "timestep:455, pyg_AUC: 0.4901\n",
      "timestep:456, pyg_AUC: 0.4901\n",
      "timestep:457, pyg_AUC: 0.4915\n",
      "timestep:458, pyg_AUC: 0.4887\n",
      "timestep:459, pyg_AUC: 0.4901\n",
      "timestep:460, pyg_AUC: 0.4901\n",
      "timestep:461, pyg_AUC: 0.4887\n",
      "timestep:462, pyg_AUC: 0.4859\n",
      "timestep:463, pyg_AUC: 0.4901\n",
      "timestep:464, pyg_AUC: 0.4915\n",
      "timestep:465, pyg_AUC: 0.4901\n",
      "timestep:466, pyg_AUC: 0.4901\n",
      "timestep:467, pyg_AUC: 0.4887\n",
      "timestep:468, pyg_AUC: 0.4901\n",
      "timestep:469, pyg_AUC: 0.4901\n",
      "timestep:470, pyg_AUC: 0.4901\n",
      "timestep:471, pyg_AUC: 0.4901\n",
      "timestep:472, pyg_AUC: 0.4901\n",
      "timestep:473, pyg_AUC: 0.4915\n",
      "timestep:474, pyg_AUC: 0.4901\n",
      "timestep:475, pyg_AUC: 0.4901\n",
      "timestep:476, pyg_AUC: 0.4873\n",
      "timestep:477, pyg_AUC: 0.4915\n",
      "timestep:478, pyg_AUC: 0.4901\n",
      "timestep:479, pyg_AUC: 0.4887\n",
      "timestep:480, pyg_AUC: 0.4901\n",
      "timestep:481, pyg_AUC: 0.4915\n",
      "timestep:482, pyg_AUC: 0.4901\n",
      "timestep:483, pyg_AUC: 0.4929\n",
      "timestep:484, pyg_AUC: 0.4915\n",
      "timestep:485, pyg_AUC: 0.4915\n",
      "timestep:486, pyg_AUC: 0.4901\n",
      "timestep:487, pyg_AUC: 0.4915\n",
      "timestep:488, pyg_AUC: 0.4901\n",
      "timestep:489, pyg_AUC: 0.4915\n",
      "timestep:490, pyg_AUC: 0.4901\n",
      "timestep:491, pyg_AUC: 0.4887\n",
      "timestep:492, pyg_AUC: 0.4915\n",
      "timestep:493, pyg_AUC: 0.4901\n",
      "timestep:494, pyg_AUC: 0.4901\n",
      "timestep:495, pyg_AUC: 0.4887\n",
      "timestep:496, pyg_AUC: 0.4915\n",
      "timestep:497, pyg_AUC: 0.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [30:29<03:23, 101.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:498, pyg_AUC: 0.4915\n",
      "timestep:499, pyg_AUC: 0.4887\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 43.06204\n",
      "Epoch: 0010 loss= 27.59716\n",
      "Epoch: 0020 loss= 21.59381\n",
      "Epoch: 0030 loss= 18.75920\n",
      "Epoch: 0040 loss= 10.48564\n",
      "Epoch: 0050 loss= 6.08166\n",
      "Epoch: 0060 loss= 1.56193\n",
      "Epoch: 0070 loss= 0.83657\n",
      "Epoch: 0080 loss= 0.63943\n",
      "Epoch: 0090 loss= 0.56184\n",
      "Epoch: 0100 loss= 0.54279\n",
      "Epoch: 0110 loss= 0.58262\n",
      "Epoch: 0120 loss= 0.56408\n",
      "Epoch: 0130 loss= 0.61578\n",
      "Epoch: 0140 loss= 0.56002\n",
      "Epoch: 0150 loss= 0.57404\n",
      "Epoch: 0160 loss= 0.62803\n",
      "Epoch: 0170 loss= 0.60751\n",
      "Epoch: 0180 loss= 0.56756\n",
      "Epoch: 0190 loss= 0.57722\n",
      "Epoch: 0200 loss= 0.66611\n",
      "Epoch: 0210 loss= 0.55400\n",
      "Epoch: 0220 loss= 0.59936\n",
      "Epoch: 0230 loss= 0.48546\n",
      "Epoch: 0240 loss= 0.61613\n",
      "Epoch: 0250 loss= 0.58629\n",
      "Epoch: 0260 loss= 0.56399\n",
      "Epoch: 0270 loss= 0.57619\n",
      "Epoch: 0280 loss= 0.57528\n",
      "Epoch: 0290 loss= 0.62420\n",
      "Epoch: 0300 loss= 0.53195\n",
      "Epoch: 0310 loss= 0.56362\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.8256,  4.7612, -5.3951, -4.6208, -4.4406,  5.6871,  5.5066, -5.2621]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 39.48774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss= 29.35122\n",
      "Epoch: 0020 loss= 17.37148\n",
      "Epoch: 0030 loss= 13.07904\n",
      "Epoch: 0040 loss= 8.53599\n",
      "Epoch: 0050 loss= 1.63882\n",
      "Epoch: 0060 loss= 0.75722\n",
      "Epoch: 0070 loss= 0.68202\n",
      "Epoch: 0080 loss= 0.68124\n",
      "Epoch: 0090 loss= 0.59762\n",
      "Epoch: 0100 loss= 0.57713\n",
      "Epoch: 0110 loss= 0.63347\n",
      "Epoch: 0120 loss= 0.60384\n",
      "Epoch: 0130 loss= 0.56794\n",
      "Epoch: 0140 loss= 0.57344\n",
      "Epoch: 0150 loss= 0.51956\n",
      "Epoch: 0160 loss= 0.60322\n",
      "Epoch: 0170 loss= 0.52328\n",
      "Epoch: 0180 loss= 0.55711\n",
      "Epoch: 0190 loss= 0.52471\n",
      "Epoch: 0200 loss= 0.53583\n",
      "Epoch: 0210 loss= 0.63664\n",
      "Epoch: 0220 loss= 0.60140\n",
      "Epoch: 0230 loss= 0.52650\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4859\n",
      "timestep:1, pyg_AUC: 0.4873\n",
      "timestep:2, pyg_AUC: 0.4915\n",
      "timestep:3, pyg_AUC: 0.4873\n",
      "timestep:4, pyg_AUC: 0.4873\n",
      "timestep:5, pyg_AUC: 0.4915\n",
      "timestep:6, pyg_AUC: 0.4915\n",
      "timestep:7, pyg_AUC: 0.4887\n",
      "timestep:8, pyg_AUC: 0.4901\n",
      "timestep:9, pyg_AUC: 0.4845\n",
      "timestep:10, pyg_AUC: 0.4873\n",
      "timestep:11, pyg_AUC: 0.4887\n",
      "timestep:12, pyg_AUC: 0.4901\n",
      "timestep:13, pyg_AUC: 0.4873\n",
      "timestep:14, pyg_AUC: 0.4816\n",
      "timestep:15, pyg_AUC: 0.4845\n",
      "timestep:16, pyg_AUC: 0.4887\n",
      "timestep:17, pyg_AUC: 0.4887\n",
      "timestep:18, pyg_AUC: 0.4873\n",
      "timestep:19, pyg_AUC: 0.4873\n",
      "timestep:20, pyg_AUC: 0.4901\n",
      "timestep:21, pyg_AUC: 0.4873\n",
      "timestep:22, pyg_AUC: 0.4901\n",
      "timestep:23, pyg_AUC: 0.4887\n",
      "timestep:24, pyg_AUC: 0.4873\n",
      "timestep:25, pyg_AUC: 0.4873\n",
      "timestep:26, pyg_AUC: 0.4929\n",
      "timestep:27, pyg_AUC: 0.4845\n",
      "timestep:28, pyg_AUC: 0.4915\n",
      "timestep:29, pyg_AUC: 0.4915\n",
      "timestep:30, pyg_AUC: 0.4915\n",
      "timestep:31, pyg_AUC: 0.4873\n",
      "timestep:32, pyg_AUC: 0.4901\n",
      "timestep:33, pyg_AUC: 0.4873\n",
      "timestep:34, pyg_AUC: 0.4901\n",
      "timestep:35, pyg_AUC: 0.4873\n",
      "timestep:36, pyg_AUC: 0.4887\n",
      "timestep:37, pyg_AUC: 0.4901\n",
      "timestep:38, pyg_AUC: 0.4887\n",
      "timestep:39, pyg_AUC: 0.4944\n",
      "timestep:40, pyg_AUC: 0.4887\n",
      "timestep:41, pyg_AUC: 0.4873\n",
      "timestep:42, pyg_AUC: 0.4915\n",
      "timestep:43, pyg_AUC: 0.4859\n",
      "timestep:44, pyg_AUC: 0.4859\n",
      "timestep:45, pyg_AUC: 0.4873\n",
      "timestep:46, pyg_AUC: 0.4887\n",
      "timestep:47, pyg_AUC: 0.4887\n",
      "timestep:48, pyg_AUC: 0.4887\n",
      "timestep:49, pyg_AUC: 0.4873\n",
      "timestep:50, pyg_AUC: 0.4859\n",
      "timestep:51, pyg_AUC: 0.4901\n",
      "timestep:52, pyg_AUC: 0.4944\n",
      "timestep:53, pyg_AUC: 0.4915\n",
      "timestep:54, pyg_AUC: 0.4845\n",
      "timestep:55, pyg_AUC: 0.4873\n",
      "timestep:56, pyg_AUC: 0.4859\n",
      "timestep:57, pyg_AUC: 0.4873\n",
      "timestep:58, pyg_AUC: 0.4873\n",
      "timestep:59, pyg_AUC: 0.4859\n",
      "timestep:60, pyg_AUC: 0.4859\n",
      "timestep:61, pyg_AUC: 0.4887\n",
      "timestep:62, pyg_AUC: 0.4901\n",
      "timestep:63, pyg_AUC: 0.4915\n",
      "timestep:64, pyg_AUC: 0.4944\n",
      "timestep:65, pyg_AUC: 0.4915\n",
      "timestep:66, pyg_AUC: 0.4859\n",
      "timestep:67, pyg_AUC: 0.4859\n",
      "timestep:68, pyg_AUC: 0.4873\n",
      "timestep:69, pyg_AUC: 0.4901\n",
      "timestep:70, pyg_AUC: 0.4873\n",
      "timestep:71, pyg_AUC: 0.4915\n",
      "timestep:72, pyg_AUC: 0.4887\n",
      "timestep:73, pyg_AUC: 0.4859\n",
      "timestep:74, pyg_AUC: 0.4887\n",
      "timestep:75, pyg_AUC: 0.4887\n",
      "timestep:76, pyg_AUC: 0.4873\n",
      "timestep:77, pyg_AUC: 0.4859\n",
      "timestep:78, pyg_AUC: 0.4887\n",
      "timestep:79, pyg_AUC: 0.4859\n",
      "timestep:80, pyg_AUC: 0.4887\n",
      "timestep:81, pyg_AUC: 0.4929\n",
      "timestep:82, pyg_AUC: 0.4901\n",
      "timestep:83, pyg_AUC: 0.4873\n",
      "timestep:84, pyg_AUC: 0.4859\n",
      "timestep:85, pyg_AUC: 0.4901\n",
      "timestep:86, pyg_AUC: 0.4901\n",
      "timestep:87, pyg_AUC: 0.4901\n",
      "timestep:88, pyg_AUC: 0.4887\n",
      "timestep:89, pyg_AUC: 0.4915\n",
      "timestep:90, pyg_AUC: 0.4958\n",
      "timestep:91, pyg_AUC: 0.4944\n",
      "timestep:92, pyg_AUC: 0.4915\n",
      "timestep:93, pyg_AUC: 0.4887\n",
      "timestep:94, pyg_AUC: 0.4859\n",
      "timestep:95, pyg_AUC: 0.4901\n",
      "timestep:96, pyg_AUC: 0.4887\n",
      "timestep:97, pyg_AUC: 0.4887\n",
      "timestep:98, pyg_AUC: 0.4873\n",
      "timestep:99, pyg_AUC: 0.4887\n",
      "timestep:100, pyg_AUC: 0.4915\n",
      "timestep:101, pyg_AUC: 0.4887\n",
      "timestep:102, pyg_AUC: 0.4873\n",
      "timestep:103, pyg_AUC: 0.4887\n",
      "timestep:104, pyg_AUC: 0.4915\n",
      "timestep:105, pyg_AUC: 0.4915\n",
      "timestep:106, pyg_AUC: 0.4915\n",
      "timestep:107, pyg_AUC: 0.4915\n",
      "timestep:108, pyg_AUC: 0.4887\n",
      "timestep:109, pyg_AUC: 0.4929\n",
      "timestep:110, pyg_AUC: 0.4901\n",
      "timestep:111, pyg_AUC: 0.4887\n",
      "timestep:112, pyg_AUC: 0.4915\n",
      "timestep:113, pyg_AUC: 0.4915\n",
      "timestep:114, pyg_AUC: 0.4887\n",
      "timestep:115, pyg_AUC: 0.4958\n",
      "timestep:116, pyg_AUC: 0.4901\n",
      "timestep:117, pyg_AUC: 0.4901\n",
      "timestep:118, pyg_AUC: 0.4901\n",
      "timestep:119, pyg_AUC: 0.4929\n",
      "timestep:120, pyg_AUC: 0.4901\n",
      "timestep:121, pyg_AUC: 0.4929\n",
      "timestep:122, pyg_AUC: 0.4859\n",
      "timestep:123, pyg_AUC: 0.4887\n",
      "timestep:124, pyg_AUC: 0.4901\n",
      "timestep:125, pyg_AUC: 0.4901\n",
      "timestep:126, pyg_AUC: 0.4915\n",
      "timestep:127, pyg_AUC: 0.4944\n",
      "timestep:128, pyg_AUC: 0.4901\n",
      "timestep:129, pyg_AUC: 0.4901\n",
      "timestep:130, pyg_AUC: 0.4901\n",
      "timestep:131, pyg_AUC: 0.4901\n",
      "timestep:132, pyg_AUC: 0.4929\n",
      "timestep:133, pyg_AUC: 0.4915\n",
      "timestep:134, pyg_AUC: 0.4887\n",
      "timestep:135, pyg_AUC: 0.4845\n",
      "timestep:136, pyg_AUC: 0.4915\n",
      "timestep:137, pyg_AUC: 0.4901\n",
      "timestep:138, pyg_AUC: 0.4915\n",
      "timestep:139, pyg_AUC: 0.4901\n",
      "timestep:140, pyg_AUC: 0.4915\n",
      "timestep:141, pyg_AUC: 0.4901\n",
      "timestep:142, pyg_AUC: 0.4901\n",
      "timestep:143, pyg_AUC: 0.4901\n",
      "timestep:144, pyg_AUC: 0.4915\n",
      "timestep:145, pyg_AUC: 0.4887\n",
      "timestep:146, pyg_AUC: 0.4901\n",
      "timestep:147, pyg_AUC: 0.4887\n",
      "timestep:148, pyg_AUC: 0.4901\n",
      "timestep:149, pyg_AUC: 0.4887\n",
      "timestep:150, pyg_AUC: 0.4859\n",
      "timestep:151, pyg_AUC: 0.4901\n",
      "timestep:152, pyg_AUC: 0.4915\n",
      "timestep:153, pyg_AUC: 0.4887\n",
      "timestep:154, pyg_AUC: 0.4887\n",
      "timestep:155, pyg_AUC: 0.4901\n",
      "timestep:156, pyg_AUC: 0.4915\n",
      "timestep:157, pyg_AUC: 0.4915\n",
      "timestep:158, pyg_AUC: 0.4859\n",
      "timestep:159, pyg_AUC: 0.4859\n",
      "timestep:160, pyg_AUC: 0.4915\n",
      "timestep:161, pyg_AUC: 0.4859\n",
      "timestep:162, pyg_AUC: 0.4887\n",
      "timestep:163, pyg_AUC: 0.4915\n",
      "timestep:164, pyg_AUC: 0.4887\n",
      "timestep:165, pyg_AUC: 0.4901\n",
      "timestep:166, pyg_AUC: 0.4901\n",
      "timestep:167, pyg_AUC: 0.4873\n",
      "timestep:168, pyg_AUC: 0.4873\n",
      "timestep:169, pyg_AUC: 0.4901\n",
      "timestep:170, pyg_AUC: 0.4887\n",
      "timestep:171, pyg_AUC: 0.4873\n",
      "timestep:172, pyg_AUC: 0.4887\n",
      "timestep:173, pyg_AUC: 0.4915\n",
      "timestep:174, pyg_AUC: 0.4887\n",
      "timestep:175, pyg_AUC: 0.4887\n",
      "timestep:176, pyg_AUC: 0.4887\n",
      "timestep:177, pyg_AUC: 0.4887\n",
      "timestep:178, pyg_AUC: 0.4873\n",
      "timestep:179, pyg_AUC: 0.4901\n",
      "timestep:180, pyg_AUC: 0.4887\n",
      "timestep:181, pyg_AUC: 0.4887\n",
      "timestep:182, pyg_AUC: 0.4873\n",
      "timestep:183, pyg_AUC: 0.4887\n",
      "timestep:184, pyg_AUC: 0.4887\n",
      "timestep:185, pyg_AUC: 0.4859\n",
      "timestep:186, pyg_AUC: 0.4915\n",
      "timestep:187, pyg_AUC: 0.4887\n",
      "timestep:188, pyg_AUC: 0.4859\n",
      "timestep:189, pyg_AUC: 0.4873\n",
      "timestep:190, pyg_AUC: 0.4873\n",
      "timestep:191, pyg_AUC: 0.4887\n",
      "timestep:192, pyg_AUC: 0.4901\n",
      "timestep:193, pyg_AUC: 0.4887\n",
      "timestep:194, pyg_AUC: 0.4901\n",
      "timestep:195, pyg_AUC: 0.4901\n",
      "timestep:196, pyg_AUC: 0.4887\n",
      "timestep:197, pyg_AUC: 0.4887\n",
      "timestep:198, pyg_AUC: 0.4915\n",
      "timestep:199, pyg_AUC: 0.4887\n",
      "timestep:200, pyg_AUC: 0.4887\n",
      "timestep:201, pyg_AUC: 0.4901\n",
      "timestep:202, pyg_AUC: 0.4873\n",
      "timestep:203, pyg_AUC: 0.4887\n",
      "timestep:204, pyg_AUC: 0.4901\n",
      "timestep:205, pyg_AUC: 0.4873\n",
      "timestep:206, pyg_AUC: 0.4901\n",
      "timestep:207, pyg_AUC: 0.4901\n",
      "timestep:208, pyg_AUC: 0.4887\n",
      "timestep:209, pyg_AUC: 0.4873\n",
      "timestep:210, pyg_AUC: 0.4887\n",
      "timestep:211, pyg_AUC: 0.4887\n",
      "timestep:212, pyg_AUC: 0.4901\n",
      "timestep:213, pyg_AUC: 0.4873\n",
      "timestep:214, pyg_AUC: 0.4887\n",
      "timestep:215, pyg_AUC: 0.4887\n",
      "timestep:216, pyg_AUC: 0.4873\n",
      "timestep:217, pyg_AUC: 0.4901\n",
      "timestep:218, pyg_AUC: 0.4873\n",
      "timestep:219, pyg_AUC: 0.4873\n",
      "timestep:220, pyg_AUC: 0.4873\n",
      "timestep:221, pyg_AUC: 0.4887\n",
      "timestep:222, pyg_AUC: 0.4887\n",
      "timestep:223, pyg_AUC: 0.4901\n",
      "timestep:224, pyg_AUC: 0.4873\n",
      "timestep:225, pyg_AUC: 0.4859\n",
      "timestep:226, pyg_AUC: 0.4887\n",
      "timestep:227, pyg_AUC: 0.4887\n",
      "timestep:228, pyg_AUC: 0.4873\n",
      "timestep:229, pyg_AUC: 0.4887\n",
      "timestep:230, pyg_AUC: 0.4859\n",
      "timestep:231, pyg_AUC: 0.4915\n",
      "timestep:232, pyg_AUC: 0.4901\n",
      "timestep:233, pyg_AUC: 0.4845\n",
      "timestep:234, pyg_AUC: 0.4901\n",
      "timestep:235, pyg_AUC: 0.4831\n",
      "timestep:236, pyg_AUC: 0.4873\n",
      "timestep:237, pyg_AUC: 0.4873\n",
      "timestep:238, pyg_AUC: 0.4887\n",
      "timestep:239, pyg_AUC: 0.4859\n",
      "timestep:240, pyg_AUC: 0.4887\n",
      "timestep:241, pyg_AUC: 0.4887\n",
      "timestep:242, pyg_AUC: 0.4901\n",
      "timestep:243, pyg_AUC: 0.4859\n",
      "timestep:244, pyg_AUC: 0.4887\n",
      "timestep:245, pyg_AUC: 0.4859\n",
      "timestep:246, pyg_AUC: 0.4859\n",
      "timestep:247, pyg_AUC: 0.4845\n",
      "timestep:248, pyg_AUC: 0.4887\n",
      "timestep:249, pyg_AUC: 0.4873\n",
      "timestep:250, pyg_AUC: 0.4887\n",
      "timestep:251, pyg_AUC: 0.4873\n",
      "timestep:252, pyg_AUC: 0.4873\n",
      "timestep:253, pyg_AUC: 0.4873\n",
      "timestep:254, pyg_AUC: 0.4887\n",
      "timestep:255, pyg_AUC: 0.4887\n",
      "timestep:256, pyg_AUC: 0.4887\n",
      "timestep:257, pyg_AUC: 0.4873\n",
      "timestep:258, pyg_AUC: 0.4831\n",
      "timestep:259, pyg_AUC: 0.4901\n",
      "timestep:260, pyg_AUC: 0.4859\n",
      "timestep:261, pyg_AUC: 0.4845\n",
      "timestep:262, pyg_AUC: 0.4859\n",
      "timestep:263, pyg_AUC: 0.4887\n",
      "timestep:264, pyg_AUC: 0.4887\n",
      "timestep:265, pyg_AUC: 0.4887\n",
      "timestep:266, pyg_AUC: 0.4845\n",
      "timestep:267, pyg_AUC: 0.4859\n",
      "timestep:268, pyg_AUC: 0.4901\n",
      "timestep:269, pyg_AUC: 0.4873\n",
      "timestep:270, pyg_AUC: 0.4845\n",
      "timestep:271, pyg_AUC: 0.4859\n",
      "timestep:272, pyg_AUC: 0.4873\n",
      "timestep:273, pyg_AUC: 0.4859\n",
      "timestep:274, pyg_AUC: 0.4859\n",
      "timestep:275, pyg_AUC: 0.4859\n",
      "timestep:276, pyg_AUC: 0.4859\n",
      "timestep:277, pyg_AUC: 0.4873\n",
      "timestep:278, pyg_AUC: 0.4873\n",
      "timestep:279, pyg_AUC: 0.4873\n",
      "timestep:280, pyg_AUC: 0.4859\n",
      "timestep:281, pyg_AUC: 0.4859\n",
      "timestep:282, pyg_AUC: 0.4887\n",
      "timestep:283, pyg_AUC: 0.4873\n",
      "timestep:284, pyg_AUC: 0.4831\n",
      "timestep:285, pyg_AUC: 0.4859\n",
      "timestep:286, pyg_AUC: 0.4816\n",
      "timestep:287, pyg_AUC: 0.4873\n",
      "timestep:288, pyg_AUC: 0.4859\n",
      "timestep:289, pyg_AUC: 0.4859\n",
      "timestep:290, pyg_AUC: 0.4873\n",
      "timestep:291, pyg_AUC: 0.4887\n",
      "timestep:292, pyg_AUC: 0.4859\n",
      "timestep:293, pyg_AUC: 0.4873\n",
      "timestep:294, pyg_AUC: 0.4859\n",
      "timestep:295, pyg_AUC: 0.4873\n",
      "timestep:296, pyg_AUC: 0.4859\n",
      "timestep:297, pyg_AUC: 0.4859\n",
      "timestep:298, pyg_AUC: 0.4873\n",
      "timestep:299, pyg_AUC: 0.4873\n",
      "timestep:300, pyg_AUC: 0.4859\n",
      "timestep:301, pyg_AUC: 0.4887\n",
      "timestep:302, pyg_AUC: 0.4901\n",
      "timestep:303, pyg_AUC: 0.4859\n",
      "timestep:304, pyg_AUC: 0.4845\n",
      "timestep:305, pyg_AUC: 0.4873\n",
      "timestep:306, pyg_AUC: 0.4873\n",
      "timestep:307, pyg_AUC: 0.4859\n",
      "timestep:308, pyg_AUC: 0.4859\n",
      "timestep:309, pyg_AUC: 0.4845\n",
      "timestep:310, pyg_AUC: 0.4859\n",
      "timestep:311, pyg_AUC: 0.4901\n",
      "timestep:312, pyg_AUC: 0.4901\n",
      "timestep:313, pyg_AUC: 0.4873\n",
      "timestep:314, pyg_AUC: 0.4887\n",
      "timestep:315, pyg_AUC: 0.4873\n",
      "timestep:316, pyg_AUC: 0.4873\n",
      "timestep:317, pyg_AUC: 0.4901\n",
      "timestep:318, pyg_AUC: 0.4873\n",
      "timestep:319, pyg_AUC: 0.4887\n",
      "timestep:320, pyg_AUC: 0.4859\n",
      "timestep:321, pyg_AUC: 0.4887\n",
      "timestep:322, pyg_AUC: 0.4873\n",
      "timestep:323, pyg_AUC: 0.4901\n",
      "timestep:324, pyg_AUC: 0.4859\n",
      "timestep:325, pyg_AUC: 0.4859\n",
      "timestep:326, pyg_AUC: 0.4873\n",
      "timestep:327, pyg_AUC: 0.4859\n",
      "timestep:328, pyg_AUC: 0.4873\n",
      "timestep:329, pyg_AUC: 0.4873\n",
      "timestep:330, pyg_AUC: 0.4873\n",
      "timestep:331, pyg_AUC: 0.4873\n",
      "timestep:332, pyg_AUC: 0.4901\n",
      "timestep:333, pyg_AUC: 0.4873\n",
      "timestep:334, pyg_AUC: 0.4873\n",
      "timestep:335, pyg_AUC: 0.4873\n",
      "timestep:336, pyg_AUC: 0.4873\n",
      "timestep:337, pyg_AUC: 0.4873\n",
      "timestep:338, pyg_AUC: 0.4859\n",
      "timestep:339, pyg_AUC: 0.4873\n",
      "timestep:340, pyg_AUC: 0.4859\n",
      "timestep:341, pyg_AUC: 0.4859\n",
      "timestep:342, pyg_AUC: 0.4873\n",
      "timestep:343, pyg_AUC: 0.4873\n",
      "timestep:344, pyg_AUC: 0.4873\n",
      "timestep:345, pyg_AUC: 0.4873\n",
      "timestep:346, pyg_AUC: 0.4873\n",
      "timestep:347, pyg_AUC: 0.4831\n",
      "timestep:348, pyg_AUC: 0.4859\n",
      "timestep:349, pyg_AUC: 0.4915\n",
      "timestep:350, pyg_AUC: 0.4859\n",
      "timestep:351, pyg_AUC: 0.4887\n",
      "timestep:352, pyg_AUC: 0.4887\n",
      "timestep:353, pyg_AUC: 0.4887\n",
      "timestep:354, pyg_AUC: 0.4887\n",
      "timestep:355, pyg_AUC: 0.4887\n",
      "timestep:356, pyg_AUC: 0.4901\n",
      "timestep:357, pyg_AUC: 0.4915\n",
      "timestep:358, pyg_AUC: 0.4859\n",
      "timestep:359, pyg_AUC: 0.4915\n",
      "timestep:360, pyg_AUC: 0.4901\n",
      "timestep:361, pyg_AUC: 0.4887\n",
      "timestep:362, pyg_AUC: 0.4873\n",
      "timestep:363, pyg_AUC: 0.4859\n",
      "timestep:364, pyg_AUC: 0.4873\n",
      "timestep:365, pyg_AUC: 0.4873\n",
      "timestep:366, pyg_AUC: 0.4873\n",
      "timestep:367, pyg_AUC: 0.4901\n",
      "timestep:368, pyg_AUC: 0.4873\n",
      "timestep:369, pyg_AUC: 0.4915\n",
      "timestep:370, pyg_AUC: 0.4887\n",
      "timestep:371, pyg_AUC: 0.4887\n",
      "timestep:372, pyg_AUC: 0.4845\n",
      "timestep:373, pyg_AUC: 0.4873\n",
      "timestep:374, pyg_AUC: 0.4873\n",
      "timestep:375, pyg_AUC: 0.4873\n",
      "timestep:376, pyg_AUC: 0.4915\n",
      "timestep:377, pyg_AUC: 0.4915\n",
      "timestep:378, pyg_AUC: 0.4887\n",
      "timestep:379, pyg_AUC: 0.4901\n",
      "timestep:380, pyg_AUC: 0.4915\n",
      "timestep:381, pyg_AUC: 0.4873\n",
      "timestep:382, pyg_AUC: 0.4887\n",
      "timestep:383, pyg_AUC: 0.4873\n",
      "timestep:384, pyg_AUC: 0.4901\n",
      "timestep:385, pyg_AUC: 0.4901\n",
      "timestep:386, pyg_AUC: 0.4887\n",
      "timestep:387, pyg_AUC: 0.4873\n",
      "timestep:388, pyg_AUC: 0.4901\n",
      "timestep:389, pyg_AUC: 0.4887\n",
      "timestep:390, pyg_AUC: 0.4901\n",
      "timestep:391, pyg_AUC: 0.4873\n",
      "timestep:392, pyg_AUC: 0.4887\n",
      "timestep:393, pyg_AUC: 0.4887\n",
      "timestep:394, pyg_AUC: 0.4901\n",
      "timestep:395, pyg_AUC: 0.4873\n",
      "timestep:396, pyg_AUC: 0.4887\n",
      "timestep:397, pyg_AUC: 0.4901\n",
      "timestep:398, pyg_AUC: 0.4901\n",
      "timestep:399, pyg_AUC: 0.4887\n",
      "timestep:400, pyg_AUC: 0.4901\n",
      "timestep:401, pyg_AUC: 0.4901\n",
      "timestep:402, pyg_AUC: 0.4845\n",
      "timestep:403, pyg_AUC: 0.4873\n",
      "timestep:404, pyg_AUC: 0.4887\n",
      "timestep:405, pyg_AUC: 0.4929\n",
      "timestep:406, pyg_AUC: 0.4873\n",
      "timestep:407, pyg_AUC: 0.4901\n",
      "timestep:408, pyg_AUC: 0.4887\n",
      "timestep:409, pyg_AUC: 0.4901\n",
      "timestep:410, pyg_AUC: 0.4887\n",
      "timestep:411, pyg_AUC: 0.4873\n",
      "timestep:412, pyg_AUC: 0.4915\n",
      "timestep:413, pyg_AUC: 0.4901\n",
      "timestep:414, pyg_AUC: 0.4873\n",
      "timestep:415, pyg_AUC: 0.4887\n",
      "timestep:416, pyg_AUC: 0.4859\n",
      "timestep:417, pyg_AUC: 0.4901\n",
      "timestep:418, pyg_AUC: 0.4887\n",
      "timestep:419, pyg_AUC: 0.4859\n",
      "timestep:420, pyg_AUC: 0.4901\n",
      "timestep:421, pyg_AUC: 0.4873\n",
      "timestep:422, pyg_AUC: 0.4887\n",
      "timestep:423, pyg_AUC: 0.4873\n",
      "timestep:424, pyg_AUC: 0.4887\n",
      "timestep:425, pyg_AUC: 0.4887\n",
      "timestep:426, pyg_AUC: 0.4873\n",
      "timestep:427, pyg_AUC: 0.4873\n",
      "timestep:428, pyg_AUC: 0.4873\n",
      "timestep:429, pyg_AUC: 0.4901\n",
      "timestep:430, pyg_AUC: 0.4873\n",
      "timestep:431, pyg_AUC: 0.4887\n",
      "timestep:432, pyg_AUC: 0.4887\n",
      "timestep:433, pyg_AUC: 0.4901\n",
      "timestep:434, pyg_AUC: 0.4887\n",
      "timestep:435, pyg_AUC: 0.4887\n",
      "timestep:436, pyg_AUC: 0.4859\n",
      "timestep:437, pyg_AUC: 0.4887\n",
      "timestep:438, pyg_AUC: 0.4901\n",
      "timestep:439, pyg_AUC: 0.4887\n",
      "timestep:440, pyg_AUC: 0.4901\n",
      "timestep:441, pyg_AUC: 0.4873\n",
      "timestep:442, pyg_AUC: 0.4901\n",
      "timestep:443, pyg_AUC: 0.4859\n",
      "timestep:444, pyg_AUC: 0.4887\n",
      "timestep:445, pyg_AUC: 0.4873\n",
      "timestep:446, pyg_AUC: 0.4859\n",
      "timestep:447, pyg_AUC: 0.4873\n",
      "timestep:448, pyg_AUC: 0.4887\n",
      "timestep:449, pyg_AUC: 0.4873\n",
      "timestep:450, pyg_AUC: 0.4887\n",
      "timestep:451, pyg_AUC: 0.4887\n",
      "timestep:452, pyg_AUC: 0.4915\n",
      "timestep:453, pyg_AUC: 0.4901\n",
      "timestep:454, pyg_AUC: 0.4929\n",
      "timestep:455, pyg_AUC: 0.4887\n",
      "timestep:456, pyg_AUC: 0.4859\n",
      "timestep:457, pyg_AUC: 0.4887\n",
      "timestep:458, pyg_AUC: 0.4887\n",
      "timestep:459, pyg_AUC: 0.4915\n",
      "timestep:460, pyg_AUC: 0.4873\n",
      "timestep:461, pyg_AUC: 0.4873\n",
      "timestep:462, pyg_AUC: 0.4887\n",
      "timestep:463, pyg_AUC: 0.4887\n",
      "timestep:464, pyg_AUC: 0.4901\n",
      "timestep:465, pyg_AUC: 0.4915\n",
      "timestep:466, pyg_AUC: 0.4901\n",
      "timestep:467, pyg_AUC: 0.4901\n",
      "timestep:468, pyg_AUC: 0.4887\n",
      "timestep:469, pyg_AUC: 0.4873\n",
      "timestep:470, pyg_AUC: 0.4915\n",
      "timestep:471, pyg_AUC: 0.4887\n",
      "timestep:472, pyg_AUC: 0.4901\n",
      "timestep:473, pyg_AUC: 0.4873\n",
      "timestep:474, pyg_AUC: 0.4887\n",
      "timestep:475, pyg_AUC: 0.4887\n",
      "timestep:476, pyg_AUC: 0.4887\n",
      "timestep:477, pyg_AUC: 0.4873\n",
      "timestep:478, pyg_AUC: 0.4887\n",
      "timestep:479, pyg_AUC: 0.4901\n",
      "timestep:480, pyg_AUC: 0.4845\n",
      "timestep:481, pyg_AUC: 0.4873\n",
      "timestep:482, pyg_AUC: 0.4901\n",
      "timestep:483, pyg_AUC: 0.4873\n",
      "timestep:484, pyg_AUC: 0.4915\n",
      "timestep:485, pyg_AUC: 0.4901\n",
      "timestep:486, pyg_AUC: 0.4901\n",
      "timestep:487, pyg_AUC: 0.4859\n",
      "timestep:488, pyg_AUC: 0.4915\n",
      "timestep:489, pyg_AUC: 0.4887\n",
      "timestep:490, pyg_AUC: 0.4901\n",
      "timestep:491, pyg_AUC: 0.4915\n",
      "timestep:492, pyg_AUC: 0.4915\n",
      "timestep:493, pyg_AUC: 0.4873\n",
      "timestep:494, pyg_AUC: 0.4887\n",
      "timestep:495, pyg_AUC: 0.4859\n",
      "timestep:496, pyg_AUC: 0.4887\n",
      "timestep:497, pyg_AUC: 0.4929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [32:08<01:41, 101.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:498, pyg_AUC: 0.4901\n",
      "timestep:499, pyg_AUC: 0.4915\n",
      "Training diffusion model (unconditional) ...\n",
      "Epoch: 0000 loss= 34.92589\n",
      "Epoch: 0010 loss= 23.20242\n",
      "Epoch: 0020 loss= 21.63348\n",
      "Epoch: 0030 loss= 18.02320\n",
      "Epoch: 0040 loss= 17.32604\n",
      "Epoch: 0050 loss= 8.92889\n",
      "Epoch: 0060 loss= 2.53881\n",
      "Epoch: 0070 loss= 1.02574\n",
      "Epoch: 0080 loss= 0.97408\n",
      "Epoch: 0090 loss= 0.69554\n",
      "Epoch: 0100 loss= 0.62242\n",
      "Epoch: 0110 loss= 0.67575\n",
      "Epoch: 0120 loss= 0.65921\n",
      "Epoch: 0130 loss= 0.69209\n",
      "Epoch: 0140 loss= 0.62293\n",
      "Epoch: 0150 loss= 0.58934\n",
      "Epoch: 0160 loss= 0.56748\n",
      "Epoch: 0170 loss= 0.64608\n",
      "Epoch: 0180 loss= 0.64471\n",
      "Epoch: 0190 loss= 0.58903\n",
      "Epoch: 0200 loss= 0.54537\n",
      "Epoch: 0210 loss= 0.55741\n",
      "Epoch: 0220 loss= 0.58101\n",
      "Epoch: 0230 loss= 0.56812\n",
      "Epoch: 0240 loss= 0.63996\n",
      "Epoch: 0250 loss= 0.56430\n",
      "Epoch: 0260 loss= 0.59395\n",
      "Epoch: 0270 loss= 0.55968\n",
      "Epoch: 0280 loss= 0.63170\n",
      "Epoch: 0290 loss= 0.50627\n",
      "Epoch: 0300 loss= 0.63401\n",
      "Epoch: 0310 loss= 0.49713\n",
      "Epoch: 0320 loss= 0.53002\n",
      "Epoch: 0330 loss= 0.49404\n",
      "Epoch: 0340 loss= 0.53206\n",
      "Epoch: 0350 loss= 0.51175\n",
      "Early stopping\n",
      "Common feature: tensor([[-4.7951,  4.7515, -5.3709, -4.6632, -4.4222,  5.6774,  5.4586, -5.2359]],\n",
      "       device='cuda:0')\n",
      "Training diffusion model (conditional) ...\n",
      "Epoch: 0000 loss= 36.47488\n",
      "Epoch: 0010 loss= 26.95674\n",
      "Epoch: 0020 loss= 10.99074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0030 loss= 6.09505\n",
      "Epoch: 0040 loss= 2.86004\n",
      "Epoch: 0050 loss= 1.28045\n",
      "Epoch: 0060 loss= 0.86548\n",
      "Epoch: 0070 loss= 0.82537\n",
      "Epoch: 0080 loss= 0.69127\n",
      "Epoch: 0090 loss= 0.62643\n",
      "Epoch: 0100 loss= 0.62424\n",
      "Epoch: 0110 loss= 0.60933\n",
      "Epoch: 0120 loss= 0.56010\n",
      "Epoch: 0130 loss= 0.60436\n",
      "Epoch: 0140 loss= 0.62943\n",
      "Epoch: 0150 loss= 0.59407\n",
      "Epoch: 0160 loss= 0.60490\n",
      "Epoch: 0170 loss= 0.60129\n",
      "Epoch: 0180 loss= 0.56619\n",
      "Epoch: 0190 loss= 0.52025\n",
      "Epoch: 0200 loss= 0.61643\n",
      "Epoch: 0210 loss= 0.54675\n",
      "Epoch: 0220 loss= 0.60639\n",
      "Epoch: 0230 loss= 0.59419\n",
      "Epoch: 0240 loss= 0.57819\n",
      "Epoch: 0250 loss= 0.54647\n",
      "Epoch: 0260 loss= 0.54994\n",
      "Epoch: 0270 loss= 0.57956\n",
      "Epoch: 0280 loss= 0.58097\n",
      "Epoch: 0290 loss= 0.52719\n",
      "Epoch: 0300 loss= 0.59527\n",
      "Epoch: 0310 loss= 0.54600\n",
      "Epoch: 0320 loss= 0.58268\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-26d505bf6e9c>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:0, pyg_AUC: 0.4816\n",
      "timestep:1, pyg_AUC: 0.4887\n",
      "timestep:2, pyg_AUC: 0.4915\n",
      "timestep:3, pyg_AUC: 0.4915\n",
      "timestep:4, pyg_AUC: 0.4859\n",
      "timestep:5, pyg_AUC: 0.4901\n",
      "timestep:6, pyg_AUC: 0.4845\n",
      "timestep:7, pyg_AUC: 0.4859\n",
      "timestep:8, pyg_AUC: 0.4929\n",
      "timestep:9, pyg_AUC: 0.4901\n",
      "timestep:10, pyg_AUC: 0.4887\n",
      "timestep:11, pyg_AUC: 0.4944\n",
      "timestep:12, pyg_AUC: 0.4845\n",
      "timestep:13, pyg_AUC: 0.4915\n",
      "timestep:14, pyg_AUC: 0.4901\n",
      "timestep:15, pyg_AUC: 0.4873\n",
      "timestep:16, pyg_AUC: 0.4901\n",
      "timestep:17, pyg_AUC: 0.4887\n",
      "timestep:18, pyg_AUC: 0.4887\n",
      "timestep:19, pyg_AUC: 0.4859\n",
      "timestep:20, pyg_AUC: 0.4831\n",
      "timestep:21, pyg_AUC: 0.4929\n",
      "timestep:22, pyg_AUC: 0.4929\n",
      "timestep:23, pyg_AUC: 0.4859\n",
      "timestep:24, pyg_AUC: 0.4873\n",
      "timestep:25, pyg_AUC: 0.4859\n",
      "timestep:26, pyg_AUC: 0.4929\n",
      "timestep:27, pyg_AUC: 0.4915\n",
      "timestep:28, pyg_AUC: 0.4845\n",
      "timestep:29, pyg_AUC: 0.4887\n",
      "timestep:30, pyg_AUC: 0.4901\n",
      "timestep:31, pyg_AUC: 0.4901\n",
      "timestep:32, pyg_AUC: 0.4915\n",
      "timestep:33, pyg_AUC: 0.4901\n",
      "timestep:34, pyg_AUC: 0.4901\n",
      "timestep:35, pyg_AUC: 0.4901\n",
      "timestep:36, pyg_AUC: 0.4859\n",
      "timestep:37, pyg_AUC: 0.4887\n",
      "timestep:38, pyg_AUC: 0.4859\n",
      "timestep:39, pyg_AUC: 0.4915\n",
      "timestep:40, pyg_AUC: 0.4887\n",
      "timestep:41, pyg_AUC: 0.4944\n",
      "timestep:42, pyg_AUC: 0.4929\n",
      "timestep:43, pyg_AUC: 0.4901\n",
      "timestep:44, pyg_AUC: 0.4929\n",
      "timestep:45, pyg_AUC: 0.4915\n",
      "timestep:46, pyg_AUC: 0.4944\n",
      "timestep:47, pyg_AUC: 0.4873\n",
      "timestep:48, pyg_AUC: 0.4929\n",
      "timestep:49, pyg_AUC: 0.4873\n",
      "timestep:50, pyg_AUC: 0.4901\n",
      "timestep:51, pyg_AUC: 0.4915\n",
      "timestep:52, pyg_AUC: 0.4958\n",
      "timestep:53, pyg_AUC: 0.4929\n",
      "timestep:54, pyg_AUC: 0.4929\n",
      "timestep:55, pyg_AUC: 0.4958\n",
      "timestep:56, pyg_AUC: 0.4915\n",
      "timestep:57, pyg_AUC: 0.4915\n",
      "timestep:58, pyg_AUC: 0.4901\n",
      "timestep:59, pyg_AUC: 0.4859\n",
      "timestep:60, pyg_AUC: 0.4873\n",
      "timestep:61, pyg_AUC: 0.4915\n",
      "timestep:62, pyg_AUC: 0.4915\n",
      "timestep:63, pyg_AUC: 0.4887\n",
      "timestep:64, pyg_AUC: 0.4901\n",
      "timestep:65, pyg_AUC: 0.4915\n",
      "timestep:66, pyg_AUC: 0.4873\n",
      "timestep:67, pyg_AUC: 0.4915\n",
      "timestep:68, pyg_AUC: 0.4901\n",
      "timestep:69, pyg_AUC: 0.4929\n",
      "timestep:70, pyg_AUC: 0.4901\n",
      "timestep:71, pyg_AUC: 0.4901\n",
      "timestep:72, pyg_AUC: 0.4901\n",
      "timestep:73, pyg_AUC: 0.4887\n",
      "timestep:74, pyg_AUC: 0.4887\n",
      "timestep:75, pyg_AUC: 0.4901\n",
      "timestep:76, pyg_AUC: 0.4958\n",
      "timestep:77, pyg_AUC: 0.4901\n",
      "timestep:78, pyg_AUC: 0.4901\n",
      "timestep:79, pyg_AUC: 0.4901\n",
      "timestep:80, pyg_AUC: 0.4901\n",
      "timestep:81, pyg_AUC: 0.4944\n",
      "timestep:82, pyg_AUC: 0.4929\n",
      "timestep:83, pyg_AUC: 0.4915\n",
      "timestep:84, pyg_AUC: 0.4873\n",
      "timestep:85, pyg_AUC: 0.4873\n",
      "timestep:86, pyg_AUC: 0.4873\n",
      "timestep:87, pyg_AUC: 0.4887\n",
      "timestep:88, pyg_AUC: 0.4873\n",
      "timestep:89, pyg_AUC: 0.4873\n",
      "timestep:90, pyg_AUC: 0.4845\n",
      "timestep:91, pyg_AUC: 0.4901\n",
      "timestep:92, pyg_AUC: 0.4901\n",
      "timestep:93, pyg_AUC: 0.4887\n",
      "timestep:94, pyg_AUC: 0.4859\n",
      "timestep:95, pyg_AUC: 0.4915\n",
      "timestep:96, pyg_AUC: 0.4873\n",
      "timestep:97, pyg_AUC: 0.4859\n",
      "timestep:98, pyg_AUC: 0.4887\n",
      "timestep:99, pyg_AUC: 0.4901\n",
      "timestep:100, pyg_AUC: 0.4859\n",
      "timestep:101, pyg_AUC: 0.4901\n",
      "timestep:102, pyg_AUC: 0.4873\n",
      "timestep:103, pyg_AUC: 0.4915\n",
      "timestep:104, pyg_AUC: 0.4929\n",
      "timestep:105, pyg_AUC: 0.4929\n",
      "timestep:106, pyg_AUC: 0.4929\n",
      "timestep:107, pyg_AUC: 0.4915\n",
      "timestep:108, pyg_AUC: 0.4887\n",
      "timestep:109, pyg_AUC: 0.4873\n",
      "timestep:110, pyg_AUC: 0.4873\n",
      "timestep:111, pyg_AUC: 0.4915\n",
      "timestep:112, pyg_AUC: 0.4915\n",
      "timestep:113, pyg_AUC: 0.4859\n",
      "timestep:114, pyg_AUC: 0.4915\n",
      "timestep:115, pyg_AUC: 0.4887\n",
      "timestep:116, pyg_AUC: 0.4901\n",
      "timestep:117, pyg_AUC: 0.4887\n",
      "timestep:118, pyg_AUC: 0.4944\n",
      "timestep:119, pyg_AUC: 0.4887\n",
      "timestep:120, pyg_AUC: 0.4873\n",
      "timestep:121, pyg_AUC: 0.4901\n",
      "timestep:122, pyg_AUC: 0.4873\n",
      "timestep:123, pyg_AUC: 0.4873\n",
      "timestep:124, pyg_AUC: 0.4887\n",
      "timestep:125, pyg_AUC: 0.4944\n",
      "timestep:126, pyg_AUC: 0.4929\n",
      "timestep:127, pyg_AUC: 0.4901\n",
      "timestep:128, pyg_AUC: 0.4915\n",
      "timestep:129, pyg_AUC: 0.4901\n",
      "timestep:130, pyg_AUC: 0.4873\n",
      "timestep:131, pyg_AUC: 0.4915\n",
      "timestep:132, pyg_AUC: 0.4901\n",
      "timestep:133, pyg_AUC: 0.4859\n",
      "timestep:134, pyg_AUC: 0.4915\n",
      "timestep:135, pyg_AUC: 0.4901\n",
      "timestep:136, pyg_AUC: 0.4929\n",
      "timestep:137, pyg_AUC: 0.4845\n",
      "timestep:138, pyg_AUC: 0.4887\n",
      "timestep:139, pyg_AUC: 0.4859\n",
      "timestep:140, pyg_AUC: 0.4887\n",
      "timestep:141, pyg_AUC: 0.4901\n",
      "timestep:142, pyg_AUC: 0.4873\n",
      "timestep:143, pyg_AUC: 0.4901\n",
      "timestep:144, pyg_AUC: 0.4887\n",
      "timestep:145, pyg_AUC: 0.4887\n",
      "timestep:146, pyg_AUC: 0.4887\n",
      "timestep:147, pyg_AUC: 0.4901\n",
      "timestep:148, pyg_AUC: 0.4915\n",
      "timestep:149, pyg_AUC: 0.4901\n",
      "timestep:150, pyg_AUC: 0.4859\n",
      "timestep:151, pyg_AUC: 0.4887\n",
      "timestep:152, pyg_AUC: 0.4901\n",
      "timestep:153, pyg_AUC: 0.4887\n",
      "timestep:154, pyg_AUC: 0.4859\n",
      "timestep:155, pyg_AUC: 0.4873\n",
      "timestep:156, pyg_AUC: 0.4887\n",
      "timestep:157, pyg_AUC: 0.4901\n",
      "timestep:158, pyg_AUC: 0.4887\n",
      "timestep:159, pyg_AUC: 0.4901\n",
      "timestep:160, pyg_AUC: 0.4915\n",
      "timestep:161, pyg_AUC: 0.4831\n",
      "timestep:162, pyg_AUC: 0.4887\n",
      "timestep:163, pyg_AUC: 0.4887\n",
      "timestep:164, pyg_AUC: 0.4901\n",
      "timestep:165, pyg_AUC: 0.4873\n",
      "timestep:166, pyg_AUC: 0.4915\n",
      "timestep:167, pyg_AUC: 0.4887\n",
      "timestep:168, pyg_AUC: 0.4901\n",
      "timestep:169, pyg_AUC: 0.4845\n",
      "timestep:170, pyg_AUC: 0.4944\n",
      "timestep:171, pyg_AUC: 0.4873\n",
      "timestep:172, pyg_AUC: 0.4929\n",
      "timestep:173, pyg_AUC: 0.4887\n",
      "timestep:174, pyg_AUC: 0.4887\n",
      "timestep:175, pyg_AUC: 0.4873\n",
      "timestep:176, pyg_AUC: 0.4859\n",
      "timestep:177, pyg_AUC: 0.4915\n",
      "timestep:178, pyg_AUC: 0.4845\n",
      "timestep:179, pyg_AUC: 0.4887\n",
      "timestep:180, pyg_AUC: 0.4887\n",
      "timestep:181, pyg_AUC: 0.4929\n",
      "timestep:182, pyg_AUC: 0.4887\n",
      "timestep:183, pyg_AUC: 0.4873\n",
      "timestep:184, pyg_AUC: 0.4845\n",
      "timestep:185, pyg_AUC: 0.4901\n",
      "timestep:186, pyg_AUC: 0.4859\n",
      "timestep:187, pyg_AUC: 0.4901\n",
      "timestep:188, pyg_AUC: 0.4901\n",
      "timestep:189, pyg_AUC: 0.4901\n",
      "timestep:190, pyg_AUC: 0.4887\n",
      "timestep:191, pyg_AUC: 0.4887\n",
      "timestep:192, pyg_AUC: 0.4831\n",
      "timestep:193, pyg_AUC: 0.4845\n",
      "timestep:194, pyg_AUC: 0.4831\n",
      "timestep:195, pyg_AUC: 0.4887\n",
      "timestep:196, pyg_AUC: 0.4901\n",
      "timestep:197, pyg_AUC: 0.4929\n",
      "timestep:198, pyg_AUC: 0.4901\n",
      "timestep:199, pyg_AUC: 0.4901\n",
      "timestep:200, pyg_AUC: 0.4831\n",
      "timestep:201, pyg_AUC: 0.4845\n",
      "timestep:202, pyg_AUC: 0.4915\n",
      "timestep:203, pyg_AUC: 0.4901\n",
      "timestep:204, pyg_AUC: 0.4873\n",
      "timestep:205, pyg_AUC: 0.4887\n",
      "timestep:206, pyg_AUC: 0.4859\n",
      "timestep:207, pyg_AUC: 0.4873\n",
      "timestep:208, pyg_AUC: 0.4816\n",
      "timestep:209, pyg_AUC: 0.4887\n",
      "timestep:210, pyg_AUC: 0.4873\n",
      "timestep:211, pyg_AUC: 0.4901\n",
      "timestep:212, pyg_AUC: 0.4887\n",
      "timestep:213, pyg_AUC: 0.4873\n",
      "timestep:214, pyg_AUC: 0.4845\n",
      "timestep:215, pyg_AUC: 0.4887\n",
      "timestep:216, pyg_AUC: 0.4859\n",
      "timestep:217, pyg_AUC: 0.4901\n",
      "timestep:218, pyg_AUC: 0.4873\n",
      "timestep:219, pyg_AUC: 0.4859\n",
      "timestep:220, pyg_AUC: 0.4859\n",
      "timestep:221, pyg_AUC: 0.4901\n",
      "timestep:222, pyg_AUC: 0.4887\n",
      "timestep:223, pyg_AUC: 0.4845\n",
      "timestep:224, pyg_AUC: 0.4859\n",
      "timestep:225, pyg_AUC: 0.4887\n",
      "timestep:226, pyg_AUC: 0.4915\n",
      "timestep:227, pyg_AUC: 0.4887\n",
      "timestep:228, pyg_AUC: 0.4873\n",
      "timestep:229, pyg_AUC: 0.4859\n",
      "timestep:230, pyg_AUC: 0.4915\n",
      "timestep:231, pyg_AUC: 0.4887\n",
      "timestep:232, pyg_AUC: 0.4873\n",
      "timestep:233, pyg_AUC: 0.4873\n",
      "timestep:234, pyg_AUC: 0.4873\n",
      "timestep:235, pyg_AUC: 0.4901\n",
      "timestep:236, pyg_AUC: 0.4845\n",
      "timestep:237, pyg_AUC: 0.4887\n",
      "timestep:238, pyg_AUC: 0.4915\n",
      "timestep:239, pyg_AUC: 0.4873\n",
      "timestep:240, pyg_AUC: 0.4873\n",
      "timestep:241, pyg_AUC: 0.4929\n",
      "timestep:242, pyg_AUC: 0.4873\n",
      "timestep:243, pyg_AUC: 0.4901\n",
      "timestep:244, pyg_AUC: 0.4901\n",
      "timestep:245, pyg_AUC: 0.4887\n",
      "timestep:246, pyg_AUC: 0.4901\n",
      "timestep:247, pyg_AUC: 0.4887\n",
      "timestep:248, pyg_AUC: 0.4887\n",
      "timestep:249, pyg_AUC: 0.4859\n",
      "timestep:250, pyg_AUC: 0.4859\n",
      "timestep:251, pyg_AUC: 0.4873\n",
      "timestep:252, pyg_AUC: 0.4859\n",
      "timestep:253, pyg_AUC: 0.4845\n",
      "timestep:254, pyg_AUC: 0.4831\n",
      "timestep:255, pyg_AUC: 0.4859\n",
      "timestep:256, pyg_AUC: 0.4887\n",
      "timestep:257, pyg_AUC: 0.4944\n",
      "timestep:258, pyg_AUC: 0.4845\n",
      "timestep:259, pyg_AUC: 0.4873\n",
      "timestep:260, pyg_AUC: 0.4901\n",
      "timestep:261, pyg_AUC: 0.4873\n",
      "timestep:262, pyg_AUC: 0.4831\n",
      "timestep:263, pyg_AUC: 0.4873\n",
      "timestep:264, pyg_AUC: 0.4859\n",
      "timestep:265, pyg_AUC: 0.4887\n",
      "timestep:266, pyg_AUC: 0.4887\n",
      "timestep:267, pyg_AUC: 0.4845\n",
      "timestep:268, pyg_AUC: 0.4915\n",
      "timestep:269, pyg_AUC: 0.4901\n",
      "timestep:270, pyg_AUC: 0.4887\n",
      "timestep:271, pyg_AUC: 0.4887\n",
      "timestep:272, pyg_AUC: 0.4845\n",
      "timestep:273, pyg_AUC: 0.4887\n",
      "timestep:274, pyg_AUC: 0.4873\n",
      "timestep:275, pyg_AUC: 0.4859\n",
      "timestep:276, pyg_AUC: 0.4845\n",
      "timestep:277, pyg_AUC: 0.4873\n",
      "timestep:278, pyg_AUC: 0.4859\n",
      "timestep:279, pyg_AUC: 0.4887\n",
      "timestep:280, pyg_AUC: 0.4873\n",
      "timestep:281, pyg_AUC: 0.4915\n",
      "timestep:282, pyg_AUC: 0.4901\n",
      "timestep:283, pyg_AUC: 0.4901\n",
      "timestep:284, pyg_AUC: 0.4901\n",
      "timestep:285, pyg_AUC: 0.4915\n",
      "timestep:286, pyg_AUC: 0.4944\n",
      "timestep:287, pyg_AUC: 0.4887\n",
      "timestep:288, pyg_AUC: 0.4901\n",
      "timestep:289, pyg_AUC: 0.4873\n",
      "timestep:290, pyg_AUC: 0.4887\n",
      "timestep:291, pyg_AUC: 0.4901\n",
      "timestep:292, pyg_AUC: 0.4887\n",
      "timestep:293, pyg_AUC: 0.4901\n",
      "timestep:294, pyg_AUC: 0.4873\n",
      "timestep:295, pyg_AUC: 0.4929\n",
      "timestep:296, pyg_AUC: 0.4901\n",
      "timestep:297, pyg_AUC: 0.4901\n",
      "timestep:298, pyg_AUC: 0.4901\n",
      "timestep:299, pyg_AUC: 0.4901\n",
      "timestep:300, pyg_AUC: 0.4873\n",
      "timestep:301, pyg_AUC: 0.4873\n",
      "timestep:302, pyg_AUC: 0.4859\n",
      "timestep:303, pyg_AUC: 0.4901\n",
      "timestep:304, pyg_AUC: 0.4873\n",
      "timestep:305, pyg_AUC: 0.4873\n",
      "timestep:306, pyg_AUC: 0.4873\n",
      "timestep:307, pyg_AUC: 0.4915\n",
      "timestep:308, pyg_AUC: 0.4859\n",
      "timestep:309, pyg_AUC: 0.4873\n",
      "timestep:310, pyg_AUC: 0.4901\n",
      "timestep:311, pyg_AUC: 0.4859\n",
      "timestep:312, pyg_AUC: 0.4929\n",
      "timestep:313, pyg_AUC: 0.4901\n",
      "timestep:314, pyg_AUC: 0.4944\n",
      "timestep:315, pyg_AUC: 0.4887\n",
      "timestep:316, pyg_AUC: 0.4887\n",
      "timestep:317, pyg_AUC: 0.4929\n",
      "timestep:318, pyg_AUC: 0.4915\n",
      "timestep:319, pyg_AUC: 0.4887\n",
      "timestep:320, pyg_AUC: 0.4873\n",
      "timestep:321, pyg_AUC: 0.4887\n",
      "timestep:322, pyg_AUC: 0.4859\n",
      "timestep:323, pyg_AUC: 0.4915\n",
      "timestep:324, pyg_AUC: 0.4887\n",
      "timestep:325, pyg_AUC: 0.4901\n",
      "timestep:326, pyg_AUC: 0.4901\n",
      "timestep:327, pyg_AUC: 0.4873\n",
      "timestep:328, pyg_AUC: 0.4915\n",
      "timestep:329, pyg_AUC: 0.4859\n",
      "timestep:330, pyg_AUC: 0.4887\n",
      "timestep:331, pyg_AUC: 0.4859\n",
      "timestep:332, pyg_AUC: 0.4915\n",
      "timestep:333, pyg_AUC: 0.4845\n",
      "timestep:334, pyg_AUC: 0.4887\n",
      "timestep:335, pyg_AUC: 0.4958\n",
      "timestep:336, pyg_AUC: 0.4887\n",
      "timestep:337, pyg_AUC: 0.4845\n",
      "timestep:338, pyg_AUC: 0.4901\n",
      "timestep:339, pyg_AUC: 0.4915\n",
      "timestep:340, pyg_AUC: 0.4859\n",
      "timestep:341, pyg_AUC: 0.4887\n",
      "timestep:342, pyg_AUC: 0.4873\n",
      "timestep:343, pyg_AUC: 0.4887\n",
      "timestep:344, pyg_AUC: 0.4887\n",
      "timestep:345, pyg_AUC: 0.4873\n",
      "timestep:346, pyg_AUC: 0.4901\n",
      "timestep:347, pyg_AUC: 0.4859\n",
      "timestep:348, pyg_AUC: 0.4873\n",
      "timestep:349, pyg_AUC: 0.4788\n",
      "timestep:350, pyg_AUC: 0.4887\n",
      "timestep:351, pyg_AUC: 0.4845\n",
      "timestep:352, pyg_AUC: 0.4873\n",
      "timestep:353, pyg_AUC: 0.4873\n",
      "timestep:354, pyg_AUC: 0.4915\n",
      "timestep:355, pyg_AUC: 0.4887\n",
      "timestep:356, pyg_AUC: 0.4873\n",
      "timestep:357, pyg_AUC: 0.4915\n",
      "timestep:358, pyg_AUC: 0.4887\n",
      "timestep:359, pyg_AUC: 0.4915\n",
      "timestep:360, pyg_AUC: 0.4944\n",
      "timestep:361, pyg_AUC: 0.4915\n",
      "timestep:362, pyg_AUC: 0.4915\n",
      "timestep:363, pyg_AUC: 0.4887\n",
      "timestep:364, pyg_AUC: 0.4915\n",
      "timestep:365, pyg_AUC: 0.4929\n",
      "timestep:366, pyg_AUC: 0.4887\n",
      "timestep:367, pyg_AUC: 0.4901\n",
      "timestep:368, pyg_AUC: 0.4887\n",
      "timestep:369, pyg_AUC: 0.4887\n",
      "timestep:370, pyg_AUC: 0.4929\n",
      "timestep:371, pyg_AUC: 0.4901\n",
      "timestep:372, pyg_AUC: 0.4901\n",
      "timestep:373, pyg_AUC: 0.4901\n",
      "timestep:374, pyg_AUC: 0.4901\n",
      "timestep:375, pyg_AUC: 0.4887\n",
      "timestep:376, pyg_AUC: 0.4901\n",
      "timestep:377, pyg_AUC: 0.4873\n",
      "timestep:378, pyg_AUC: 0.4887\n",
      "timestep:379, pyg_AUC: 0.4901\n",
      "timestep:380, pyg_AUC: 0.4859\n",
      "timestep:381, pyg_AUC: 0.4887\n",
      "timestep:382, pyg_AUC: 0.4887\n",
      "timestep:383, pyg_AUC: 0.4915\n",
      "timestep:384, pyg_AUC: 0.4901\n",
      "timestep:385, pyg_AUC: 0.4929\n",
      "timestep:386, pyg_AUC: 0.4915\n",
      "timestep:387, pyg_AUC: 0.4915\n",
      "timestep:388, pyg_AUC: 0.4915\n",
      "timestep:389, pyg_AUC: 0.4859\n",
      "timestep:390, pyg_AUC: 0.4929\n",
      "timestep:391, pyg_AUC: 0.4859\n",
      "timestep:392, pyg_AUC: 0.4901\n",
      "timestep:393, pyg_AUC: 0.4929\n",
      "timestep:394, pyg_AUC: 0.4901\n",
      "timestep:395, pyg_AUC: 0.4944\n",
      "timestep:396, pyg_AUC: 0.4929\n",
      "timestep:397, pyg_AUC: 0.4915\n",
      "timestep:398, pyg_AUC: 0.4887\n",
      "timestep:399, pyg_AUC: 0.4915\n",
      "timestep:400, pyg_AUC: 0.4929\n",
      "timestep:401, pyg_AUC: 0.4901\n",
      "timestep:402, pyg_AUC: 0.4915\n",
      "timestep:403, pyg_AUC: 0.4873\n",
      "timestep:404, pyg_AUC: 0.4873\n",
      "timestep:405, pyg_AUC: 0.4915\n",
      "timestep:406, pyg_AUC: 0.4915\n",
      "timestep:407, pyg_AUC: 0.4915\n",
      "timestep:408, pyg_AUC: 0.4915\n",
      "timestep:409, pyg_AUC: 0.4915\n",
      "timestep:410, pyg_AUC: 0.4887\n",
      "timestep:411, pyg_AUC: 0.4887\n",
      "timestep:412, pyg_AUC: 0.4901\n",
      "timestep:413, pyg_AUC: 0.4887\n",
      "timestep:414, pyg_AUC: 0.4901\n",
      "timestep:415, pyg_AUC: 0.4816\n",
      "timestep:416, pyg_AUC: 0.4901\n",
      "timestep:417, pyg_AUC: 0.4845\n",
      "timestep:418, pyg_AUC: 0.4915\n",
      "timestep:419, pyg_AUC: 0.4887\n",
      "timestep:420, pyg_AUC: 0.4901\n",
      "timestep:421, pyg_AUC: 0.4873\n",
      "timestep:422, pyg_AUC: 0.4915\n",
      "timestep:423, pyg_AUC: 0.4915\n",
      "timestep:424, pyg_AUC: 0.4887\n",
      "timestep:425, pyg_AUC: 0.4901\n",
      "timestep:426, pyg_AUC: 0.4887\n",
      "timestep:427, pyg_AUC: 0.4887\n",
      "timestep:428, pyg_AUC: 0.4901\n",
      "timestep:429, pyg_AUC: 0.4901\n",
      "timestep:430, pyg_AUC: 0.4887\n",
      "timestep:431, pyg_AUC: 0.4915\n",
      "timestep:432, pyg_AUC: 0.4915\n",
      "timestep:433, pyg_AUC: 0.4901\n",
      "timestep:434, pyg_AUC: 0.4915\n",
      "timestep:435, pyg_AUC: 0.4873\n",
      "timestep:436, pyg_AUC: 0.4859\n",
      "timestep:437, pyg_AUC: 0.4873\n",
      "timestep:438, pyg_AUC: 0.4915\n",
      "timestep:439, pyg_AUC: 0.4915\n",
      "timestep:440, pyg_AUC: 0.4901\n",
      "timestep:441, pyg_AUC: 0.4887\n",
      "timestep:442, pyg_AUC: 0.4887\n",
      "timestep:443, pyg_AUC: 0.4887\n",
      "timestep:444, pyg_AUC: 0.4873\n",
      "timestep:445, pyg_AUC: 0.4901\n",
      "timestep:446, pyg_AUC: 0.4915\n",
      "timestep:447, pyg_AUC: 0.4901\n",
      "timestep:448, pyg_AUC: 0.4915\n",
      "timestep:449, pyg_AUC: 0.4901\n",
      "timestep:450, pyg_AUC: 0.4901\n",
      "timestep:451, pyg_AUC: 0.4887\n",
      "timestep:452, pyg_AUC: 0.4859\n",
      "timestep:453, pyg_AUC: 0.4887\n",
      "timestep:454, pyg_AUC: 0.4915\n",
      "timestep:455, pyg_AUC: 0.4845\n",
      "timestep:456, pyg_AUC: 0.4901\n",
      "timestep:457, pyg_AUC: 0.4901\n",
      "timestep:458, pyg_AUC: 0.4901\n",
      "timestep:459, pyg_AUC: 0.4887\n",
      "timestep:460, pyg_AUC: 0.4901\n",
      "timestep:461, pyg_AUC: 0.4915\n",
      "timestep:462, pyg_AUC: 0.4901\n",
      "timestep:463, pyg_AUC: 0.4901\n",
      "timestep:464, pyg_AUC: 0.4901\n",
      "timestep:465, pyg_AUC: 0.4887\n",
      "timestep:466, pyg_AUC: 0.4873\n",
      "timestep:467, pyg_AUC: 0.4887\n",
      "timestep:468, pyg_AUC: 0.4816\n",
      "timestep:469, pyg_AUC: 0.4929\n",
      "timestep:470, pyg_AUC: 0.4929\n",
      "timestep:471, pyg_AUC: 0.4915\n",
      "timestep:472, pyg_AUC: 0.4845\n",
      "timestep:473, pyg_AUC: 0.4831\n",
      "timestep:474, pyg_AUC: 0.4887\n",
      "timestep:475, pyg_AUC: 0.4859\n",
      "timestep:476, pyg_AUC: 0.4901\n",
      "timestep:477, pyg_AUC: 0.4901\n",
      "timestep:478, pyg_AUC: 0.4901\n",
      "timestep:479, pyg_AUC: 0.4901\n",
      "timestep:480, pyg_AUC: 0.4831\n",
      "timestep:481, pyg_AUC: 0.4873\n",
      "timestep:482, pyg_AUC: 0.4887\n",
      "timestep:483, pyg_AUC: 0.4915\n",
      "timestep:484, pyg_AUC: 0.4859\n",
      "timestep:485, pyg_AUC: 0.4859\n",
      "timestep:486, pyg_AUC: 0.4859\n",
      "timestep:487, pyg_AUC: 0.4901\n",
      "timestep:488, pyg_AUC: 0.4901\n",
      "timestep:489, pyg_AUC: 0.4887\n",
      "timestep:490, pyg_AUC: 0.4915\n",
      "timestep:491, pyg_AUC: 0.4859\n",
      "timestep:492, pyg_AUC: 0.4887\n",
      "timestep:493, pyg_AUC: 0.4901\n",
      "timestep:494, pyg_AUC: 0.4873\n",
      "timestep:495, pyg_AUC: 0.4816\n",
      "timestep:496, pyg_AUC: 0.4887\n",
      "timestep:497, pyg_AUC: 0.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [33:48<00:00, 101.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:498, pyg_AUC: 0.4859\n",
      "timestep:499, pyg_AUC: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Callable, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor\n",
    "from torch_geometric.datasets import DGraphFin\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.nn import GCN\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# NOTE: Ensure that the following modules are available in your Kaggle environment.\n",
    "from pygod.metric import *\n",
    "from pygod.metric.metric import *\n",
    "from pygod.utils import load_data\n",
    "from pygod.nn.decoder import DotProductDecoder\n",
    "from pygod.nn.functional import double_recon_loss\n",
    "\n",
    "####################################\n",
    "# Set device (Kaggle provides GPU if enabled)\n",
    "####################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "####################################\n",
    "# Utility Functions & Schedules\n",
    "####################################\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "# define beta schedule and related terms\n",
    "timesteps = 500\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "####################################\n",
    "# Graph Autoencoder (Graph_AE)\n",
    "####################################\n",
    "class Graph_AE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 hid_dim=64,\n",
    "                 num_layers=4,\n",
    "                 dropout=0.,\n",
    "                 act=torch.nn.functional.relu,\n",
    "                 sigmoid_s=False,\n",
    "                 backbone=GCN,\n",
    "                 **kwargs):\n",
    "        super(Graph_AE, self).__init__()\n",
    "\n",
    "        # split the number of layers for the encoder and decoders\n",
    "        assert num_layers >= 2, \"Number of layers must be >= 2.\"\n",
    "        encoder_layers = math.floor(num_layers / 2)\n",
    "        decoder_layers = math.ceil(num_layers / 2)\n",
    "\n",
    "        self.shared_encoder = backbone(in_channels=in_dim,\n",
    "                                       hidden_channels=hid_dim,\n",
    "                                       num_layers=encoder_layers,\n",
    "                                       out_channels=hid_dim,\n",
    "                                       dropout=dropout,\n",
    "                                       act=act,\n",
    "                                       **kwargs)\n",
    "\n",
    "        self.attr_decoder = backbone(in_channels=hid_dim,\n",
    "                                     hidden_channels=hid_dim,\n",
    "                                     num_layers=decoder_layers,\n",
    "                                     out_channels=in_dim,\n",
    "                                     dropout=dropout,\n",
    "                                     act=act,\n",
    "                                     **kwargs)\n",
    "\n",
    "        self.struct_decoder = DotProductDecoder(in_dim=hid_dim,\n",
    "                                                hid_dim=hid_dim,\n",
    "                                                num_layers=decoder_layers - 1,\n",
    "                                                dropout=dropout,\n",
    "                                                act=act,\n",
    "                                                sigmoid_s=sigmoid_s,\n",
    "                                                backbone=backbone,\n",
    "                                                **kwargs)\n",
    "\n",
    "        self.loss_func = double_recon_loss\n",
    "        self.emb = None\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        self.emb = self.encode(x, edge_index)\n",
    "        x_, s_ = self.decode(self.emb, edge_index)\n",
    "        return x_, s_, self.emb\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        self.emb = self.shared_encoder(x, edge_index)\n",
    "        return self.emb\n",
    "\n",
    "    def decode(self, emb, edge_index):\n",
    "        x_ = self.attr_decoder(emb, edge_index)\n",
    "        s_ = self.struct_decoder(emb, edge_index)\n",
    "        return x_, s_\n",
    "\n",
    "####################################\n",
    "# DiffGAD (combining autoencoder + diffusion models)\n",
    "####################################\n",
    "class DiffGAD(BaseTransform):\n",
    "    def __init__(self,\n",
    "                 name=\"\",\n",
    "                 hid_dim=None,\n",
    "                 diff_dim=None,\n",
    "                 ae_epochs=300,\n",
    "                 diff_epochs=800,\n",
    "                 patience=100,\n",
    "                 lr=0.005,\n",
    "                 wd=0.,\n",
    "                 lamda=0.0,\n",
    "                 sample_steps=50,\n",
    "                 radius=1,\n",
    "                 ae_dropout=0.3,\n",
    "                 ae_lr=0.05, \n",
    "                 ae_alpha=0.8,\n",
    "                 verbose=True):\n",
    "\n",
    "        self.name = name\n",
    "        self.hid_dim = hid_dim\n",
    "        self.diff_dim = diff_dim\n",
    "        self.ae_epochs = ae_epochs\n",
    "        self.diff_epochs = diff_epochs\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.sample_steps = sample_steps\n",
    "        self.verbose = verbose\n",
    "        self.lamda = lamda\n",
    "        \n",
    "        self.common_feat = None\n",
    "        self.dm = None\n",
    "\n",
    "        self.ae = None\n",
    "        self.ae_dropout = ae_dropout\n",
    "        self.ae_lr = ae_lr\n",
    "        self.ae_alpha = ae_alpha\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.timesteps = timesteps\n",
    "        self.radius = radius\n",
    "\n",
    "    def forward(self, dset):\n",
    "        self.dataset = dset \n",
    "        data = load_data(self.dataset)\n",
    "\n",
    "        if self.hid_dim is None:\n",
    "            self.hid_dim = 2 ** int(math.log2(data.x.size(1)) - 1)\n",
    "        if self.diff_dim is None:\n",
    "            self.diff_dim = 2 * self.hid_dim\n",
    "        \n",
    "        self.ae = Graph_AE(in_dim=data.num_node_features, \n",
    "                           hid_dim=self.hid_dim,\n",
    "                           dropout=self.ae_dropout).to(device)\n",
    "        self.save_dir = os.path.join(os.getcwd(), 'models', self.dataset, 'full_batch')\n",
    "        self.ae_path = os.path.join(self.save_dir, f\"{self.ae_dropout}_{self.ae_lr}_{self.ae_alpha}_{self.hid_dim}\")\n",
    "        if not os.path.exists(self.ae_path):\n",
    "            os.makedirs(self.ae_path)\n",
    "        ######################## Train Autoencoder #######################\n",
    "        self.train_ae(data)\n",
    "        ae_dict = torch.load(os.path.join(self.ae_path, 'Graph_AE.pt'))\n",
    "        self.ae.load_state_dict(ae_dict['state_dict'])\n",
    "\n",
    "        num_trial = 20\n",
    "        for _ in tqdm.tqdm(range(num_trial)):\n",
    "            ##################################\n",
    "            # Unconditional diffusion models\n",
    "            denoise_fn = MLPDiffusion(self.hid_dim, self.diff_dim).to(device)\n",
    "            self.dm = Model(denoise_fn=denoise_fn, hid_dim=self.hid_dim).to(device)\n",
    "            self.common_feat = self.train_dm(data)\n",
    "            dm_dict = torch.load(os.path.join(self.ae_path, 'edm.pt'))\n",
    "            self.dm.load_state_dict(dm_dict['state_dict'])\n",
    "            self.common_feat = dm_dict['common_feat']\n",
    "            #################################\n",
    "            # Conditional diffusion models\n",
    "            print(\"Common feature:\", self.common_feat)\n",
    "            denoise_condition = MLPDiffusion(self.hid_dim, self.diff_dim).to(device)\n",
    "            self.dm_condition = Model(denoise_fn=denoise_condition, hid_dim=self.hid_dim).to(device)\n",
    "            self.train_dm_condition(data)\n",
    "            dm_free_dict = torch.load(os.path.join(self.ae_path, 'conditional_edm.pt'))\n",
    "            self.dm_condition.load_state_dict(dm_free_dict['state_dict'])\n",
    "            #################################\n",
    "            # Evaluation (sampling)\n",
    "            self.sample_free(self.dm_condition, self.dm, data)\n",
    "\n",
    "    def train_ae(self, data):\n",
    "        if self.verbose:\n",
    "            print('Training autoencoder ...')\n",
    "        optimizer = torch.optim.Adam(self.ae.parameters(), self.ae_lr, weight_decay=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)   \n",
    "        \n",
    "        for epoch in range(1, self.ae_epochs + 1):\n",
    "            self.ae.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = data.x.to(device)\n",
    "            edge_index = data.edge_index.to(device)\n",
    "            y = data.y.bool()\n",
    "            s = to_dense_adj(edge_index)[0].to(device)\n",
    "            x_, s_, embedding = self.ae(x, edge_index)\n",
    "            score = self.ae.loss_func(x, x_, s, s_, self.ae_alpha)\n",
    "            loss = torch.mean(score)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.ae.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Save model checkpoint\n",
    "            torch.save({'state_dict': self.ae.state_dict()},\n",
    "                       os.path.join(self.ae_path, 'Graph_AE.pt'))\n",
    "\n",
    "    def train_dm(self, data):\n",
    "        if self.verbose:\n",
    "            print('Training diffusion model (unconditional) ...')      \n",
    "        optimizer = torch.optim.Adam(self.dm.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "        self.dm.train()\n",
    "        best_loss = float('inf')\n",
    "        patience = 0\n",
    "        common_feat = None\n",
    "        for epoch in range(self.diff_epochs):\n",
    "            x = data.x.to(device)\n",
    "            edge_index = data.edge_index.to(device)\n",
    "            inputs = self.ae.encode(x, edge_index) \n",
    "\n",
    "            if epoch == 0:\n",
    "                common_feat = torch.mean(inputs, dim=0)\n",
    "            else:\n",
    "                s_v = self.cos(common_feat, reconstructed)\n",
    "                omega = softmax_with_temperature(s_v, t=5).reshape(1, -1)\n",
    "                common_feat = torch.mm(omega, reconstructed).detach() \n",
    "\n",
    "            loss, score_train, reconstructed = self.dm(inputs)\n",
    "            loss = loss.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.dm.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Epoch: {:04d} loss= {:.5f}\".format(epoch, loss.item()))\n",
    "            scheduler.step()\n",
    "\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                patience = 0\n",
    "                torch.save({'state_dict': self.dm.state_dict(),\n",
    "                            'common_feat': common_feat},\n",
    "                           os.path.join(self.ae_path, 'edm.pt'))\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience == self.patience:\n",
    "                    if self.verbose:\n",
    "                        print('Early stopping')\n",
    "                    break\n",
    "\n",
    "        return common_feat\n",
    "\n",
    "    def train_dm_condition(self, data):\n",
    "        if self.verbose:\n",
    "            print('Training diffusion model (conditional) ...')      \n",
    "        optimizer = torch.optim.Adam(self.dm_condition.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "        self.dm_condition.train()\n",
    "        best_loss = float('inf')\n",
    "        patience = 0\n",
    "        for epoch in range(self.diff_epochs):\n",
    "            x = data.x.to(device)\n",
    "            edge_index = data.edge_index.to(device)\n",
    "            inputs = self.ae.encode(x, edge_index)\n",
    "            loss, score_train, reconstructed = self.dm_condition(inputs, common_feat=self.common_feat)\n",
    "            loss = loss.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.dm_condition.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Epoch: {:04d} loss= {:.5f}\".format(epoch, loss.item()))\n",
    "            scheduler.step()\n",
    "\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                patience = 0\n",
    "                torch.save({'state_dict': self.dm_condition.state_dict()},\n",
    "                           os.path.join(self.ae_path, 'conditional_edm.pt'))\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience == self.patience:\n",
    "                    if self.verbose:\n",
    "                        print('Early stopping')\n",
    "                    break\n",
    "\n",
    "    def sample_free(self, condition_model, uncondition_model, data):\n",
    "        self.ae.eval()\n",
    "        condition_model.eval()\n",
    "        uncondition_model.eval()\n",
    "        condition_net = condition_model.denoise_fn_D\n",
    "        uncondition_net = uncondition_model.denoise_fn_D\n",
    "        auc = []\n",
    "        x = data.x.to(device)\n",
    "        edge_index = data.edge_index.to(device)\n",
    "        y = data.y.bool()\n",
    "        Z_0 = self.ae.encode(x, edge_index)\n",
    "        ###############  forward process  ####################\n",
    "        noise = torch.randn_like(Z_0)\n",
    "        for i in range(0, self.timesteps):\n",
    "            t = torch.tensor([i] * Z_0.size(0), device=device).long()\n",
    "            sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, Z_0.shape)\n",
    "            sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, Z_0.shape)\n",
    "            Z_t = sqrt_alphas_cumprod_t * Z_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "            if self.sample_steps > 0:\n",
    "                reconstructed = sample_dm_free(condition_net, uncondition_net, Z_t, self.sample_steps,\n",
    "                                               common_feat=self.common_feat, lamda=self.lamda)\n",
    "            s = to_dense_adj(edge_index)[0].to(device)\n",
    "            x_, s_ = self.ae.decode(reconstructed, edge_index)\n",
    "            score = self.ae.loss_func(x, x_, s, s_, self.ae_alpha)\n",
    "\n",
    "            pyg_auc = eval_roc_auc(y, score.cpu().detach())\n",
    "            auc.append(pyg_auc)\n",
    "            print(\"timestep:{}, pyg_AUC: {:.4f}\".format(i, pyg_auc))\n",
    "\n",
    "####################################\n",
    "# Diffusion Model Components (from diffusion_models.py)\n",
    "####################################\n",
    "\n",
    "ModuleType = Union[str, Callable[..., nn.Module]]\n",
    "SIGMA_MIN = 0.002\n",
    "SIGMA_MAX = 80\n",
    "rho = 7\n",
    "S_churn = 1\n",
    "S_min = 0\n",
    "S_max = float('inf')\n",
    "S_noise = 1\n",
    "\n",
    "def softmax_with_temperature(input, t=1, axis=-1):\n",
    "    ex = torch.exp(input / t)\n",
    "    sum_ex = torch.sum(ex, axis=axis)\n",
    "    return ex / sum_ex\n",
    "\n",
    "class EDMLoss:\n",
    "    def __init__(self, P_mean=-1.2, P_std=1.2, sigma_data=0.5, hid_dim=100,\n",
    "                 gamma=5, opts=None):\n",
    "        self.P_mean = P_mean\n",
    "        self.P_std = P_std\n",
    "        self.sigma_data = sigma_data\n",
    "        self.hid_dim = hid_dim\n",
    "        self.gamma = gamma\n",
    "        self.opts = opts\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.KLDiv = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def __call__(self, denoise_fn, data, common_feat=None):\n",
    "        rnd_normal = torch.randn(data.shape[0], device=data.device)\n",
    "        sigma = (rnd_normal * self.P_std + self.P_mean).exp()\n",
    "        weight = (sigma ** 2 + self.sigma_data ** 2) / ((sigma * self.sigma_data) ** 2)\n",
    "\n",
    "        y = data\n",
    "        n = torch.randn_like(y) * sigma.unsqueeze(1)\n",
    "        D_yn = denoise_fn(y + n, sigma, common_feat)\n",
    "        target = y\n",
    "        loss = weight.unsqueeze(1) * ((D_yn - target) ** 2)\n",
    "        reconstruction_errors = (D_yn - target) ** 2   \n",
    "        score = torch.sqrt(torch.sum(reconstruction_errors, 1))\n",
    "        return loss, score, D_yn\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_channels, max_positions=10000, endpoint=False):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.max_positions = max_positions\n",
    "        self.endpoint = endpoint\n",
    "\n",
    "    def forward(self, x):\n",
    "        freqs = torch.arange(start=0, end=self.num_channels // 2,\n",
    "                             dtype=torch.float32, device=x.device)\n",
    "        freqs = freqs / (self.num_channels // 2 - (1 if self.endpoint else 0))\n",
    "        freqs = (1 / self.max_positions) ** freqs\n",
    "        x = x.ger(freqs.to(x.dtype))\n",
    "        x = torch.cat([x.cos(), x.sin()], dim=1)\n",
    "        return x\n",
    "\n",
    "class MLPDiffusion(nn.Module):\n",
    "    def __init__(self, d_in, dim_t=512):\n",
    "        super().__init__()\n",
    "        self.dim_t = dim_t\n",
    "        self.proj = nn.Linear(d_in, dim_t)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim_t, dim_t * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim_t * 2, dim_t * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim_t * 2, dim_t),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim_t, d_in),\n",
    "        )\n",
    "        self.map_noise = PositionalEmbedding(num_channels=dim_t)\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(dim_t, dim_t),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim_t, dim_t)\n",
    "        )\n",
    "        self.feat_proj = nn.Linear(d_in, dim_t)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(dim_t, dim_t)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, noise_labels, common_feat=None):\n",
    "        emb = self.map_noise(noise_labels)\n",
    "        emb = emb.reshape(emb.shape[0], 2, -1).flip(1).reshape(*emb.shape)\n",
    "        emb = self.time_embed(emb)\n",
    "        if common_feat is None:\n",
    "            x = self.proj(x) + emb\n",
    "        else:\n",
    "            x = self.proj(x) + emb + self.feat_proj(common_feat)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class Precond(nn.Module):\n",
    "    def __init__(self, denoise_fn, hid_dim, sigma_min=0, sigma_max=float('inf'), sigma_data=0.5):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_max = sigma_max\n",
    "        self.sigma_data = sigma_data\n",
    "        self.denoise_fn_F = denoise_fn\n",
    "\n",
    "    def forward(self, x, sigma, common_feat=None):\n",
    "        x = x.to(torch.float32)\n",
    "        sigma = sigma.to(torch.float32).reshape(-1, 1)\n",
    "        dtype = torch.float32\n",
    "\n",
    "        c_skip = self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2)\n",
    "        c_out = sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2).sqrt()\n",
    "        c_in = 1 / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n",
    "        c_noise = sigma.log() / 4\n",
    "\n",
    "        x_in = c_in * x \n",
    "        F_x = self.denoise_fn_F(x_in.to(dtype), c_noise.flatten(), common_feat)\n",
    "        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n",
    "        return D_x\n",
    "\n",
    "    def round_sigma(self, sigma):\n",
    "        return torch.as_tensor(sigma)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, denoise_fn, hid_dim, P_mean=-1.2, P_std=1.2, sigma_data=0.5, gamma=5, opts=None, pfgmpp=False):\n",
    "        super().__init__()\n",
    "        self.denoise_fn_D = Precond(denoise_fn, hid_dim)\n",
    "        self.loss_fn = EDMLoss(P_mean, P_std, sigma_data, hid_dim=hid_dim, gamma=5, opts=None)\n",
    "\n",
    "    def forward(self, x, common_feat=None):\n",
    "        loss, score, reconstructed = self.loss_fn(self.denoise_fn_D, x, common_feat)\n",
    "        return loss.mean(-1).mean(), score, reconstructed\n",
    "\n",
    "def sample_step(net, num_steps, i, t_cur, t_next, x_next, common_feat=None):\n",
    "    x_cur = x_next\n",
    "    # Increase noise temporarily.    \n",
    "    gamma = min(S_churn / num_steps, math.sqrt(2) - 1) if S_min <= t_cur <= S_max else 0\n",
    "    t_hat = net.round_sigma(t_cur + gamma * t_cur)\n",
    "    x_hat = x_cur + (t_hat ** 2 - t_cur ** 2).sqrt() * S_noise * torch.randn_like(x_cur)\n",
    "    # Euler step.\n",
    "    denoised = net(x_hat, t_hat, common_feat).to(torch.float32)\n",
    "    d_cur = (x_hat - denoised) / t_hat\n",
    "    x_next = x_hat + (t_next - t_hat) * d_cur\n",
    "    # Apply 2nd order correction.\n",
    "    if i < num_steps - 1:\n",
    "        denoised = net(x_next, t_next, common_feat).to(torch.float32)\n",
    "        d_prime = (x_next - denoised) / t_next\n",
    "        x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "    return x_next\n",
    "\n",
    "def sample_dm(net, noise, num_steps, common_feat=None):\n",
    "    step_indices = torch.arange(num_steps, dtype=torch.float32, device=noise.device)\n",
    "    sigma_min = max(SIGMA_MIN, net.sigma_min)\n",
    "    sigma_max = min(SIGMA_MAX, net.sigma_max)\n",
    "    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
    "    t_steps = torch.cat([net.round_sigma(t_steps), torch.zeros_like(t_steps[:1])])\n",
    "    z = noise.to(torch.float32) * t_steps[0]\n",
    "    with torch.no_grad():\n",
    "        for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):\n",
    "            z = sample_step(net, num_steps, i, t_cur, t_next, z, common_feat)\n",
    "    return z\n",
    "\n",
    "def sample_step_free(condition_net, uncondition_net, num_steps, i, t_cur, t_next, x_next, common_feat=None, lamda=None):\n",
    "    x_cur = x_next\n",
    "    gamma = min(S_churn / num_steps, math.sqrt(2) - 1) if S_min <= t_cur <= S_max else 0\n",
    "    t_hat = uncondition_net.round_sigma(t_cur + gamma * t_cur)\n",
    "    x_hat = x_cur + (t_hat ** 2 - t_cur ** 2).sqrt() * S_noise * torch.randn_like(x_cur)\n",
    "    \n",
    "    denoised_condition = condition_net(x_hat, t_hat, common_feat=common_feat).to(torch.float32)\n",
    "    denoised_uncondition = uncondition_net(x_hat, t_hat).to(torch.float32)\n",
    "    d_cur_condition = (x_hat - denoised_condition) / t_hat\n",
    "    d_cur_uncondition = (x_hat - denoised_uncondition) / t_hat\n",
    "\n",
    "    # Guidance Process\n",
    "    d_cur = (1 + lamda) * d_cur_uncondition - lamda * d_cur_condition\n",
    "    x_next = x_hat + (t_next - t_hat) * d_cur\n",
    "    # Apply 2nd order correction.\n",
    "    if i < num_steps - 1:\n",
    "        denoised_condition = condition_net(x_next, t_next, common_feat=common_feat).to(torch.float32)\n",
    "        denoised_uncondition = uncondition_net(x_next, t_next).to(torch.float32)\n",
    "        d_prime_condition = (x_next - denoised_condition) / t_next\n",
    "        d_prime_uncondition = (x_next - denoised_uncondition) / t_next\n",
    "        d_prime = (1 + lamda) * d_prime_uncondition - lamda * d_prime_condition\n",
    "        x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "    return x_next\n",
    "\n",
    "def sample_dm_free(condition_net, uncondition_net, noise, num_steps, common_feat=None, lamda=None):\n",
    "    step_indices = torch.arange(num_steps, dtype=torch.float32, device=noise.device)\n",
    "    sigma_min = max(SIGMA_MIN, uncondition_net.sigma_min)\n",
    "    sigma_max = min(SIGMA_MAX, uncondition_net.sigma_max)\n",
    "    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
    "    t_steps = torch.cat([uncondition_net.round_sigma(t_steps), torch.zeros_like(t_steps[:1])])\n",
    "    z = noise.to(torch.float32) * t_steps[0]\n",
    "    with torch.no_grad():\n",
    "        for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):\n",
    "            z = sample_step_free(condition_net, uncondition_net, num_steps, i, t_cur, t_next, z, common_feat, lamda=lamda)\n",
    "    return z\n",
    "\n",
    "####################################\n",
    "# Main Execution (replacing main.py)\n",
    "####################################\n",
    "if __name__ == '__main__':\n",
    "    # Set your parameters directly (since Kaggle notebooks do not use CLI args)\n",
    "    lamda = 0.2\n",
    "    dataset = 'disney'\n",
    "    ae_lr = 0.05\n",
    "    ae_alpha = 0.1\n",
    "    ae_dropout = 0.3\n",
    "\n",
    "    # Instantiate and run DiffGAD\n",
    "    model = DiffGAD(lr=0.004,\n",
    "                    ae_alpha=ae_alpha,\n",
    "                    ae_lr=ae_lr,\n",
    "                    ae_dropout=ae_dropout,\n",
    "                    lamda=lamda)\n",
    "    model(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f0512",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-11T16:10:38.542375Z",
     "iopub.status.idle": "2025-02-11T16:10:38.542619Z",
     "shell.execute_reply": "2025-02-11T16:10:38.542520Z"
    },
    "papermill": {
     "duration": 0.279301,
     "end_time": "2025-02-13T06:23:02.748324",
     "exception": false,
     "start_time": "2025-02-13T06:23:02.469023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88c869",
   "metadata": {
    "papermill": {
     "duration": 0.282065,
     "end_time": "2025-02-13T06:23:03.348346",
     "exception": false,
     "start_time": "2025-02-13T06:23:03.066281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1ba00",
   "metadata": {
    "papermill": {
     "duration": 0.281654,
     "end_time": "2025-02-13T06:23:03.901794",
     "exception": false,
     "start_time": "2025-02-13T06:23:03.620140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6647986,
     "sourceId": 10724028,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2059.876752,
   "end_time": "2025-02-13T06:23:06.751790",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-13T05:48:46.875038",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
